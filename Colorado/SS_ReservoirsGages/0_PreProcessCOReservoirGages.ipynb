{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Colorado Site Specific Reservoir and Gage data for WaDE upload.\n",
    "\n",
    "Notes:\n",
    "- Using two different API Colorado CDSS REST web service. 1) [**Division Data**](https://dwr.state.co.us/Rest/GET/Help/Api/GET-api-v2-structures-divrec-waterclasses) api for Division 1-7 site specific information. 2) [**Annual WDID Time Series Data**](https://dwr.state.co.us/Rest/GET/Help/Api/GET-api-v2-structures-divrec-divrecyear) api using sites of interest wdid list produced from Division 1-7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed Libararies\n",
    "\n",
    "# Working with data\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Visulizaiton\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Working with API\n",
    "import requests\n",
    "import io\n",
    "import json\n",
    "\n",
    "# Cleanup\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "pd.set_option('display.max_columns', 999)  # How to display all columns of a Pandas DataFrame in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working Directory\n",
    "workingDir = \"G:/Shared drives/WaDE Data/Colorado/SS_ReservoirsGages/RawInputData\"\n",
    "os.chdir(workingDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Site Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done already\n",
    "\n",
    "# Query by Division (1-7) to get a full list of WDIDs per division.\n",
    "# Plug in \"division=1\" etc into API request.\n",
    "# Save results as Division1.csv, etc.\n",
    "# Rinse and Repeat to retreive all data for Divisions 1-7.\n",
    "\n",
    "# url = \"https://dwr.state.co.us/Rest/GET/api/v2/structures/divrec/waterclasses/?division=7&apiKey=wAC6ZmzcPJ30dyy6nYu6jQmG7BBedcem\"\n",
    "# responseD = json.loads(requests.get(url).text)\n",
    "# LD = responseD['ResultList']\n",
    "\n",
    "# df_ts = pd.DataFrame()\n",
    "# for n in range(len(LD)):\n",
    "#     row = pd.DataFrame([LD[n]])\n",
    "#     df_ts = df_ts.append(row)\n",
    "# df_ts\n",
    "\n",
    "# #Exporting to Finished File\n",
    "# df_ts.to_csv('Division7.csv', index=False)  # The output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division 1\n",
    "fileInput = \"Success/Division1.csv\"\n",
    "dfs1 = pd.read_csv(fileInput)\n",
    "print(len(dfs1))\n",
    "dfs1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division 2\n",
    "fileInput = \"Success/Division2.csv\"\n",
    "dfs2 = pd.read_csv(fileInput)\n",
    "print(len(dfs2))\n",
    "dfs2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division 3\n",
    "fileInput = \"Success/Division3.csv\"\n",
    "dfs3 = pd.read_csv(fileInput)\n",
    "print(len(dfs3))\n",
    "dfs3.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division 4\n",
    "fileInput = \"Success/Division4.csv\"\n",
    "dfs4 = pd.read_csv(fileInput)\n",
    "print(len(dfs4))\n",
    "dfs4.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division 5\n",
    "fileInput = \"Success/Division5.csv\"\n",
    "dfs5 = pd.read_csv(fileInput)\n",
    "print(len(dfs5))\n",
    "dfs5.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division 6\n",
    "fileInput = \"Success/Division6.csv\"\n",
    "dfs6 = pd.read_csv(fileInput)\n",
    "print(len(dfs6))\n",
    "dfs6.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division 7\n",
    "fileInput = \"Success/Division7.csv\"\n",
    "dfs7 = pd.read_csv(fileInput)\n",
    "print(len(dfs7))\n",
    "dfs7.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate\n",
    "frames = [dfs1, dfs2, dfs3, dfs4, dfs5, dfs6, dfs7]\n",
    "dfs = pd.concat(frames)\n",
    "print(len(dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want DivTotal, Year, and Active Records\n",
    "dfs = dfs[dfs[\"divrectype\"] == 'DivTotal']\n",
    "dfs = dfs[dfs[\"availableTimesteps\"] == 'Year']\n",
    "dfs = dfs[dfs[\"ciuCode\"] == 'A']\n",
    "print(len(dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix wdid values that are less then 7 chars long.\n",
    "def formatWDIDValue(colVal):\n",
    "    n = str(colVal)\n",
    "    if len(n) < 7:\n",
    "        outString = \"0\" + n\n",
    "    else:\n",
    "        outString = n\n",
    "\n",
    "    return int(outString)\n",
    "dfs['wdid'] = dfs.apply(lambda row: formatWDIDValue(row['wdid']), axis=1)\n",
    "dfs['wdid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "dfs = dfs.drop_duplicates(subset='wdid', keep=\"first\").reset_index(drop=True)\n",
    "print(len(dfs))\n",
    "dfs.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Timeseries info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done already\n",
    "\n",
    "# %%time\n",
    "\n",
    "# # create wdid list for API\n",
    "# wdidList = dfs['wdid'].tolist()\n",
    "# # Split list into catagories that are 100 long. Issue with CO API timing out after too long.\n",
    "# wdidListB = [wdidList[i:i + 100] for i in range(0, len(wdidList), 100)]\n",
    "\n",
    "# # Use list of WDIDs (from Division data) as inputs, retreive time series data.\n",
    "\n",
    "# # Time Series Dataframe\n",
    "# dfts = pd.DataFrame()\n",
    "\n",
    "# str2 = \"https://dwr.state.co.us/Rest/GET/api/v2/structures/divrec/divrecyear/?wdid=\"\n",
    "# str3 = \"%2C&apiKey=wAC6ZmzcPJ30dyy6nYu6jQmG7BBedcem\"\n",
    "\n",
    "# for i in range(len(wdidListB)):\n",
    "#     lstC = wdidListB[i]\n",
    "#     lstCa = '%2C'.join([str(n) for n in lstC]) \n",
    "    \n",
    "#     url = str2 + lstCa + str3\n",
    "#     responseD = json.loads(requests.get(url).text)\n",
    "#     LD = responseD['ResultList']\n",
    "    \n",
    "#     for n in range(len(LD)):\n",
    "#         row = pd.DataFrame([LD[n]])\n",
    "#         dfts = dfts.append(row)\n",
    "\n",
    "# print(len(dfts))\n",
    "# dfts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done already\n",
    "\n",
    "# # Exporting finished time series file for records sake.\n",
    "# dfts.to_excel('P_TimeSeries.xlsx', index=False)  # The output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeseris data\n",
    "fileInput = \"P_TimeSeries.xlsx\"\n",
    "dfts = pd.read_excel(fileInput)\n",
    "print(len(dfts))\n",
    "dfts.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use measInterval = 'Annual' data\n",
    "dfts = dfts[dfts['measInterval'] == \"Annual\"].reset_index(drop=True)\n",
    "print(len(dfts))\n",
    "dfts.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging division & wdid dataframes into one, using left-join.\n",
    "# need to create a unique join key, use wdid & waterclassNum value.\n",
    "\n",
    "dfs['key'] = dfs['wdid'].astype(str) + dfs['waterclassNum'].astype(str)\n",
    "dfts['key'] = dfts['wdid'].astype(str) + dfts['waterClassNum'].astype(str) \n",
    "\n",
    "df = pd.merge(dfs, dfts, on='key', how='left')\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfout = pd.DataFrame(index=df.index)\n",
    "\n",
    "# Water Source Info\n",
    "dfout['in_WaterSourceName'] = df['waterSource']\n",
    "dfout['in_WaterSourceTypeCV'] = df['waterSource']\n",
    "\n",
    "# Site Info\n",
    "dfout['in_County'] = df['county']\n",
    "dfout['in_GNISCodeCV'] = df['gnisId']\n",
    "dfout['in_Latitude'] = df['latdecdeg'].astype(float)\n",
    "dfout['in_Longitude'] = df['longdecdeg'].astype(float)\n",
    "dfout['in_PODorPOUSite'] = \"Gage\"\n",
    "dfout['in_SiteName'] = df['structureName']\n",
    "dfout['in_SiteNativeID'] = df['wdid_x']\n",
    "dfout['in_SiteTypeCV'] = df['structureType']\n",
    "\n",
    "# Site VariableAmounts Info\n",
    "dfout['in_Amount'] = df['dataValue'].astype(float)\n",
    "dfout['in_BeneficialUseCategory'] = \"DivTotal\"\n",
    "dfout['in_ReportYearCV'] = df['dataMeasDate']\n",
    "dfout['in_TimeframeEnd'] = \"\" # will fill in below with dataMeasDate value\n",
    "dfout['in_TimeframeStart'] = \"\" # will fill in below with dataMeasDate value\n",
    "\n",
    "print(len(dfout))\n",
    "dfout.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WaDE Custom Elements (due to missing sate info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create WaterSourceTypeCV\n",
    "    \n",
    "def createWaterSourceTypeCV(valA):\n",
    "    if \"GROUNDWATER:\" in valA:\n",
    "        outString = \"Groundwater\"\n",
    "    else:\n",
    "        outString = \"Surface Water\"\n",
    "        \n",
    "    return outString\n",
    "\n",
    "dfout['in_WaterSourceTypeCV'] = dfout.apply(lambda row: createWaterSourceTypeCV( row['in_WaterSourceTypeCV']), axis=1)\n",
    "dfout['in_WaterSourceTypeCV'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Creating WaDE Custom water source native ID for easy water source identification\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create temp WaterSourceNativeID dataframe of unique water source.\n",
    "def assignWaterSourceNativeID(colrowValue):\n",
    "    string1 = str(colrowValue)\n",
    "    outstring = \"WaDECO_WS\" + string1\n",
    "    return outstring\n",
    "\n",
    "dfWaterSourceNativeID = pd.DataFrame()\n",
    "dfWaterSourceNativeID['in_WaterSourceName'] = dfout['in_WaterSourceName']\n",
    "dfWaterSourceNativeID['in_WaterSourceTypeCV'] = dfout['in_WaterSourceTypeCV']\n",
    "dfWaterSourceNativeID = dfWaterSourceNativeID.drop_duplicates()\n",
    "\n",
    "dftemp = pd.DataFrame(index=dfWaterSourceNativeID.index)\n",
    "dftemp[\"Count\"] = range(1, len(dftemp.index) + 1)\n",
    "dfWaterSourceNativeID['in_WaterSourceNativeID'] = dftemp.apply(lambda row: assignWaterSourceNativeID(row['Count']), axis=1)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Retreive WaDE Custom water source native ID\n",
    "def retrieveWaterSourceNativeID(A, B):\n",
    "    if (A == '' and B == '') or (pd.isnull(A) and pd.isnull(B)):\n",
    "        outList = ''\n",
    "    else:\n",
    "        ml = dfWaterSourceNativeID.loc[(dfWaterSourceNativeID['in_WaterSourceName'] == A)& \n",
    "                                       (dfWaterSourceNativeID['in_WaterSourceTypeCV'] == B), 'in_WaterSourceNativeID']\n",
    "        if not (ml.empty):  # check if the series is empty\n",
    "            outList = ml.iloc[0]\n",
    "        else:\n",
    "            outList = ''\n",
    "    return outList\n",
    "\n",
    "dfout['in_WaterSourceNativeID'] = dfout.apply(lambda row: retrieveWaterSourceNativeID( row['in_WaterSourceName'], row['in_WaterSourceTypeCV']), axis=1)\n",
    "dfout.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatVarspec(WST):\n",
    "    WST = WST.strip()\n",
    "    outString = \"Stream Gage_Annual_DivTotal_\" + WST\n",
    "    return outString\n",
    "\n",
    "dfout['in_VariableSpecificCV'] = dfout.apply(lambda row: creatVarspec(row['in_WaterSourceTypeCV']), axis=1)\n",
    "dfout['in_VariableSpecificCV'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values of ReportYearCV. Convert to int\n",
    "\n",
    "dfout = dfout.dropna(subset=['in_ReportYearCV']).reset_index(drop=True)\n",
    "dfout['in_ReportYearCV'] = dfout['in_ReportYearCV'].astype(int)\n",
    "print(len(dfout))\n",
    "dfout.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeframeStart & TimeframeEnd\n",
    "\n",
    "dfout['in_TimeframeStart'] = '01/01/' + dfout['in_ReportYearCV'].astype(str)\n",
    "dfout['in_TimeframeEnd'] = '12/31/' + dfout['in_ReportYearCV'].astype(str)\n",
    "dfout.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting to Finished File.\n",
    "dfout.to_csv('P_coSSRGMain.csv', index=False)  # The output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
