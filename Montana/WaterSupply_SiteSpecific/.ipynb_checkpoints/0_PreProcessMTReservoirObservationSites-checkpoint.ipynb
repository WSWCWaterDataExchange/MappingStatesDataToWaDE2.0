{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Montana Site Specific Reservoir and Obervation Site data for WaDE upload.\n",
    "\n",
    "### Goal\n",
    "- Create sites_input.csv, contains location and site information.\n",
    "- Create cleaned input_timeseries.csv with a native site ID field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed Libararies\n",
    "\n",
    "# Working with data\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Visulizaiton\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Working with API\n",
    "import requests\n",
    "import io\n",
    "import json\n",
    "\n",
    "# Cleanup\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "pd.set_option('display.max_columns', 999)  # How to display all columns of a Pandas DataFrame in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working Directory\n",
    "workingDir = \"G:/Shared drives/WaDE Data/Montana/SS_ReservoirsGages/RawInputData\"\n",
    "os.chdir(workingDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe Creation - location data\n",
    "inputFile1 = \"MGS_locations.csv\"\n",
    "df_loc = pd.read_csv(inputFile1)\n",
    "print(len(df_loc))\n",
    "df_loc.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataframe Creation - datasets data\n",
    "inputFile2 = \"MGS_datasets.csv\"\n",
    "df_dase = pd.read_csv(inputFile2)\n",
    "print(len(df_dase))\n",
    "df_dase.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe Creation - timeseries data\n",
    "inputFile3 = \"MGS_timeseries.csv\"\n",
    "df_ts = pd.read_csv(inputFile3)\n",
    "print(len(df_ts))\n",
    "df_ts.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location & Site Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only working with SensorLabel = Daily Average, & ParameterLabel = Discharge or Stage, to recreatie their plots.\n",
    "df_dasetemp = df_dase[(df_dase['SensorLabel'] == 'Daily Average')]\n",
    "df_dasetemp = df_dasetemp[(df_dasetemp['ParameterLabel'] == 'Discharge') | (df_dasetemp['ParameterLabel'] == 'Stage')]\n",
    "\n",
    "print(len(df_dasetemp))\n",
    "df_dasetemp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VariableSpecificCV\n",
    "\n",
    "def createVariableSpecificCV(A):\n",
    "    if A == 'Discharge':\n",
    "        outString = \"Discharge / Flow_Daily_Discharge_Surface Water\"\n",
    "    if A == 'Stage':\n",
    "        outString = \"Reservoir Level_Daily_Stage_Surface Water\"\n",
    "    return outString\n",
    "\n",
    "df_dasetemp['in_VariableSpecificCV'] = df_dasetemp.apply(lambda row: createVariableSpecificCV(row['ParameterLabel']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim down dataset to those fields that are needed.\n",
    "df_dasetemp_sub = df_dasetemp[['LocationCode', 'SensorID', 'SensorLabel', 'TimeSeriesType', 'Parameter', 'UnitOfMeasure', 'ComputationMethod', 'ComputationPeriod', 'LastModifiedTime', 'ParameterLabel', 'in_VariableSpecificCV']]\n",
    "print(len(df_dasetemp_sub))\n",
    "df_dasetemp_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge location info with trimmed down dataset data\n",
    "df_loctemp=pd.DataFrame()\n",
    "df_loctemp = pd.merge(df_loc, df_dasetemp_sub, on='LocationCode', how='left')\n",
    "print(len(df_loctemp))\n",
    "df_loctemp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of SensorID to find and perserve in timesers data\n",
    "SensorIDList = df_dasetemp_sub['SensorID'].tolist()\n",
    "SensorIDList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only those timeseries rows who's SensorID is in the list\n",
    "df_tstemp = df_ts\n",
    "df_tstemp = df_tstemp[df_tstemp['SensorID'].isin(SensorIDList)].drop_duplicates().reset_index(drop=True)\n",
    "print(len(df_tstemp))\n",
    "df_tstemp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract date and time values from Timestamp field\n",
    "# -------------------------------------------------\n",
    "\n",
    "#convert from string to datetime\n",
    "df_tstemp['Timestamp'] = pd.to_datetime(df_tstemp['Timestamp']) \n",
    "\n",
    "# extract date, year and time, create three new fields\n",
    "df_tstemp['Timestamp_Date'] = df_tstemp['Timestamp'].dt.date\n",
    "df_tstemp['Timestamp_Date'] = pd.to_datetime(df_tstemp['Timestamp_Date'], errors = 'coerce')\n",
    "df_tstemp['Timestamp_Date'] = pd.to_datetime(df_tstemp['Timestamp_Date'].dt.strftime('%m/%d/%Y'))\n",
    "\n",
    "# Year\n",
    "df_tstemp['Timestamp_Year'] = pd.DatetimeIndex(df_tstemp['Timestamp_Date']).year\n",
    "\n",
    "# time\n",
    "df_tstemp['Timestamp_Time'] = df_tstemp['Timestamp'].dt.time\n",
    "\n",
    "print(len(df_tstemp))\n",
    "df_tstemp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unused fields\n",
    "# -------------------------------------------------\n",
    "df_tstemp = df_tstemp.drop(['OID_', 'GradeCode', 'GradeName', 'ApprovalLevel'], axis=1)\n",
    "df_tstemp = df_tstemp.drop_duplicates().reset_index(drop=True)\n",
    "df_tstemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export out csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge ts with database\n",
    "dfsupertemp = pd.merge(df_tstemp, df_loctemp, on='SensorID', how='left')\n",
    "print(len(dfsupertemp))\n",
    "dfsupertemp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export out to CSV.\n",
    "dfsupertemp.to_csv('P_mtOSMaster.csv', index=False) # The output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
