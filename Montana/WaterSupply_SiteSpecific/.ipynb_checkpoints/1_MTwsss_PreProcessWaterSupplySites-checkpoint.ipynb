{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Water Supply Site Time Series data for WaDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notes\n",
    "- used MT Esri MapService. Used the query approach.\n",
    "- managed to download location, datasets, and timeseries, will save a local copy for easier use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed Libraries / Modules\n",
    "\n",
    "# ---- working with data ----\n",
    "import numpy as np  # mathematical array manipulation\n",
    "import pandas as pd  # data structure and data analysis\n",
    "import geopandas as gpd  # geo-data structure and data analysis\n",
    "\n",
    "# ---- visualization ----\n",
    "import matplotlib.pyplot as plt  # plotting library\n",
    "import seaborn as sns  # plotting library\n",
    "\n",
    "# ---- API data retrieval ----\n",
    "import requests  # http requests\n",
    "import csv # csv parse\n",
    "import json  # JSON parse\n",
    "import time\n",
    "\n",
    "# ---- Cleanup ----\n",
    "import re  # string regular expression manipulation\n",
    "from datetime import datetime  # date and time manipulation\n",
    "pd.set_option('display.max_columns', 999)  # How to display all columns of a Pandas DataFrame in Jupyter Notebook\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)  # suppress scientific notation in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working Directory\n",
    "workingDir = \"G:/Shared drives/WaDE Data/WaDE Data Folder/Montana/WaterSupply_SiteSpecific\"  # change here\n",
    "os.chdir(workingDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get API Data\n",
    "- done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# This does work, it will return the unique values of the talbes in the MapServer.\n",
    "# I tried to used this to help filter down the timeseries data (Mapserver/2) before I downloaded it as it was too large\n",
    "# \"\"\"\n",
    "\n",
    "# def get_distinct_values(layer_url, field_name):\n",
    "#     params = {\n",
    "#         \"where\": \"1=1\",\n",
    "#         \"outFields\": field_name,\n",
    "#         \"returnGeometry\": \"false\",\n",
    "#         \"returnDistinctValues\": \"true\",\n",
    "#         \"f\": \"json\"\n",
    "#     }\n",
    "#     resp = requests.get(f\"{layer_url}/query\", params=params)\n",
    "#     resp.raise_for_status()\n",
    "#     js = resp.json()\n",
    "#     # The distinct values will be in js[\"features\"] (attributes only)\n",
    "#     distinct = [feat[\"attributes\"][field_name] for feat in js.get(\"features\", [])]\n",
    "#     return distinct\n",
    "\n",
    "# # Example usage:\n",
    "# layer2_url = \"https://gis.dnrc.mt.gov/arcgis/rest/services/WRD/WMB_StAGE/MapServer/2\"\n",
    "# unique_ds = get_distinct_values(layer2_url, \"ApprovalLevel\")  # <---- substitute the field you care about\n",
    "# print(unique_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # Base URL for the “locations” layer’s query endpoint\n",
    "# BASE_URL = \"https://gis.dnrc.mt.gov/arcgis/rest/services/WRD/WMB_StAGE/MapServer/1/query\"\n",
    "\n",
    "# def fetch_all_locations():\n",
    "#     params = {\n",
    "#         \"where\": \"1=1\",\n",
    "#         \"outFields\": \"*\",\n",
    "#         \"f\": \"json\",\n",
    "#         \"returnGeometry\": \"true\",\n",
    "#         # optional: specify the spatial reference for output geometry, e.g., 4326\n",
    "#         \"outSR\": 4326,\n",
    "#         \"resultRecordCount\": 10000,\n",
    "#         \"resultOffset\": 0\n",
    "#     }\n",
    "    \n",
    "#     all_records = []\n",
    "#     all_fields = None\n",
    "    \n",
    "#     while True:\n",
    "#         resp = requests.get(BASE_URL, params=params)\n",
    "#         resp.raise_for_status()\n",
    "#         js = resp.json()\n",
    "        \n",
    "#         features = js.get(\"features\", [])\n",
    "#         if not features:\n",
    "#             break\n",
    "        \n",
    "#         # Capture fields list from the first batch\n",
    "#         if all_fields is None:\n",
    "#             # js[\"fields\"] gives the list of field definitions\n",
    "#             all_fields = [fld[\"name\"] for fld in js[\"fields\"]]\n",
    "#             # We'll also add geometry columns\n",
    "#             geom_props = list(features[0].get(\"geometry\", {}).keys())\n",
    "        \n",
    "#         for feat in features:\n",
    "#             rec = {}\n",
    "#             rec.update(feat[\"attributes\"])\n",
    "#             # flatten geometry: e.g. rec[\"x\"], rec[\"y\"] for point; for more complex geometries, adapt\n",
    "#             geom = feat.get(\"geometry\")\n",
    "#             if geom is not None:\n",
    "#                 for key, val in geom.items():\n",
    "#                     rec[key] = val\n",
    "#             all_records.append(rec)\n",
    "        \n",
    "#         # If fewer features than the page size, we are at the end\n",
    "#         if len(features) < params[\"resultRecordCount\"]:\n",
    "#             break\n",
    "        \n",
    "#         # Otherwise increase offset for next chunk\n",
    "#         params[\"resultOffset\"] += params[\"resultRecordCount\"]\n",
    "        \n",
    "#         # Optional: sleep to avoid hammering the server\n",
    "#         time.sleep(0.2)\n",
    "    \n",
    "#     # Build DataFrame\n",
    "#     df = pd.DataFrame(all_records)\n",
    "    \n",
    "#     # Ensure consistent ordering: attributes first, then geometry keys\n",
    "#     # Let's put all_fields first, then geometry keys\n",
    "#     geom_keys = [k for k in df.columns if k not in all_fields]\n",
    "#     ordered_cols = all_fields + geom_keys\n",
    "#     df = df[ordered_cols]\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# def main():\n",
    "#     print(\"Fetching locations (with geometry)...\")\n",
    "#     df = fetch_all_locations()\n",
    "#     print(f\"Retrieved {len(df)} records.\")\n",
    "#     out_csv = \"locations_with_geometry.csv\"\n",
    "#     df.to_csv(out_csv, index=False)\n",
    "#     print(f\"Saved to {out_csv}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done\n",
    "# %%time\n",
    "# # Base URL for the “datasets” layer’s query endpoint\n",
    "# BASE_URL = \"https://gis.dnrc.mt.gov/arcgis/rest/services/WRD/WMB_StAGE/MapServer/3/query\"\n",
    "\n",
    "# def fetch_all_locations():\n",
    "#     params = {\n",
    "#         \"where\": \"1=1\",\n",
    "#         \"outFields\": \"*\",\n",
    "#         \"f\": \"json\",\n",
    "#         \"resultRecordCount\": 10000,\n",
    "#         \"resultOffset\": 0\n",
    "#     }\n",
    "    \n",
    "#     all_records = []\n",
    "#     all_fields = None\n",
    "    \n",
    "#     while True:\n",
    "#         resp = requests.get(BASE_URL, params=params)\n",
    "#         resp.raise_for_status()\n",
    "#         js = resp.json()\n",
    "        \n",
    "#         features = js.get(\"features\", [])\n",
    "#         if not features:\n",
    "#             break\n",
    "        \n",
    "#         # Capture fields list from the first batch\n",
    "#         if all_fields is None:\n",
    "#             # js[\"fields\"] gives the list of field definitions\n",
    "#             all_fields = [fld[\"name\"] for fld in js[\"fields\"]]\n",
    "#             # We'll also add geometry columns\n",
    "#             geom_props = list(features[0].get(\"geometry\", {}).keys())\n",
    "        \n",
    "#         for feat in features:\n",
    "#             rec = {}\n",
    "#             rec.update(feat[\"attributes\"])\n",
    "#             # flatten geometry: e.g. rec[\"x\"], rec[\"y\"] for point; for more complex geometries, adapt\n",
    "#             geom = feat.get(\"geometry\")\n",
    "#             if geom is not None:\n",
    "#                 for key, val in geom.items():\n",
    "#                     rec[key] = val\n",
    "#             all_records.append(rec)\n",
    "        \n",
    "#         # If fewer features than the page size, we are at the end\n",
    "#         if len(features) < params[\"resultRecordCount\"]:\n",
    "#             break\n",
    "        \n",
    "#         # Otherwise increase offset for next chunk\n",
    "#         params[\"resultOffset\"] += params[\"resultRecordCount\"]\n",
    "        \n",
    "#         # Optional: sleep to avoid hammering the server\n",
    "#         time.sleep(0.2)\n",
    "    \n",
    "#     # Build DataFrame\n",
    "#     df = pd.DataFrame(all_records)\n",
    "    \n",
    "#     # Ensure consistent ordering: attributes first, then geometry keys\n",
    "#     # Let's put all_fields first, then geometry keys\n",
    "#     geom_keys = [k for k in df.columns if k not in all_fields]\n",
    "#     ordered_cols = all_fields + geom_keys\n",
    "#     df = df[ordered_cols]\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# def main():\n",
    "#     print(\"Fetching datasets...\")\n",
    "#     df = fetch_all_locations()\n",
    "#     print(f\"Retrieved {len(df)} records.\")\n",
    "#     out_csv = \"datasets_table.csv\"\n",
    "#     df.to_csv(out_csv, index=False)\n",
    "#     print(f\"Saved to {out_csv}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input File - datasets_table.zip\n",
    "# fileInput = \"RawInputData/datasets_table.zip\"\n",
    "# dfin1 = pd.read_csv(fileInput).replace(np.nan, \"\")\n",
    "\n",
    "# # WaDE UUID tracker for data assessment\n",
    "# if 'WaDEUUID' not in dfin1:\n",
    "#     dfin1['WaDEUUID'] = \"in1\" + dfin1.index.astype(str)\n",
    "#     dfin1.to_csv('RawInputData/datasets_table.zip', compression=dict(method='zip', archive_name='datasets_table.csv'), index=False)\n",
    "\n",
    "# print(len(dfin1))\n",
    "# dfin1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Only working with SensorLabel = Daily Average & ParameterLabel = Discharge or Stage, to recreatie their plots.  Ignore all other fields.\n",
    "# # dfin1 = dfin1[(dfin1['SensorLabel'] == 'Daily Average')]\n",
    "# # dfin1 = dfin1[(dfin1['ParameterLabel'] == 'Discharge') | (dfin1['ParameterLabel'] == 'Stage')]\n",
    "# # print(len(dfin1))\n",
    "# # dfin1.head(1)\n",
    "\n",
    "# ParameterLabel_to_filter = [\"Discharge\", \"Lake Res Elevation\" ,\"Stage\", \"Total Storage\", \"Water Level\"]\n",
    "# dfin1 = dfin1[dfin1['ParameterLabel'].isin(ParameterLabel_to_filter)]\n",
    "\n",
    "# SensorLabel_to_filter = [\n",
    "# \"Cooney Reservoir Elevation\",\n",
    "# \"Cooney Reservoir Storage\",\n",
    "# \"Daily Average\",\n",
    "# \"Daily Average Discharge\",\n",
    "# \"Daily Average GW Elevation\",\n",
    "# \"Daily Average Stage\",\n",
    "# \"Daily elevation\",\n",
    "# \"DailyAverage\",\n",
    "# \"Depth to Groundawter\",\n",
    "# \"Depth to Groundwater\",\n",
    "# \"discharge\",\n",
    "# \"discharge-historic\",\n",
    "# \"disharge\",\n",
    "# \"Elevation\",\n",
    "# \"GW.Elevation\",\n",
    "# \"GW.LEVEL\",\n",
    "# \"GW_ELEVATION\",\n",
    "# \"GW_LEVEL_Daily_Average\",\n",
    "# \"historic\",\n",
    "# \"Historical Daily Average\",\n",
    "# \"Historical Discharge\",\n",
    "# \"Historical.Discharge\",\n",
    "# \"Historical.GW\",\n",
    "# \"Lake Bowdoin\",\n",
    "# \"Reservoir Elevation\",\n",
    "# \"Ruby Reservoir Storage\",\n",
    "# \"stage\",\n",
    "# \"Stage Working\",\n",
    "# \"Storage\",\n",
    "# \"Tongue Reservoir elevation\"]\n",
    "# dfin1 = dfin1[dfin1['SensorLabel'].isin(SensorLabel_to_filter)]\n",
    "\n",
    "# print(len(dfin1))\n",
    "# dfin1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensorList = dfin1['SensorID'].tolist()\n",
    "# set(sensorList)\n",
    "# sensorList.sort()\n",
    "# print(len(sensorList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import csv\n",
    "# import time\n",
    "# import zipfile\n",
    "\n",
    "# url = \"https://gis.dnrc.mt.gov/arcgis/rest/services/WRD/WMB_StAGE/MapServer/2/query\"\n",
    "\n",
    "# all_fields = None\n",
    "# all_rows = []\n",
    "\n",
    "# for sensor in sensorList:\n",
    "#     print(f\"Fetching data for SensorID={sensor} ...\")\n",
    "#     params = {\n",
    "#         \"where\": f\"SensorID='{sensor}' AND RecordedValue IS NOT NULL\",\n",
    "#         \"outFields\": \"*\",\n",
    "#         \"f\": \"json\",\n",
    "#         \"returnGeometry\": \"false\",\n",
    "#         \"resultRecordCount\": 10000,\n",
    "#         \"resultOffset\": 0\n",
    "#     }\n",
    "    \n",
    "#     while True:\n",
    "#         resp = requests.get(url, params=params)\n",
    "#         resp.raise_for_status()\n",
    "#         js = resp.json()\n",
    "        \n",
    "#         features = js.get(\"features\", [])\n",
    "#         if not features:\n",
    "#             print(f\"No records found for SensorID={sensor}\")\n",
    "#             break\n",
    "        \n",
    "#         # First batch: capture field names\n",
    "#         if all_fields is None:\n",
    "#             all_fields = [f[\"name\"] for f in js[\"fields\"]]\n",
    "        \n",
    "#         # Collect attributes\n",
    "#         for feat in features:\n",
    "#             all_rows.append(feat[\"attributes\"])\n",
    "        \n",
    "#         # Paging\n",
    "#         if len(features) < params[\"resultRecordCount\"]:\n",
    "#             break\n",
    "#         params[\"resultOffset\"] += params[\"resultRecordCount\"]\n",
    "        \n",
    "#         time.sleep(0.2)\n",
    "\n",
    "# # Save results to a zipped CSV\n",
    "# if all_rows:\n",
    "#     zip_path = \"RawInputData/timeseries_table.zip\"\n",
    "#     csv_name = \"timeseries_table.csv\"\n",
    "    \n",
    "#     with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "#         with zf.open(csv_name, \"w\") as f:\n",
    "#             # Wrap in text mode for csv.writer\n",
    "#             f = open(f.fileno(), mode=\"w\", encoding=\"utf-8\", newline=\"\", closefd=False)\n",
    "#             writer = csv.DictWriter(f, fieldnames=all_fields)\n",
    "#             writer.writeheader()\n",
    "#             for row in all_rows:\n",
    "#                 writer.writerow(row)\n",
    "#             f.flush()\n",
    "    \n",
    "#     print(f\"✅ Saved {len(all_rows)} records into {zip_path}\")\n",
    "# else:\n",
    "#     print(\"⚠️ No records retrieved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input File - datasets_table\n",
    "fileInput = \"RawInputData/datasets_table.zip\"\n",
    "dfin1 = pd.read_csv(fileInput).replace(np.nan, \"\")\n",
    "\n",
    "# WaDE UUID tracker for data assessment\n",
    "if 'WaDEUUID' not in dfin1:\n",
    "    dfin1['WaDEUUID'] = \"in1\" + dfin1.index.astype(str)\n",
    "    dfin1.to_csv('RawInputData/datasets_table.zip', compression=dict(method='zip', archive_name='datasets_table.csv'), index=False)\n",
    "\n",
    "print(len(dfin1))\n",
    "dfin1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut down the number of records to use\n",
    "\n",
    "ParameterLabel_to_filter = [\"Discharge\", \"Lake Res Elevation\" ,\"Stage\", \"Total Storage\", \"Water Level\"]\n",
    "dfin1 = dfin1[dfin1['ParameterLabel'].isin(ParameterLabel_to_filter)]\n",
    "\n",
    "SensorLabel_to_filter = [\n",
    "\"Cooney Reservoir Elevation\",\n",
    "\"Cooney Reservoir Storage\",\n",
    "\"Daily Average\",\n",
    "\"Daily Average Discharge\",\n",
    "\"Daily Average GW Elevation\",\n",
    "\"Daily Average Stage\",\n",
    "\"Daily elevation\",\n",
    "\"DailyAverage\",\n",
    "\"Depth to Groundawter\",\n",
    "\"Depth to Groundwater\",\n",
    "\"discharge\",\n",
    "\"discharge-historic\",\n",
    "\"disharge\",\n",
    "\"Elevation\",\n",
    "\"GW.Elevation\",\n",
    "\"GW.LEVEL\",\n",
    "\"GW_ELEVATION\",\n",
    "\"GW_LEVEL_Daily_Average\",\n",
    "\"historic\",\n",
    "\"Historical Daily Average\",\n",
    "\"Historical Discharge\",\n",
    "\"Historical.Discharge\",\n",
    "\"Historical.GW\",\n",
    "\"Lake Bowdoin\",\n",
    "\"Reservoir Elevation\",\n",
    "\"Ruby Reservoir Storage\",\n",
    "\"stage\",\n",
    "\"Stage Working\",\n",
    "\"Storage\",\n",
    "\"Tongue Reservoir elevation\"]\n",
    "dfin1 = dfin1[dfin1['SensorLabel'].isin(SensorLabel_to_filter)]\n",
    "\n",
    "print(len(dfin1))\n",
    "dfin1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input File - locations_with_geometry\n",
    "fileInput = \"RawInputData/locations_with_geometry.zip\"\n",
    "dfin2 = pd.read_csv(fileInput).replace(np.nan, \"\")\n",
    "\n",
    "# WaDE UUID tracker for data assessment\n",
    "if 'WaDEUUID' not in dfin2:\n",
    "    dfin2['WaDEUUID'] = \"in2\" + dfin2.index.astype(str)\n",
    "    dfin2.to_csv('RawInputData/locations_with_geometry.zip', compression=dict(method='zip', archive_name='locations_with_geometry.csv'), index=False)\n",
    "\n",
    "print(len(dfin2))\n",
    "dfin2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input File - timeseries_table\n",
    "fileInput = \"RawInputData/timeseries_table.zip\"\n",
    "dfin3 = pd.read_csv(fileInput).replace(np.nan, \"\")\n",
    "\n",
    "# WaDE UUID tracker for data assessment\n",
    "if 'WaDEUUID' not in dfin3:\n",
    "    dfin3['WaDEUUID'] = \"in3\" + dfin3.index.astype(str)\n",
    "    dfin3.to_csv('RawInputData/timeseries_table.zip', compression=dict(method='zip', archive_name='timeseries_table.csv'), index=False)\n",
    "\n",
    "print(len(dfin3))\n",
    "dfin3.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract date and time values from Timestamp field\n",
    "\n",
    "#convert from string to datetime\n",
    "dfin3['Timestamp'] = pd.to_datetime(dfin3['Timestamp'], unit=\"ms\") \n",
    "\n",
    "# Date\n",
    "dfin3['Timestamp_Date'] = dfin3['Timestamp'].dt.date\n",
    "dfin3['Timestamp_Date'] = pd.to_datetime(dfin3['Timestamp_Date'], errors = 'coerce')\n",
    "dfin3['Timestamp_Date'] = pd.to_datetime(dfin3['Timestamp_Date'].dt.strftime('%m/%d/%Y'))\n",
    "\n",
    "# Year\n",
    "dfin3['Timestamp_Year'] = pd.DatetimeIndex(dfin3['Timestamp_Date']).year\n",
    "\n",
    "print(len(dfin3))\n",
    "dfin3.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim inputs\n",
    "dfin1_trim = dfin1[[\"SensorID\", \"LocationID\", \"ParameterLabel\", \"SensorLabel\", \"ComputationPeriod\", \"ComputationMethod\", \"UnitOfMeasure\"]]\n",
    "dfin2_trim = dfin2[[\"LocationID\", \"Latitude\", \"Longitude\", \"LocationName\", \"LocationCode\", \"LocationType\"]]\n",
    "dfin3_trim = dfin3[[\"SensorID\", \"RecordedValue\", \"Timestamp_Year\", \"Timestamp_Date\"]]\n",
    "\n",
    "# merge inputs\n",
    "dfin = pd.merge(dfin1_trim, dfin2_trim, on='LocationID', how='inner')\n",
    "dfin = pd.merge(dfin, dfin3_trim, on='SensorID', how='inner')\n",
    "\n",
    "print(len(dfin))\n",
    "dfin.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some site names have a \"Mt\" in them, like \"West Fork Trail Creek Nr Livingston, Mt\"\n",
    "\n",
    "dfin['LocationName'] = dfin['LocationName'].str.split(',').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix units to match those used in WaDE\n",
    "\n",
    "unitFixDict = {\n",
    "    \"ft^3/s\" : \"CFS\",\n",
    "    \"Acre-ft\" : \"AF\",\n",
    "    \"ft\" : \"FT\"\n",
    "}\n",
    "\n",
    "def fixUnitFunc(val):\n",
    "    val = str(val).strip()\n",
    "    try:\n",
    "        outString = unitFixDict[val]\n",
    "    except:\n",
    "        outString = val\n",
    "    return outString\n",
    "\n",
    "dfin['UnitOfMeasure'] = dfin.apply(lambda row: fixUnitFunc(row['UnitOfMeasure']), axis=1)\n",
    "dfin['UnitOfMeasure'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set watersource type based on SensorLabel\n",
    "\n",
    "wsTypeDict = {\n",
    "    \"Daily Average GW Elevation\" : \"Groundwater\", \n",
    "    \"Depth to Groundawter\" : \"Groundwater\", \n",
    "    \"GW.Elevation\" : \"Groundwater\", \n",
    "    \"GW.LEVEL\" : \"Groundwater\", \n",
    "    \"GW_ELEVATION\" : \"Groundwater\", \n",
    "    \"GW_LEVEL_Daily_Average\" : \"Groundwater\"\n",
    "}\n",
    "\n",
    "def checkWSTypeFunc(val):\n",
    "    val = str(val).strip()\n",
    "    try:\n",
    "        outString = wsTypeDict[val]\n",
    "    except:\n",
    "        outString = \"Surface Water\"\n",
    "    return outString\n",
    "\n",
    "dfin['in_WaterSourceTypeCV'] = dfin.apply(lambda row: checkWSTypeFunc(row['SensorLabel']), axis=1)\n",
    "dfin['in_WaterSourceTypeCV'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output POD dataframe\n",
    "outdf1 = pd.DataFrame(index=dfin.index)\n",
    "\n",
    "# Data Assessment UUID\n",
    "outdf1['WaDEUUID'] = \"\"\n",
    "\n",
    "# Method Info\n",
    "outdf1['in_MethodUUID'] = \"MTwsss_M1\"\n",
    "\n",
    "# Variable Info\n",
    "outdf1['in_AggregationIntervalUnitCV'] = dfin['ComputationPeriod']\n",
    "outdf1['in_AggregationStatisticCV'] = dfin['ComputationMethod']\n",
    "outdf1['in_AmountUnitCV'] = dfin['UnitOfMeasure']\n",
    "outdf1['in_VariableCV'] = dfin['SensorLabel']\n",
    "\n",
    "# Organization Info\n",
    "outdf1['in_OrganizationUUID'] = \"MTwsss_O1\"\n",
    "\n",
    "# WaterSource Info\n",
    "outdf1['in_Geometry'] = \"\"\n",
    "outdf1['in_GNISFeatureNameCV'] = \"\"\n",
    "outdf1['in_WaterQualityIndicatorCV'] = \"\"\n",
    "outdf1['in_WaterSourceName'] = \"\"\n",
    "outdf1['in_WaterSourceNativeID'] = \"\" # auto fill in below\n",
    "outdf1['in_WaterSourceTypeCV'] = dfin['in_WaterSourceTypeCV']\n",
    "\n",
    "# Site Info\n",
    "outdf1['in_CoordinateAccuracy'] = \"\"\n",
    "outdf1['in_CoordinateMethodCV'] = \"\"\n",
    "outdf1['in_County'] = \"\"\n",
    "outdf1['in_EPSGCodeCV'] = \"4326\"\n",
    "outdf1['in_Geometry'] = \"\"\n",
    "outdf1['in_GNISCodeCV'] = \"\"\n",
    "outdf1['in_HUC12'] = \"\"\n",
    "outdf1['in_HUC8'] = \"\"\n",
    "outdf1['in_Latitude'] = dfin['Latitude']\n",
    "outdf1['in_Longitude'] = dfin['Longitude']\n",
    "outdf1['in_NHDNetworkStatusCV'] = \"\"\n",
    "outdf1['in_NHDProductCV'] = \"\"\n",
    "outdf1['in_PODorPOUSite'] = \"Observation Site\"\n",
    "outdf1['in_SiteName'] = dfin['LocationName']\n",
    "outdf1['in_SiteNativeID'] = dfin['LocationCode']\n",
    "outdf1['in_SitePoint'] = \"\"\n",
    "outdf1['in_SiteTypeCV'] = \"\"\n",
    "outdf1['in_StateCV'] = \"MT\"\n",
    "outdf1['in_USGSSiteID'] = \"\"\n",
    "\n",
    "# Site VariableAmounts Info\n",
    "outdf1['in_Amount'] = dfin['RecordedValue']\n",
    "outdf1['in_AllocationCropDutyAmount'] = \"\"\n",
    "outdf1['in_AssociatedNativeAllocationIDs'] = \"\"\n",
    "outdf1['in_BeneficialUseCategory'] = dfin['ParameterLabel']\n",
    "outdf1['in_CommunityWaterSupplySystem'] = \"\"\n",
    "outdf1['in_CropTypeCV'] = \"\"\n",
    "outdf1['in_CustomerTypeCV'] = \"\"\n",
    "outdf1['in_DataPublicationDate'] = \"\"\n",
    "outdf1['in_DataPublicationDOI'] = \"\"\n",
    "outdf1['in_Geometry'] = \"\"\n",
    "outdf1['in_IrrigatedAcreage'] = \"\"\n",
    "outdf1['in_IrrigationMethodCV'] = \"\"\n",
    "outdf1['in_PopulationServed'] = \"\"\n",
    "outdf1['in_PowerGeneratedGWh'] = \"\"\n",
    "outdf1['in_PowerType'] = \"\"\n",
    "outdf1['in_PrimaryUseCategory'] = \"\"\n",
    "outdf1['in_ReportYearCV'] = dfin['Timestamp_Year']\n",
    "outdf1['in_SDWISIdentifier'] = \"\"\n",
    "outdf1['in_TimeframeEnd'] = dfin['Timestamp_Date']\n",
    "outdf1['in_TimeframeStart'] = dfin['Timestamp_Date']\n",
    "\n",
    "outdf1 = outdf1.drop_duplicates().reset_index(drop=True)\n",
    "print(len(outdf1))\n",
    "outdf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes\n",
    "frames = [outdf1]\n",
    "outdf = pd.concat(frames)\n",
    "outdf = outdf.drop_duplicates().reset_index(drop=True).replace(np.nan, \"\")\n",
    "print(len(outdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data / data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean name entries of spcial characters\n",
    "def removeSpecialCharsFunc(Val):\n",
    "    Val = str(Val)\n",
    "    Val = re.sub(\"[$@&.^;/\\)(-]\", \"\", Val).title().replace(\"  \", \" \").strip().rstrip(',')\n",
    "    return Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf['in_WaterSourceName'] = outdf.apply(lambda row: removeSpecialCharsFunc(row['in_WaterSourceName']), axis=1)\n",
    "outdf['in_WaterSourceName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outdf['in_SiteName'] = outdf.apply(lambda row: removeSpecialCharsFunc(row['in_SiteName']), axis=1)\n",
    "outdf['in_SiteName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf['in_County'] = outdf.apply(lambda row: removeSpecialCharsFunc(row['in_County']), axis=1)\n",
    "outdf['in_County'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Empty String / remove string value of \"nan\"\n",
    "\n",
    "def ensureEmptyString(val):\n",
    "    val = str(val).strip()\n",
    "    if val == \"\" or val == \" \" or val == \"nan\" or pd.isnull(val):\n",
    "        outString = \"\"\n",
    "    else:\n",
    "        outString = val\n",
    "    return outString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf['in_WaterSourceName'] = outdf.apply(lambda row: ensureEmptyString(row['in_WaterSourceName']), axis=1)\n",
    "outdf['in_WaterSourceName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf['in_WaterSourceTypeCV'] = outdf.apply(lambda row: ensureEmptyString(row['in_WaterSourceTypeCV']), axis=1)\n",
    "outdf['in_WaterSourceTypeCV'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf['in_SiteTypeCV'] = outdf.apply(lambda row: ensureEmptyString(row['in_SiteTypeCV']), axis=1)\n",
    "outdf['in_SiteTypeCV'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf['in_SiteName'] = outdf.apply(lambda row: ensureEmptyString(row['in_SiteName']), axis=1)\n",
    "outdf['in_SiteName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf['in_County'] = outdf.apply(lambda row: ensureEmptyString(row['in_County']), axis=1)\n",
    "outdf['in_County'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf['in_BeneficialUseCategory'] = outdf.apply(lambda row: ensureEmptyString(row['in_BeneficialUseCategory']), axis=1)\n",
    "uniqueList = list(set([i.strip() for i in ','.join(outdf['in_BeneficialUseCategory'].astype(str)).split(',')]))\n",
    "uniqueList.sort()\n",
    "uniqueList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ensure Latitude entry is numireic, replace '0' values for removal\n",
    "outdf['in_Latitude'] = pd.to_numeric(outdf['in_Latitude'], errors='coerce').replace(0,\"\").fillna(\"\")\n",
    "outdf['in_Latitude'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Longitude entry is numireic, replace '0' values for removal\n",
    "outdf['in_Longitude'] = pd.to_numeric(outdf['in_Longitude'], errors='coerce').replace(0,\"\").fillna(\"\")\n",
    "outdf['in_Longitude'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Amount entry is either numireic or blank, no 0 entries\n",
    "outdf['in_Amount'] = pd.to_numeric(outdf['in_Amount'], errors='coerce').round(2).replace(0,\"\").fillna(\"\")\n",
    "outdf['in_Amount'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure PopulationServed entry is numireic WITH 0 entries (no blank strings)\n",
    "outdf['in_PopulationServed'] = pd.to_numeric(outdf['in_PopulationServed'], errors='coerce').round().replace(\"\",0).fillna(0).astype(int).replace(0,\"\").fillna(\"\")\n",
    "outdf['in_PopulationServed'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TimeframeEnd to YYYY-MM-DD format.\n",
    "outdf['in_TimeframeEnd'] = pd.to_datetime(outdf['in_TimeframeEnd'], utc=True, errors = 'coerce').fillna(\"\")\n",
    "outdf['in_TimeframeEnd'] = pd.to_datetime(outdf[\"in_TimeframeEnd\"].dt.strftime('%m/%d/%Y'))\n",
    "outdf['in_TimeframeEnd'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TimeframeStart to YYYY-MM-DD format.\n",
    "outdf['in_TimeframeStart'] = pd.to_datetime(outdf['in_TimeframeStart'], utc=True, errors = 'coerce').fillna(\"\")\n",
    "outdf['in_TimeframeStart'] = pd.to_datetime(outdf[\"in_TimeframeStart\"].dt.strftime('%m/%d/%Y'))\n",
    "outdf['in_TimeframeStart'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract year out\n",
    "# outdf['in_ReportYearCV'] = pd.to_datetime(outdf['in_ReportYearCV'], utc=True, errors = 'coerce').fillna(\"\")\n",
    "# outdf['in_ReportYearCV'] = pd.to_datetime(outdf[\"in_ReportYearCV\"].dt.strftime('%m/%d/%Y'))\n",
    "# outdf['in_ReportYearCV'] = outdf['in_ReportYearCV'].dt.year\n",
    "# outdf['in_ReportYearCV'] = outdf['in_ReportYearCV'].fillna(0).astype(int)\n",
    "outdf['in_ReportYearCV'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assign Primary Use Category\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"C:/Users/rjame/Documents/WSWC Documents/MappingStatesDataToWaDE2.0/5_CustomFunctions/AssignPrimaryUseCategory\")\n",
    "import AssignPrimaryUseCategoryFile # Use Custom import file\n",
    "\n",
    "outdf['in_PrimaryUseCategory'] = outdf.apply(lambda row: AssignPrimaryUseCategoryFile.retrievePrimaryUseCategory(row['in_BeneficialUseCategory']), axis=1)\n",
    "outdf['in_PrimaryUseCategory'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf['in_BeneficialUseCategory'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating WaDE Custom VariableSpecificCV\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "def createVariableSpecificCV(inV, inAIU, inPU, inWST):\n",
    "    inV = str(inV).strip()\n",
    "    inAIU = str(inAIU).strip()\n",
    "    inPU = str(inPU).strip().title()\n",
    "    inWST = str(inWST).strip()\n",
    "    outString = inV + \"_\" + inAIU + \"_\" + inPU + \"_\" + inWST\n",
    "    return outString\n",
    "\n",
    "outdf['in_VariableSpecificCV'] = outdf.apply(lambda row: createVariableSpecificCV(row['in_VariableCV'], \n",
    "                                                                                  row['in_AggregationIntervalUnitCV'],\n",
    "                                                                                  row['in_BeneficialUseCategory'],\n",
    "                                                                                  row['in_WaterSourceTypeCV']), axis=1)\n",
    "outdf['in_VariableSpecificCV'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating WaDE Custom water source native ID for easy water source identification\n",
    "# use unique WaterSourceName and WaterSourceType values\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create temp in_WaterSourceNativeID dataframe of unique water source.\n",
    "def assignIdValueFunc(colRowValue):\n",
    "    string1 = str(colRowValue)\n",
    "    outstring = \"wadeId\" + string1\n",
    "    return outstring\n",
    "\n",
    "dfTempID = pd.DataFrame()\n",
    "dfTempID['in_WaterSourceName'] = outdf['in_WaterSourceName'].astype(str).str.strip()\n",
    "dfTempID['in_WaterSourceTypeCV'] = outdf['in_WaterSourceTypeCV'].astype(str).str.strip()\n",
    "dfTempID = dfTempID.drop_duplicates()\n",
    "\n",
    "dfTempCount = pd.DataFrame(index=dfTempID.index)\n",
    "dfTempCount[\"Count\"] = range(1, len(dfTempCount.index) + 1)\n",
    "dfTempID['in_WaterSourceNativeID'] = dfTempCount.apply(lambda row: assignIdValueFunc(row['Count']), axis=1)\n",
    "dfTempID['linkKey'] = dfTempID['in_WaterSourceName'].astype(str) + dfTempID['in_WaterSourceTypeCV'].astype(str)\n",
    "IdDict = pd.Series(dfTempID.in_WaterSourceNativeID.values, index=dfTempID.linkKey.astype(str)).to_dict()\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Retreive WaDE Custom site native ID\n",
    "def retrieveIdValueFunc(checkVal, valA, valB):\n",
    "    checkVal = str(checkVal).strip()\n",
    "    if checkVal == \"\":\n",
    "        linkKeyVal = str(valA).strip() + str(valB).strip()\n",
    "        outString = IdDict[linkKeyVal]\n",
    "    else:\n",
    "        outString = checkVal\n",
    "    return outString\n",
    "\n",
    "outdf['in_WaterSourceNativeID'] = outdf.apply(lambda row: retrieveIdValueFunc(row['in_WaterSourceNativeID'], \n",
    "                                                                              row['in_WaterSourceName'], row['in_WaterSourceTypeCV']), axis=1)\n",
    "outdf['in_WaterSourceNativeID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating WaDE Custom site native ID for easy site identification\n",
    "# use Unique Latitude, Longitude, SiteName and SiteTypeCV values\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create temp in_SiteNativeID dataframe of unique water source.\n",
    "def assignIdValueFunc(colRowValue):\n",
    "    string1 = str(colRowValue)\n",
    "    outstring = \"wadeId\" + string1\n",
    "    return outstring\n",
    "\n",
    "dfTempID = pd.DataFrame()\n",
    "dfTempID['in_Latitude'] = outdf['in_Latitude'].astype(str).str.strip()\n",
    "dfTempID['in_Longitude'] = outdf['in_Longitude'].astype(str).str.strip()\n",
    "dfTempID['in_SiteName'] = outdf['in_SiteName'].astype(str).str.strip()\n",
    "dfTempID['in_SiteTypeCV'] = outdf['in_SiteTypeCV'].astype(str).str.strip()\n",
    "dfTempID = dfTempID.drop_duplicates()\n",
    "\n",
    "dfTempCount = pd.DataFrame(index=dfTempID.index)\n",
    "dfTempCount[\"Count\"] = range(1, len(dfTempCount.index) + 1)\n",
    "dfTempID['in_SiteNativeID'] = dfTempCount.apply(lambda row: assignIdValueFunc(row['Count']), axis=1)\n",
    "dfTempID['linkKey'] = dfTempID['in_Latitude'].astype(str) + dfTempID['in_Longitude'].astype(str) + dfTempID['in_SiteName'].astype(str)+ dfTempID['in_SiteTypeCV'].astype(str)\n",
    "IdDict = pd.Series(dfTempID.in_SiteNativeID.values, index=dfTempID.linkKey.astype(str)).to_dict()\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Retreive WaDE Custom site native ID\n",
    "def retrieveIdValueFunc(checkVal, valA, valB, valC, valD):\n",
    "    checkVal = str(checkVal).strip()\n",
    "    if checkVal == \"\":\n",
    "        linkKeyVal = str(valA).strip() + str(valB).strip() + str(valC).strip() + str(valD).strip()\n",
    "        outString = IdDict[linkKeyVal]\n",
    "    else:\n",
    "        outString = checkVal\n",
    "    return outString\n",
    "\n",
    "outdf['in_SiteNativeID'] = outdf.apply(lambda row: retrieveIdValueFunc(row['in_SiteNativeID'], \n",
    "                                                                       row['in_Latitude'], row['in_Longitude'],\n",
    "                                                                       row['in_SiteName'], row['in_SiteTypeCV']), axis=1)\n",
    "outdf['in_SiteNativeID'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(outdf))\n",
    "outdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the output dataframe\n",
    "outdf.to_csv('RawInputData/Pwsss_Main.zip', compression=dict(method='zip', archive_name='Pwsss_Main.csv'), index=False)  # The output, save as a zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfv_temp = outdf[['in_VariableSpecificCV', 'in_AggregationIntervalUnitCV', 'in_AggregationStatisticCV', 'in_AmountUnitCV', 'in_VariableCV', 'in_PrimaryUseCategory']]\n",
    "dfv_temp = dfv_temp.drop_duplicates().reset_index(drop=True)\n",
    "dfv_temp.to_csv('RawInputData/dfv_temp.zip', compression=dict(method='zip', archive_name='dfv_temp.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
