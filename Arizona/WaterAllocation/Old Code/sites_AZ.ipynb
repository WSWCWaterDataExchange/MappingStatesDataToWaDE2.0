{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sites_dim\n",
    "Code to generate sites.csv as input to the WaDE db for WA water rights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from pyproj import CRS, Transformer, Proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified: 'ProcessedInputData'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b94f197689bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# working directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mworking_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"ProcessedInputData\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworking_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'ProcessedInputData'"
     ]
    }
   ],
   "source": [
    "# working directory\n",
    "working_dir = \"ProcessedInputData\"\n",
    "os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column names\n",
    "#10.24.19 rename 'WaDESiteUUID' to 'SiteUUID'\n",
    "columns=['SiteUUID', 'SiteNativeID', 'SiteName', 'USGSSiteID', 'SiteTypeCV', 'Longitude', 'Latitude',\n",
    "          'SitePoint', 'SiteNativeURL', 'Geometry', 'CoordinateMethodCV', 'CoordinateAccuracy', 'GNISCodeCV',\n",
    "          'EPSGCodeCV', 'NHDNetworkStatusCV', 'NHDProductCV', 'NHDUpdateDate', 'NHDReachCode', 'NHDMeasureNumber',\n",
    "          'StateCV', 'HUC8', 'HUC12', 'County'\n",
    "]\n",
    "\n",
    "# These are not used currently. Data types inferred from the inputs\n",
    "dtypesx = ['NVarChar(55)\tNVarChar(50)\tNVarChar(500)\tNVarChar(250)\tNVarChar(100)\tDouble\tDouble\tGeometry',\n",
    "           'NVarChar(250)\tGeometry\tNVarChar(100)\tNVarChar(255)\tNVarChar(50)\tNVarChar(50)\tNVarChar(50)',\n",
    "           'NVarChar(50)\tDate\tNVarChar(50)\tNVarChar(50)\tNChar(5)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target dataframe\n",
    "\n",
    "#assumes dtypes inferred from CO file\n",
    "outdf100=pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input files\n",
    "fileInput1 = \"Well_Registry_Wells55.csv\" \n",
    "fileInput2 = \"GWSI_Sites.csv\"\n",
    "\n",
    "# output sites\n",
    "out_sitdim = 'sites.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading inputs...\")\n",
    "\n",
    "# Read Inputs \n",
    "# \n",
    "df200 = pd.read_csv(fileInput1,encoding = \"ISO-8859-1\") #, or alternatively encoding = \"utf-8\"\n",
    "print (len(df200.index))\n",
    "df200.drop_duplicates(inplace=True)\n",
    "print(len(df200))\n",
    "#df200\n",
    "\n",
    "# columns of GWSI_Sites\n",
    "# \"X\", \"Y\", \"OBJECTID\", \"SITE_ID\", \"LOCAL_ID\", \"REG_ID\", \"WELL_TYPE\", \"DD_LAT\", \"DD_LONG\",\n",
    "# \"LAT_NAD27\", \"LONG_NAD27\", \"WELL_ALT\", \"WATER_USE\", \"WELL_DEPTH\", \"CASE_DIAM\", \"DRILL_DATE\", \n",
    "# \"WL_COUNT\", \"LASTWLDATE\", \"WL_DTW\", \"WL_ELEV\", \"SOURCE\", \"IDXBK\n",
    "\n",
    "cols_GWSI = [\"SITE_ID\", \"LOCAL_ID\", \"REG_ID\", \"WELL_TYPE\", \"DD_LAT\", \"DD_LONG\",\n",
    "      \"WATER_USE\",  \"DRILL_DATE\", \"LASTWLDATE\", \"WL_DTW\", \"SOURCE\", \"IDXBK\"]\n",
    "\n",
    "df300 = pd.read_csv(fileInput2,encoding = \"ISO-8859-1\", usecols = cols_GWSI) #, or alternatively encoding = \"utf-8\"\n",
    "print (len(df300.index))\n",
    "df200.drop_duplicates(inplace=True)\n",
    "print(len(df300))\n",
    "#df300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Join tables...\")\n",
    "\n",
    "df100=pd.merge(df200, df300, left_on='REGISTRY_ID', right_on='REG_ID', how='inner') #\n",
    "\n",
    "#print (len(df100.index))\n",
    "\n",
    "#df100 = df100.head(10000) #only runs first 100 lines for testing.\n",
    "\n",
    "df100 = df100.replace(np.nan, '')\n",
    "\n",
    "df100\n",
    "#df100.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only unique water rights that may have multiple sites/pds\n",
    "print(\"Dropping duplicates...\")\n",
    "\n",
    "df100.drop_duplicates(subset = ['REGISTRY_ID'], inplace=True)   #\n",
    "df100 = df100.reset_index(drop=True)\n",
    "\n",
    "print (len(df100.index))\n",
    "\n",
    "df100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df100.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding SiteUUID...\")\n",
    "\n",
    "df100 = df100.assign(SiteUUID='')  #add new column and make is nan\n",
    "\n",
    "#Permit Number\n",
    "df100['SiteUUID'] = df100.apply(lambda row: '_'.join(['AZ', str(row[\"SITE_ID\"])]), axis=1)\n",
    "\n",
    "df100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Project to longitude/ latitude  \")\n",
    "\n",
    "df100 = df100.assign(Longitude='')\n",
    "df100 = df100.assign(Latitude='')\n",
    "\n",
    "# use pyproj to project to lat lon\n",
    "crs_to = CRS('EPSG:4326')  # CRS(\"WGS84\")\n",
    "# NAD27 crs_from = CRS(\"EPSG:4267\") \n",
    "# NAD83 UTM Zone 12N\n",
    "crs_from = CRS(\"EPSG:26912\") \n",
    "transformer = Transformer.from_crs(crs_from, crs_to)\n",
    "\n",
    "# drop cells with no x or y coordinate\n",
    "df100 = df100.replace(np.nan, '') \n",
    "dropIndex = df100.loc[(df100['UTM_X_METERS'] == '') | (df100['UTM_Y_METERS'] == '')].index\n",
    "if len(dropIndex) > 0:\n",
    "    df100 = df100.drop(dropIndex)\n",
    "    df100 = df100.reset_index(drop=True)\n",
    "\n",
    "lonList = []\n",
    "latList = []\n",
    "for ix in range(len(df100.index)):\n",
    "    #print(ix)\n",
    "    x1 = df100.loc[ix, 'UTM_X_METERS']\n",
    "    y1 = df100.loc[ix, 'UTM_Y_METERS']\n",
    "    try:\n",
    "        lat, lon  = transformer.transform(float(x1), float(y1))\n",
    "        lonList.append(lon)\n",
    "        latList.append(lat)\n",
    "    except:\n",
    "        lonList.append('')\n",
    "        latList.append('')\n",
    "\n",
    "df100['Longitude'] = lonList\n",
    "df100['Latitude'] = latList\n",
    "\n",
    "df100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Direct mapping columns...\")\n",
    "#\n",
    "# directly mapped cells\n",
    "# \n",
    "destCols=['SiteNativeID', 'SiteUUID', 'SiteTypeCV', 'Longitude', 'Latitude']\n",
    "srsCols=['SITE_ID', 'SiteUUID', 'WELL_TYPE', 'Longitude', 'Latitude']\n",
    "\n",
    "outdf100[destCols] = df100[srsCols]\n",
    "\n",
    "# replace NaN with blank cells\n",
    "outdf100 = outdf100.replace(np.nan, '') \n",
    "\n",
    "outdf100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Empty lat/lon\")\n",
    "#TODO there are too many empty location coordinates so we are not dropping them here\n",
    "\n",
    "outdf100purge = outdf100.loc[(outdf100['Longitude'] == '') | (outdf100['Longitude'] == np.nan)\n",
    "                             | (outdf100['Latitude'] == '') | (outdf100['Latitude'] == np.nan)]\n",
    "if len(outdf100purge.index) > 0:\n",
    "    outdf100purge.to_csv('sites_latlon_missing.csv')    #index=False,\n",
    "    dropIndex = outdf100purge.index\n",
    "    outdf100 = outdf100.drop(dropIndex) \n",
    "    outdf100 = outdf100.reset_index(drop=True)\n",
    "    \n",
    "outdf100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dropping duplicates...\")\n",
    "#filter the whole table based on a unique combination of site ID, SiteName, SiteType\n",
    "#10.24.19 added lat lon to list\n",
    "print(len(outdf100.index))\n",
    "outdf100 = outdf100.drop_duplicates(subset=['SiteNativeID', 'SiteName', 'SiteTypeCV', 'Longitude', 'Latitude'])   #\n",
    "outdf100 = outdf100.reset_index(drop=True)\n",
    "print(len(outdf100.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hardcoded columns\n",
    "print(\"Hard coded\")\n",
    "\n",
    "outdf100.EPSGCodeCV = 'EPSG:4326'\n",
    "outdf100.SiteName = \"Unspecified\"\n",
    "outdf100.CoordinateMethodCV = \"Unspecified\"\n",
    "outdf100.StateCV = \"AZ\"\n",
    "\n",
    "outdf100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Droping duplicates...\")\n",
    "\n",
    "# replace NaN with blank cells\n",
    "outdf100 = outdf100.replace(np.nan, '')\n",
    "#drop duplicate rows; just make sure\n",
    "outdf100Duplicated=outdf100.loc[outdf100.duplicated()]\n",
    "if len(outdf100Duplicated.index) > 0:\n",
    "    outdf100Duplicated.to_csv(\"sites_duplicaterows.csv\")  # index=False,\n",
    "    outdf100.drop_duplicates(inplace=True)   #\n",
    "    outdf100 = outdf100.reset_index(drop=True)\n",
    "\n",
    "outdf100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking required isnot null...\")\n",
    "# check if any cell of these columns is null\n",
    "requiredCols = ['WaDESiteUUID', 'SiteName', 'CoordinateMethodCV', 'GNISCodeCV', 'EPSGCodeCV']\n",
    "\n",
    "# replace NaN with blank cells\n",
    "outdf100 = outdf100.replace(np.nan, '')\n",
    "\n",
    "outdf100_nullMand = outdf100.loc[(outdf100[\"SiteUUID\"] == '') |\n",
    "                                 (outdf100[\"SiteName\"] == '') | \n",
    "                                 (outdf100[\"CoordinateMethodCV\"] == '') |\n",
    "                                 (outdf100[\"GNISCodeCV\"] == '') | \n",
    "                                 (outdf100[\"EPSGCodeCV\"] == '')]\n",
    "\n",
    "if (len(outdf100_nullMand.index) > 0):\n",
    "    outdf100_nullMand.to_csv('sites_mandatoryFieldMissing.csv')  # index=False,\n",
    "\n",
    "# ToDO: purge these cells if there is any missing? #For now left to be inspected and reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing out...\")\n",
    "\n",
    "#write out\n",
    "outdf100.to_csv(out_sitdim, index=False, encoding = \"utf-8\")\n",
    "\n",
    "print(\"Done sites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
