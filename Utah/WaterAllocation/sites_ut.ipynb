{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sites_dim\n",
    "Code to generate sites.csv as input to the WaDE db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "pip install sodapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sodapy import socrata\n",
    "import os\n",
    "from pyproj import CRS, Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working directory\n",
    "working_dir = \"C:/tseg/jupyterWaDE\"\n",
    "os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the following cell, make sure the input csv file is in the working directory. To obtain the data, go to the following link and download the table: WATER_MASTER (Master Table containing Water Right and Exchange Information). \n",
    "https://www.waterrights.utah.gov/cgi-bin/pubdump.exe?DBNAME=WRDB&SECURITYKEY=wrt2012access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Read Utah input csv file \n",
    "#(file must be already downloaded and stored in the working directory)\n",
    "\n",
    "# input csv\n",
    "input_csv = 'Water_Master.csv'\n",
    "df100 = pd.readcsv(input_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column names\n",
    "columns=['WaDESiteUUID', 'SiteNativeID', 'SiteName', 'USGSSiteID', 'SiteTypeCV', 'Longitude_x', 'Latitude_y',\n",
    "          'SitePoint', 'SiteNativeURL', 'Geometry', 'CoordinateMethodCV', 'CoordinateAccuracy', 'GNISCodeCV',\n",
    "          'EPSGCodeCV', 'NHDNetworkStatusCV', 'NHDProductCV', 'NHDUpdateDate', 'NHDReachCode', 'NHDMeasureNumber',\n",
    "          'StateCV']\n",
    "\n",
    "# These are not used currently. Data types inferred from the inputs \n",
    "dtypesx = ['NVarChar(55)\tNVarChar(50)\tNVarChar(500)\tNVarChar(250)\tNVarChar(100)\tDouble\tDouble\tGeometry',\n",
    "           'NVarChar(250)\tGeometry\tNVarChar(100)\tNVarChar(255)\tNVarChar(50)\tNVarChar(50)\tNVarChar(50)',\n",
    "           'NVarChar(50)\tDate\tNVarChar(50)\tNVarChar(50)\tNChar(5)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target dataframe\n",
    "\n",
    "#assumes dtypes inferred from CO file\n",
    "outdf100=pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utah directly mapped cells\n",
    "destCols=['SiteNativeID']\n",
    "srsCols=['WRCHEX']\n",
    "\n",
    "outdf100[destCols] = df100[srsCols]\n",
    "\n",
    "# UT temporary columns--these are further processed to get mapped columns below\n",
    "srsdestCols = ['POD_TYPE','X_UTM','Y_UTM']\n",
    "outdf100[srsdestCols] = df100[srsdestCols]\n",
    "\n",
    "#replace blank cells by NaN\n",
    "outdf100 = outdf100.replace('', np.nan) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Utah SiteTypeCV\n",
    "\n",
    "Get SiteTypeCV based on the field \"POD_TYPE\" and map:\n",
    "\n",
    "    Blank to “unknown” \n",
    "    \n",
    "    A to Abandoned\n",
    "    \n",
    "    D to Drain\n",
    "    \n",
    "    C, F, N, or P to Sewage\n",
    "    \n",
    "    G to Spring\n",
    "    \n",
    "    R to Point of Rediversion\n",
    "    \n",
    "    S to Surface\n",
    "    \n",
    "    T – Point of Return\n",
    "    \n",
    "    U - Underground\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UT SiteTypeCV mapping \n",
    "\n",
    "#beneficialUseDictionary\n",
    "siteTypedict = {\n",
    "    \"A\":\"Abandoned\",\n",
    "    \"D\":\"Drain\",\n",
    "    \"C\":\"Sewage\",\n",
    "    \"F\":\"Sewage\",\n",
    "    \"N\":\"Sewage\",\n",
    "    \"P\":\"Sewage\",\n",
    "    \"G\":\"Spring\",\n",
    "    \"R\":\"Point of Rediversion\",\n",
    "    \"S\":\"Surface\",\n",
    "    \"T\":\"Point of Return\",\n",
    "    \"U\":\"Underground\"\n",
    "}\n",
    "\n",
    "# temporary column 'POD_TYPE'  \n",
    "#outdf100['POD_TYPE'] = df100['POD_TYPE']\n",
    "\n",
    "nanIndex = outdf100.loc[outdf100['POD_TYPE'].isnull()].index\n",
    "# find no-loop approach\n",
    "for ix in range(len(outdf100.index)):\n",
    "    #if rank == 0: print(ix)\n",
    "    if ix in nanInex:\n",
    "        outdf100.loc[ix, 'SiteTypeCV'] = 'Unknown'\n",
    "    else:\n",
    "        siteTypeListStrStr = outdf100.loc[ix, 'POD_TYPE']\n",
    "        siteTypeListStr = siteTypeListStrStr.strip()  # remove whitespace chars\n",
    "        outdf100.loc[ix, 'SiteTypeCV'] = \",\".join(siteTypedict[inx] for inx in list(str(siteTypeListStr)))  \n",
    "\n",
    "# drop the temporary column\n",
    "# outdf100 = outdf100.drop(columns=['POD_TYPE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utah longitude and latitude coordinates \n",
    "Project the x and y (UTM NAD 83) coordinates to WGS84 lat lon\n",
    "project from the North American Datum of 1983, UTM Zone 12 North, Meters as units\n",
    "to the World Geodetic System 1984 (WGS84)\n",
    "\n",
    "Longitude_x <--- X_UTM\n",
    "Latitude_y <--- Y_UTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UT temporary columns  \n",
    "#outdf100['X_UTM'] = df100['X_UTM']\n",
    "#outdf100['Y_UTM'] = df100['Y_UTM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pyproj to project to lat lon\n",
    "\n",
    "crs_from = CRS('EPSG:4326') #CRS(\"WGS84\")\n",
    "crs_to = CRS(\"EPSG:26912\")\n",
    "transformer = Transformer.from_crs(crs_from, crs_to)\n",
    "\n",
    "X_UTM = outdf100['X_UTM'] \n",
    "Y_UTM = outdf100['Y_UTM'] \n",
    "lonX = []\n",
    "latY = []\n",
    "for x1, y1 in X_UTM, Y_UTM:\n",
    "    lon, lat = transformer.transform(x1, y1)\n",
    "    lonX.append(lon)\n",
    "    latY.append(lat)\n",
    "    \n",
    "outdf100['Longitude_x'] = lonX\n",
    "outdf100['Latitude_y'] = latY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1de8a5859add>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0muu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mzz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0muu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "A=[1, 2, 3]\n",
    "B = [2, 4, 6]\n",
    "\n",
    "uu=[]\n",
    "zz=[]\n",
    "for (x,y) in (A,B):\n",
    "    u,z=x*y\n",
    "    uu.append(u)\n",
    "    zz.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UT drop temp columns\n",
    "outdf100 = outdf100.drop(columns=srsdestCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "\n",
    "#filter the whole table based on a unique combination of site ID, SiteName, SiteType\n",
    "outdf100 = outdf100.drop_duplicates(subset=['SiteNativeID', 'SiteName', 'SiteTypeCV'])   #\n",
    "outdf100 = outdf100.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping empty lat/lon\n",
    "\n",
    "#drop the sites with no long and lat.\n",
    "outdf100 = outdf100.replace('', np.nan) #replace blank strings by NaN\n",
    "outdf100purge = outdf100.loc[(outdf100['Longitude_x'].isnull()) | (outdf100['Latitude_y'].isnull())]\n",
    "if len(outdf100purge.index) > 0:\n",
    "    outdf100purge.to_csv('sites_missing.csv')    #index=False,\n",
    "    dropIndex = outdf100.loc[(outdf100['Longitude_x'].isnull()) | (outdf100['Latitude_y'].isnull())].index\n",
    "    outdf100 = outdf100.drop(dropIndex)\n",
    "    outdf100 = outdf100.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hardcoded columns \n",
    "\n",
    "# hard code \"Unknown\" for SiteTypeCV value if it is missing\n",
    "#outdf100 = outdf100.replace('', np.nan) #replace blank strings by NaN\n",
    "outdf100.loc[outdf100['SiteTypeCV'].isnull(),'SiteTypeCV']='Unknown'\n",
    "#hardcoded\n",
    "outdf100.EPSGCodeCV = 'EPSG:4326'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding UUID\n",
    "\n",
    "#ToDO: no-loop approach?\n",
    "for ix in range(len(outdf100.index)):\n",
    "    outdf100.loc[ix, 'WaDESiteUUID'] = \"_\".join([\"UT\",str(outdf100.loc[ix, 'SiteNativeID'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Checking required isnot null...\")\n",
    "\n",
    "#9.9.19: Adel: check all 'required' (not NA) columns have value (not empty)\n",
    "requiredCols=['WaDESiteUUID', 'SiteName', 'CoordinateMethodCV', 'GNISCodeCV', 'EPSGCodeCV']\n",
    "\n",
    "#replace blank strings by NaN, if there are any\n",
    "outdf100 = outdf100.replace('', np.nan)\n",
    "\n",
    "# check if any cell of these columns is null\n",
    "#outdf100_nullMand = outdf100.loc[outdf100.isnull().any(axis=1)] --for all cols\n",
    "outdf100_nullMand = outdf100.loc[(outdf100[\"WaDESiteUUID\"].isnull()) |\n",
    "                                (outdf100[\"SiteName\"].isnull()) | (outdf100[\"CoordinateMethodCV\"].isnull()) |\n",
    "                                (outdf100[\"GNISCodeCV\"].isnull())|(outdf100[\"EPSGCodeCV\"].isnull())]\n",
    "#outdf100_nullMand = outdf100.loc[[False | (outdf100[varName].isnull()) for varName in requiredCols]]\n",
    "\n",
    "if(len(outdf100_nullMand.index) > 0):\n",
    "    outdf100_nullMand.to_csv('sites_mandatoryFieldMissing.csv')  # index=False,\n",
    "    \n",
    "#ToDO: purge these cells if there is any missing? #For now left to be inspected and reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Writing out...\")\n",
    "\n",
    "# output csv\n",
    "sites_csv = 'sites.csv'\n",
    "#write out\n",
    "outdf100.to_csv(siteCSV, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
