{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sites_dim\n",
    "Code to generate sites.csv as input to the WaDE db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "pip install sodapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sodapy import socrata\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working directory\n",
    "working_dir = \"C:/tseg/jupyterWaDE\"\n",
    "os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain input data, execute one of the three cells bellow. The codes in the follwing two cells do the same thing (find and open the Colorado input data). The third cell is for inputs for Utah\n",
    "1. Get input from Colorado data services (the cell immediately below): obtains the input water rights data from the Colorado water data web services\n",
    "2. Read input csv file for Colorado: reads input data from a csv file that is already stored in the working directory\n",
    "3. Read csv input data for Utah\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Get input from Colorado data services\n",
    "\n",
    "## The code in this cell comes from: https://dev.socrata.com/foundry/data.colorado.gov/a8zw-bjth\n",
    "#It calls the Colorado water data web services to obtain the input water rights data \n",
    "\n",
    "# without authentication of client\n",
    "client = Socrata(\"data.colorado.gov\", None)\n",
    "\n",
    "## authenticated client (needed for non-public datasets)\n",
    "#client = Socrata(data.colorado.gov,\n",
    "#                  MyAppToken,\n",
    "#                  userame=\"user@example.com\",\n",
    "#                  password=\"AFakePassword\")\n",
    "\n",
    "top100 = client.get(\"a8zw-bjth\") #, limit=100)\n",
    "\n",
    "## Convert to pandas DataFrame\n",
    "df100 = pd.DataFrame.from_records(top100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. Read Colorado input csv file \n",
    "#(file must be already downloaded and stored in the working directory)\n",
    "\n",
    "# input csv\n",
    "input_csv = 'Colorado_Water_rights.csv'\n",
    "df100 = pd.readcsv(input_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Read Utah input csv file \n",
    "#(file must be already downloaded and stored in the working directory)\n",
    "\n",
    "# input csv\n",
    "input_csv = 'Water_Master.csv'\n",
    "df100 = pd.readcsv(input_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column names\n",
    "columns=['WaDESiteUUID', 'SiteNativeID', 'SiteName', 'USGSSiteID', 'SiteTypeCV', 'Longitude_x', 'Latitude_y',\n",
    "          'SitePoint', 'SiteNativeURL', 'Geometry', 'CoordinateMethodCV', 'CoordinateAccuracy', 'GNISCodeCV',\n",
    "          'EPSGCodeCV', 'NHDNetworkStatusCV', 'NHDProductCV', 'NHDUpdateDate', 'NHDReachCode', 'NHDMeasureNumber',\n",
    "          'StateCV']\n",
    "\n",
    "# These are not used currently. Data types inferred from the inputs \n",
    "dtypesx = ['NVarChar(55)\tNVarChar(50)\tNVarChar(500)\tNVarChar(250)\tNVarChar(100)\tDouble\tDouble\tGeometry',\n",
    "           'NVarChar(250)\tGeometry\tNVarChar(100)\tNVarChar(255)\tNVarChar(50)\tNVarChar(50)\tNVarChar(50)',\n",
    "           'NVarChar(50)\tDate\tNVarChar(50)\tNVarChar(50)\tNChar(5)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target dataframe\n",
    "\n",
    "#assumes dtypes inferred from CO file\n",
    "outdf100=pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colorado directly mapped cells\n",
    "destCols=['SiteNativeID', 'SiteName', 'SiteTypeCV', 'Longitude_x', 'Latitude_y', 'CoordinateMethodCV', 'GNISCodeCV']\n",
    "srsCols=['WDID', 'Structure Name', 'Structure Type', 'Longitude', 'Latitude','Location Accuracy', 'GNIS ID']\n",
    "outdf100[destCols] = df100[srsCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utah directly mapped cells\n",
    "destCols=['SiteNativeID']\n",
    "srsCols=['WRCHEX']\n",
    "\n",
    "outdf100[destCols] = df100[srsCols]\n",
    "\n",
    "# UT temporary columns--these are further processed to get mapped columns below\n",
    "srsdestCols = ['POD_TYPE','X_UTM','Y_UTM']\n",
    "outdf100[srsdestCols] = df100[srsdestCols]\n",
    "\n",
    "#replace blank cells by NaN\n",
    "outdf100 = outdf100.replace('', np.nan) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Utah SiteTypeCV\n",
    "\n",
    "Get SiteTypeCV based on the field \"POD_TYPE\" and map:\n",
    "\n",
    "    Blank to “unknown” \n",
    "    \n",
    "    A to Abandoned\n",
    "    \n",
    "    D to Drain\n",
    "    \n",
    "    C, F, N, or P to Sewage\n",
    "    \n",
    "    G to Spring\n",
    "    \n",
    "    R to Point of Rediversion\n",
    "    \n",
    "    S to Surface\n",
    "    \n",
    "    T – Point of Return\n",
    "    \n",
    "    U - Underground\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UT SiteTypeCV mapping \n",
    "\n",
    "#beneficialUseDictionary\n",
    "siteTypedict = {\n",
    "    \"A\":\"Abandoned\",\n",
    "    \"D\":\"Drain\",\n",
    "    \"C\":\"Sewage\",\n",
    "    \"F\":\"Sewage\",\n",
    "    \"N\":\"Sewage\",\n",
    "    \"P\":\"Sewage\",\n",
    "    \"G\":\"Spring\",\n",
    "    \"R\":\"Point of Rediversion\",\n",
    "    \"S\":\"Surface\",\n",
    "    \"T\":\"Point of Return\",\n",
    "    \"U\":\"Underground\"\n",
    "}\n",
    "\n",
    "# temporary column 'POD_TYPE'  \n",
    "#outdf100['POD_TYPE'] = df100['POD_TYPE']\n",
    "\n",
    "nanIndex = outdf100.loc[outdf100['POD_TYPE'].isnull()].index\n",
    "# find no-loop approach\n",
    "for ix in range(len(outdf100.index)):\n",
    "    #if rank == 0: print(ix)\n",
    "    if ix in nanInex:\n",
    "        outdf100.loc[ix, 'SiteTypeCV'] = 'Unknown'\n",
    "    else:\n",
    "        siteTypeListStrStr = outdf100.loc[ix, 'POD_TYPE']\n",
    "        siteTypeListStr = siteTypeListStrStr.strip()  # remove whitespace chars\n",
    "        outdf100.loc[ix, 'SiteTypeCV'] = \",\".join(siteTypedict[inx] for inx in list(str(siteTypeListStr)))  \n",
    "\n",
    "# drop the temporary column\n",
    "# outdf100 = outdf100.drop(columns=['POD_TYPE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utah longitude and latitude coordinates \n",
    "Project the x and y (UTM NAD 83) coordinates to WGS84 lat lon\n",
    "project from the North American Datum of 1983, UTM Zone 12 North, Meters as units\n",
    "to the World Geodetic System 1984 (WGS84)\n",
    "\n",
    "Longitude_x <--- X_UTM\n",
    "Latitude_y <--- Y_UTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "pip install gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from osgeo import osr\n",
    "except ImportError: \n",
    "    import osr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This projects points pair to lon, lat pair\n",
    "def projectPointstoWGS84(point_x, point_y, in_epsgCode=4326):\n",
    "    \n",
    "    # source spatial reference from EPSG code \n",
    "    s_srs = osr.SpatialReference()\n",
    "    s_srs.ImportFromEPSG(in_epsgCode)\n",
    "    #s_projt = osr.SpatialReference()\n",
    "    #s_projt.ImportFromEPSG(in_epsgCode)\n",
    "    #s_proj = s_projt.ExportToWkt()\n",
    "    #s_srs.ImportFromWkt(s_proj)\n",
    "    \n",
    "    # target spatial reference is WGS84\n",
    "    t_srs = osr.SpatialReference()\n",
    "    t_srs.SetWellKnownGeogCS(\"WGS84\")\n",
    "   \n",
    "    transform = osr.CoordinateTransformation(s_srs,t_srs)\n",
    "    pointMarker = ogr.Geometry(ogr.wkbPoint)\n",
    "    pointMarker.SetPoint_2D(0, point_x, point_y)\n",
    "    pointMarker.Transform(transform)\n",
    "    lonP = pointMarker.GetX()\n",
    "    latP = pointMarker.GetY()\n",
    "    \n",
    "    return (lonP, latP)\n",
    "\n",
    "# This projects arrays of x and y coordinates to arrays of lon and lat\n",
    "def projectPointsListtoWGS84(xPoints, yPoints, arrSize, in_epsgCode=4326):\n",
    "    \n",
    "    # source spatial reference from EPSG code \n",
    "    s_srs = osr.SpatialReference()\n",
    "    s_srs.ImportFromEPSG(in_epsgCode)\n",
    "    \n",
    "    # target spatial reference is WGS84\n",
    "    t_srs = osr.SpatialReference()\n",
    "    t_srs.SetWellKnownGeogCS(\"WGS84\")\n",
    "   \n",
    "    transform = osr.CoordinateTransformation(s_srs,t_srs)\n",
    "    pointMarker = ogr.Geometry(ogr.wkbPoint)\n",
    "    \n",
    "    lonP = []\n",
    "    latP = []\n",
    "    for ix in range(arrSize):   #(point_x, point_y) in xyPoints:\n",
    "        pointMarker.SetPoint_2D(0, xPoints[ix], yPoints[ix])\n",
    "        pointMarker.Transform(transform)\n",
    "        lonP.append(pointMarker.GetX())\n",
    "        latP.append(pointMarker.GetY())\n",
    "    \n",
    "    return lonP, latP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UT temporary columns  \n",
    "#outdf100['X_UTM'] = df100['X_UTM']\n",
    "#outdf100['Y_UTM'] = df100['Y_UTM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pyproj to project to lat lon\n",
    "\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "crs_from = CRS('EPSG:4326') #CRS(\"WGS84\")\n",
    "crs_to = CRS(\"EPSG:26912\")\n",
    "transformer = Transformer.from_crs(crs_from, crs_to)\n",
    "\n",
    "X_UTM = outdf100['X_UTM'] \n",
    "Y_UTM = outdf100['Y_UTM'] \n",
    "lonX = []\n",
    "latY = []\n",
    "for x1, y1 in X_UTM, Y_UTM:\n",
    "    lon, lat = transformer.transform(x1, y1)\n",
    "    lonX.append(lon)\n",
    "    latY.append(lat)\n",
    "    \n",
    "outdf100['Longitude_x'] = lonX\n",
    "outdf100['Latitude_y'] = latY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UT drop temp columns\n",
    "outdf100 = outdf100.drop(columns=srsdestCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "\n",
    "#filter the whole table based on a unique combination of site ID, SiteName, SiteType\n",
    "outdf100 = outdf100.drop_duplicates(subset=['SiteNativeID', 'SiteName', 'SiteTypeCV'])   #\n",
    "outdf100 = outdf100.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping empty lat/lon\n",
    "\n",
    "#drop the sites with no long and lat.\n",
    "outdf100 = outdf100.replace('', np.nan) #replace blank strings by NaN\n",
    "outdf100purge = outdf100.loc[(outdf100['Longitude_x'].isnull()) | (outdf100['Latitude_y'].isnull())]\n",
    "if len(outdf100purge.index) > 0:\n",
    "    outdf100purge.to_csv('sites_missing.csv')    #index=False,\n",
    "    dropIndex = outdf100.loc[(outdf100['Longitude_x'].isnull()) | (outdf100['Latitude_y'].isnull())].index\n",
    "    outdf100 = outdf100.drop(dropIndex)\n",
    "    outdf100 = outdf100.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hardcoded columns \n",
    "\n",
    "# hard code \"Unknown\" for SiteTypeCV value if it is missing\n",
    "#outdf100 = outdf100.replace('', np.nan) #replace blank strings by NaN\n",
    "outdf100.loc[outdf100['SiteTypeCV'].isnull(),'SiteTypeCV']='Unknown'\n",
    "#hardcoded\n",
    "outdf100.EPSGCodeCV = 'EPSG:4326'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding UUID\n",
    "\n",
    "#ToDO: no-loop approach?\n",
    "for ix in range(len(outdf100.index)):\n",
    "    outdf100.loc[ix, 'WaDESiteUUID'] = \"_\".join([\"CODWR\",str(outdf100.loc[ix, 'SiteNativeID'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTah\n",
    "#Adding UUID\n",
    "\n",
    "#ToDO: no-loop approach?\n",
    "for ix in range(len(outdf100.index)):\n",
    "    outdf100.loc[ix, 'WaDESiteUUID'] = \"_\".join([\"UT\",str(outdf100.loc[ix, 'SiteNativeID'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Checking required isnot null...\")\n",
    "\n",
    "#9.9.19: Adel: check all 'required' (not NA) columns have value (not empty)\n",
    "requiredCols=['WaDESiteUUID', 'SiteName', 'CoordinateMethodCV', 'GNISCodeCV', 'EPSGCodeCV']\n",
    "\n",
    "#replace blank strings by NaN, if there are any\n",
    "outdf100 = outdf100.replace('', np.nan)\n",
    "\n",
    "# check if any cell of these columns is null\n",
    "#outdf100_nullMand = outdf100.loc[outdf100.isnull().any(axis=1)] --for all cols\n",
    "outdf100_nullMand = outdf100.loc[(outdf100[\"WaDESiteUUID\"].isnull()) |\n",
    "                                (outdf100[\"SiteName\"].isnull()) | (outdf100[\"CoordinateMethodCV\"].isnull()) |\n",
    "                                (outdf100[\"GNISCodeCV\"].isnull())|(outdf100[\"EPSGCodeCV\"].isnull())]\n",
    "#outdf100_nullMand = outdf100.loc[[False | (outdf100[varName].isnull()) for varName in requiredCols]]\n",
    "\n",
    "if(len(outdf100_nullMand.index) > 0):\n",
    "    outdf100_nullMand.to_csv('sites_mandatoryFieldMissing.csv')  # index=False,\n",
    "    \n",
    "#ToDO: purge these cells if there is any missing? #For now left to be inspected and reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Writing out...\")\n",
    "\n",
    "# output csv\n",
    "sites_csv = 'sites.csv'\n",
    "#write out\n",
    "outdf100.to_csv(siteCSV, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
