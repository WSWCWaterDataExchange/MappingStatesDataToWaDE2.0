{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sites_dim\n",
    "Code to generate sites.csv as input to the WaDE db for OK water rights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from waterallocationsFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working directory\n",
    "working_dir = \"C:/tseg/OKTest\"\n",
    "os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the following cell, make sure the input csv file is in the working directory. To obtain the data, go to the following link and download the tables: \n",
    "\n",
    "Permitted Surface Water Diversion Points\n",
    "http://home-owrb.opendata.arcgis.com/datasets/permitted-surface-water-diversion-points?geometry=-119.379%2C31.373%2C-77.565%2C37.701  \n",
    "\n",
    "Permitted Groundwater Wells (Point coverage)\n",
    "http://home-owrb.opendata.arcgis.com/datasets/permitted-groundwater-wells  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input files\n",
    "fileInput1 = \"Permitted_Groundwater_Wells.csv\" \n",
    "FileInput2 = \"Permitted_Surface_Water_Diversion_Points.csv\" # Points of diversion\n",
    "FileInput3 = \"Areas_of_Use.csv\"  # \n",
    "\n",
    "# output sites\n",
    "out_sitdim = 'sites.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column names\n",
    "#10.24.19 rename 'WaDESiteUUID' to 'SiteUUID'\n",
    "columns=['SiteUUID', 'SiteNativeID', 'SiteName', 'USGSSiteID', 'SiteTypeCV', 'Longitude', 'Latitude',\n",
    "          'SitePoint', 'SiteNativeURL', 'Geometry', 'CoordinateMethodCV', 'CoordinateAccuracy', 'GNISCodeCV',\n",
    "          'EPSGCodeCV', 'NHDNetworkStatusCV', 'NHDProductCV', 'NHDUpdateDate', 'NHDReachCode', 'NHDMeasureNumber',\n",
    "          'StateCV']\n",
    "\n",
    "# These are not used currently. Data types inferred from the inputs\n",
    "dtypesx = ['NVarChar(55)\tNVarChar(50)\tNVarChar(500)\tNVarChar(250)\tNVarChar(100)\tDouble\tDouble\tGeometry',\n",
    "           'NVarChar(250)\tGeometry\tNVarChar(100)\tNVarChar(255)\tNVarChar(50)\tNVarChar(50)\tNVarChar(50)',\n",
    "           'NVarChar(50)\tDate\tNVarChar(50)\tNVarChar(50)\tNChar(5)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target dataframe\n",
    "\n",
    "#assumes dtypes inferred from CO file\n",
    "outdf100=pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading inputs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3057: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading inputs...\")\n",
    "\n",
    "# Read Inputs and merge tables\n",
    "# ToDO: We are joining 'on-left': keep all rows of mater table (check if need to be refined)\n",
    "\n",
    "# ground water\n",
    "df100_l = pd.read_csv(fileInput1,encoding = \"ISO-8859-1\") #, or alternatively encoding = \"utf-8\"\n",
    "print (len(df100_l.index))\n",
    "\n",
    "#### Join tables\n",
    "\n",
    "# surface water \n",
    "df200 = pd.read_csv(FileInput2,encoding = \"ISO-8859-1\")  \n",
    "print (len(df200.index))\n",
    "\n",
    "df100=pd.merge(df100_l, df200, left_on='OBJECTID', right_on='OBJECTID', how='left') #joined Points of diversiont table into Master_Table\n",
    "#df100\n",
    "print (len(df100.index))\n",
    "\n",
    "#df100 = df100.head(10000) #only runs first 100 lines for testing.\n",
    "\n",
    "#df100 = df100.replace('', np.nan)\n",
    "df100.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying all columns...\n"
     ]
    }
   ],
   "source": [
    "print(\"Direct mapping columns...\")\n",
    "#\n",
    "# Utah directly mapped cells\n",
    "destCols=['SiteNativeID', 'SiteTypeCV', 'Longitude', 'Latitude']\n",
    "srsCols=['OBJECTID', 'Water Type', 'Longitude', 'Latitude']\n",
    "\n",
    "outdf100[destCols] = df100[srsCols]\n",
    "\n",
    "# replace NaN with blank cells\n",
    "outdf100 = outdf100.replace(np.nan, '') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping empty lat/lon\n"
     ]
    }
   ],
   "source": [
    "print(\"Dropping empty lat/lon\")\n",
    "#drop the sites with no long and lat.\n",
    "outdf100purge = outdf100.loc[(outdf100['Longitude'].isnull()) | (outdf100['Longitude'] == '') |\n",
    "                             (outdf100['Latitude'].isnull()) | (outdf100['Latitude'] == '')]\n",
    "if len(outdf100purge.index) > 0:\n",
    "    outdf100purge.to_csv('sites_missing.csv')    #index=False,\n",
    "    dropIndex = outdf100.loc[(outdf100['Longitude'].isnull()) | (outdf100['Longitude'] == '') |\n",
    "                             (outdf100['Latitude'].isnull()) | (outdf100['Latitude'] == '')].index\n",
    "    outdf100 = outdf100.drop(dropIndex)\n",
    "    outdf100 = outdf100.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dropping duplicates...\")\n",
    "#filter the whole table based on a unique combination of site ID, SiteName, SiteType\n",
    "#10.24.19 added lat lon to list\n",
    "print(len(outdf100.index))\n",
    "outdf100 = outdf100.drop_duplicates(subset=['SiteNativeID', 'SiteName', 'SiteTypeCV', 'Longitude', 'Latitude'])   #\n",
    "outdf100 = outdf100.reset_index(drop=True)\n",
    "print(len(outdf100.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding WaDESiteUUID...\n"
     ]
    }
   ],
   "source": [
    "# hardcoded columns\n",
    "print(\"Hard coded\")\n",
    "outdf100.EPSGCodeCV = 'EPSG:4326'\n",
    "outdf100.SiteName = 'Not Provided'    # site name doesn't exist so use Not provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fix empty coordinatemethodCV\")\n",
    "\n",
    "# in this case all rows for CoordinateMethod are empty so hard code it\n",
    "outdf100.CoordinateMethodCV = 'Unspecified'\n",
    "#outdf100.loc[outdf100[\"CoordinateMethodCV\"] == '', \"CoordinateMethodCV\"] = 'Unspecified'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check Site Native IDs are duplicated\")\n",
    "\n",
    "siteNativeIDdup=outdf100.loc[outdf100.duplicated(subset=['SiteNativeID'])]\n",
    "siteNIdDup = False\n",
    "if len(siteNativeIDdup.index) > 0:\n",
    "    print(\"Site Native IDs are duplicated\")\n",
    "    siteNIdDup = True\n",
    "#outdf100\n",
    "\n",
    "siteNativeIDdup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding WaDESiteUUID...\n"
     ]
    }
   ],
   "source": [
    "print(\"Adding SiteUUID...\")\n",
    "\n",
    "if siteNIdDup:    \n",
    "    # 10.24.19 create unique site uuid\n",
    "    outdf100 = outdf100.reset_index(drop=True)\n",
    "    outdf100['TempUUID'] = range(1, len(outdf100.index) + 1)\n",
    "    #append 'OK'\n",
    "    outdf100['SiteUUID'] = outdf100.apply(lambda row: \"_\".join([\"OK\", str(row['TempUUID'])]) , axis=1)\n",
    "    #drop temp uuid\n",
    "    outdf100 = outdf100.drop('TempUUID', axis=1)\n",
    "else:\n",
    "    #append 'OK'\n",
    "    outdf100['SiteUUID'] = outdf100.apply(lambda row: '' if str(row['SiteNativeID']) == '' \n",
    "                                        else \"_\".join([\"OK\", str(row['SiteNativeID'])])), axis=1)\n",
    "\n",
    "#df100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Droping duplicates...\")\n",
    "# replace NaN with blank cells\n",
    "outdf100 = outdf100.replace(np.nan, '')\n",
    "#drop duplicate rows; just make sure\n",
    "outdf100Duplicated=outdf100.loc[outdf100.duplicated()]\n",
    "if len(outdf100Duplicated.index) > 0:\n",
    "    outdf100Duplicated.to_csv(\"sites_duplicaterows.csv\")  # index=False,\n",
    "    outdf100.drop_duplicates(inplace=True)   #\n",
    "    outdf100 = outdf100.reset_index(drop=True)\n",
    "#outdf100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking required isnot null...\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking required isnot null...\")\n",
    "# check if any cell of these columns is null\n",
    "requiredCols = ['SiteUUID', 'SiteName', 'CoordinateMethodCV', 'GNISCodeCV', 'EPSGCodeCV']\n",
    "\n",
    "# replace NaN with blank cells\n",
    "outdf100 = outdf100.replace(np.nan, '')\n",
    "\n",
    "outdf100_nullMand = outdf100.loc[(outdf100[\"SiteUUID\"] == '') |\n",
    "                                 (outdf100[\"SiteName\"] == '') | \n",
    "                                 (outdf100[\"CoordinateMethodCV\"] == '') |\n",
    "                                 (outdf100[\"GNISCodeCV\"] == '') | \n",
    "                                 (outdf100[\"EPSGCodeCV\"] == '')]\n",
    "\n",
    "if (len(outdf100_nullMand.index) > 0):\n",
    "    outdf100_nullMand.to_csv('sites_mandatoryFieldMissing.csv')  # index=False,\n",
    "\n",
    "# ToDO: purge these cells if there is any missing? #For now left to be inspected and reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing out...\n",
      "Done sites\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing out...\")\n",
    "\n",
    "#write out\n",
    "outdf100.to_csv(out_sitdim, index=False, encoding = \"utf-8\")\n",
    "\n",
    "print(\"Done sites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
