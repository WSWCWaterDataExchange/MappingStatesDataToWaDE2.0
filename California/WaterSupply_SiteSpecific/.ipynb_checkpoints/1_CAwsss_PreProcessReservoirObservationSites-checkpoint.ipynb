{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Water Supply Site Time Series data for WaDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Needed Libraries / Modules\n",
    "\n",
    "# ---- working with data ----\n",
    "import os  # native operating system interaction\n",
    "import numpy as np  # mathematical array manipulation\n",
    "import pandas as pd  # data structure and data analysis\n",
    "import geopandas as gpd  # geo-data structure and data analysis\n",
    "\n",
    "# ---- visualization ----\n",
    "import matplotlib.pyplot as plt  # plotting library\n",
    "import seaborn as sns  # plotting library\n",
    "\n",
    "# ---- API data retrieval ----\n",
    "import requests  # http requests\n",
    "import json  # JSON parse\n",
    "from bs4 import BeautifulSoup # text parser\n",
    "\n",
    "# ---- Cleanup ----\n",
    "import re  # string regular expression manipulation\n",
    "from datetime import datetime  # date and time manipulation\n",
    "pd.set_option('display.max_columns', 999)  # How to display all columns of a Pandas DataFrame in Jupyter Notebook\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)  # suppress scientific notation in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Working Directory\n",
    "workingDir = \"G:/Shared drives/WaDE Data/WaDE Data Folder/California/WaterSupply_SiteSpecific\"  # change here\n",
    "os.chdir(workingDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Files\n",
    "- site & timeseries info for reservoirs\n",
    "- site & timeseries info for streamgages\n",
    "- site & timeseries info for active snow depth sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input File: Reservoirs\n",
    "fileInput = \"RawInputData/Reservoirs.zip\"\n",
    "dfr = pd.read_csv(fileInput).replace(np.nan, \"\")\n",
    "\n",
    "# WaDE UUID tracker for data assessment\n",
    "if 'WaDEUUID' not in dfr:\n",
    "    dfr['WaDEUUID'] = \"in1\" + dfr.index.astype(str)\n",
    "    dfr.to_csv('RawInputData/Reservoirs.zip', compression=dict(method='zip', archive_name='Reservoirs.csv'), index=False)\n",
    "\n",
    "dfr.rename(columns=lambda x: x.rstrip(), inplace=True)\n",
    "print(len(dfr))\n",
    "dfr.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input File: StreamGages shp file\n",
    "fileInput = \"RawInputData/shapefiles/StreamGages.zip\"\n",
    "dfsg = gpd.read_file(fileInput).replace(np.nan, \"\")\n",
    "\n",
    "# WaDE UUID tracker for data assessment\n",
    "if 'WaDEUUID' not in dfsg:\n",
    "    dfsg['WaDEUUID'] = \"in2\" + dfsg.index.astype(str)\n",
    "    dfsg.to_csv('RawInputData/StreamGages.zip', compression=dict(method='zip', archive_name='StreamGages.csv'), index=False)\n",
    "\n",
    "print(len(dfsg))\n",
    "dfsg.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only want to work with stream gages whose sites are considered \"Acitve\"\n",
    "\n",
    "dfsg = dfsg[dfsg['sitestatus'] == 'Active'].reset_index(drop=True)\n",
    "print(len(dfsg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StreamGage input data DOES contain some duplicate reservoirs, but does not cleary indicate which are reservoirs and which are stream gages\n",
    "# if site already in resevoir site data, remove from stream gages\n",
    "\n",
    "dfr_idList = dfr['ID'].tolist()\n",
    "dfr_idList = list(set(dfr_idList))\n",
    "\n",
    "dfsg = dfsg[~dfsg['siteid'].isin(dfr_idList)].reset_index(drop=True)\n",
    "print(len(dfsg))\n",
    "dfsg.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input File: Active Snow Depth Sensors\n",
    "fileInput = \"RawInputData/ActiveSnowDepthSesnsors.zip\"\n",
    "dfsd = pd.read_csv(fileInput).replace(np.nan, \"\")\n",
    "\n",
    "# WaDE UUID tracker for data assessment\n",
    "if 'WaDEUUID' not in dfsd:\n",
    "    dfsd['WaDEUUID'] = \"in3\" + dfsd.index.astype(str)\n",
    "    dfsd.to_csv('RawInputData/ActiveSnowDepthSesnsors.zip', compression=dict(method='zip', archive_name='ActiveSnowDepthSesnsors.csv'), index=False)\n",
    "\n",
    "dfsd.rename(columns=lambda x: x.rstrip(), inplace=True)\n",
    "print(len(dfsd))\n",
    "dfsd.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input File: Reservoirs_timeseries\n",
    "\n",
    "fileInput = \"RawInputData/Reservoirs_timeseries.zip\"\n",
    "dfr_ts = pd.read_csv(fileInput).replace(np.nan, \"\")\n",
    "print(len(dfr_ts))\n",
    "dfr_ts.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input File: StreamGages_timeseries\n",
    "\n",
    "fileInput = \"RawInputData/StreamGages_timeseries.zip\"\n",
    "dfsg_ts = pd.read_csv(fileInput).replace(np.nan, \"\")\n",
    "print(len(dfsg_ts))\n",
    "dfsg_ts.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input File: ActiveSnowDepthSesnsors_timeseries\n",
    "\n",
    "fileInput = \"RawInputData/ActiveSnowDepthSesnsors_timeseries.zip\"\n",
    "dfsd_ts = pd.read_csv(fileInput).replace(np.nan, \"\")\n",
    "print(len(dfsd_ts))\n",
    "dfsd_ts.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inner-join reservoir data to timeseries\n",
    "\n",
    "dfin1 = pd.merge(dfr, dfr_ts, left_on='ID', right_on='STATION_ID', how='inner').reset_index(drop=True)\n",
    "print(len(dfin1))\n",
    "dfin1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inner-join streamgage data to timeseries\n",
    "\n",
    "dfin2 = pd.merge(dfsg, dfsg_ts, left_on='siteid', right_on='STATION_ID', how='inner').reset_index(drop=True)\n",
    "print(len(dfin2))\n",
    "dfin2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner-join snow depth data to timeseries\n",
    "\n",
    "dfin3 = pd.merge(dfsd, dfsd_ts, left_on='ID', right_on='STATION_ID', how='inner').reset_index(drop=True)\n",
    "print(len(dfin3))\n",
    "dfin3.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get metadata & timeseries data\n",
    "- https://cdec.water.ca.gov/dynamicapp/staMeta\n",
    "- this is out of order. But essnetialy steps include 1) use site info to get site ids; 2) use site ids with metadata api to determine what timeseries is available; 3) retreive timeseries data for sites based on available metadata.\n",
    "- metadata and timeseries data already retreived, use hard copies for inputs instead now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reservoir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# already done\n",
    "\n",
    "# %%time\n",
    "# # get Reservoirs metadata\n",
    "\n",
    "# tempList = dfr['ID'].tolist()\n",
    "# dftemp = pd.DataFrame()\n",
    "\n",
    "# for i in range(len(tempList)):\n",
    "#     idString = str(tempList[i]).strip()   \n",
    "#     url = \"https://cdec.water.ca.gov/dynamicapp/staMeta?station_id=\" + idString\n",
    "#     try:\n",
    "#         response = requests.get(url)\n",
    "#         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#         table = soup.find_all('table')\n",
    "#         rawData = pd.read_html(str(table))[1]\n",
    "#         rawData[\"ID\"] = idString\n",
    "#         dftemp = pd.concat([dftemp, rawData])\n",
    "#     except:\n",
    "#         print(f' did not work, {url}')\n",
    "\n",
    "# dftemp.to_csv('RawInputData/Reservoirs_Metadata.zip', compression=dict(method='zip', archive_name='Reservoirs_Metadata.csv'), index=False)\n",
    "\n",
    "# print(len(dftemp))\n",
    "# dftemp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input File: Reservoirs_Metadata\n",
    "\n",
    "# fileInput = \"RawInputData/Reservoirs_Metadata.zip\"\n",
    "# dfr_m = pd.read_csv(fileInput).replace(np.nan, \"\")\n",
    "# print(len(dfr_m))\n",
    "# dfr_m.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean up reservoir metadata\n",
    "\n",
    "# dfr_m = dfr_m.rename(columns={\"0\": \"Sensor Description\",  \"1\": \"SensorNums\", \"2\" : \"Duration\", \"3\" : \"Plot\", \"4\" : \"Data Collection\", \"5\" : \"Data Available\"})\n",
    "# dfr_m[['Start Date', 'End Date']] = dfr_m['Data Available'].str.split('to', n=1, expand=True)\n",
    "# dfr_m['Start Date'] = dfr_m['Start Date'].str.strip()\n",
    "# dfr_m['End Date'] = dfr_m['End Date'].str.replace('present','01/01/2025').str.strip()\n",
    "# dfr_m.head()\n",
    "\n",
    "# dfr_m.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # left-join by reservoir metadata to reservoir site data\n",
    "\n",
    "# dfr = pd.merge(dfr, dfr_m, left_on='ID', right_on='ID', how='left')\n",
    "# print(len(dfr))\n",
    "# dfr.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StreamGages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# already done\n",
    "\n",
    "# %%time\n",
    "# # get StreamGages metadata\n",
    "\n",
    "# tempList = dfsg['siteid'].unique().tolist()\n",
    "# dftemp = pd.DataFrame()\n",
    "\n",
    "# for i in range(len(tempList)):\n",
    "#     idString = str(tempList[i]).strip()   \n",
    "#     url = \"https://cdec.water.ca.gov/dynamicapp/staMeta?station_id=\" + idString\n",
    "#     try:\n",
    "#         response = requests.get(url)\n",
    "#         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#         table = soup.find_all('table')\n",
    "#         rawData = pd.read_html(str(table))[1]\n",
    "#         rawData[\"siteid\"] = idString\n",
    "#         dftemp = pd.concat([dftemp, rawData])\n",
    "#     except:\n",
    "#         print(f' did not work, {url}')\n",
    "\n",
    "# dftemp.to_csv('RawInputData/StreamGages_Metadata.zip', compression=dict(method='zip', archive_name='StreamGages_Metadata.csv'), index=False)\n",
    "\n",
    "# print(len(dftemp))\n",
    "# dftemp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Input File: StreamGages_Metadata\n",
    "\n",
    "# fileInput = \"RawInputData/StreamGages_Metadata.zip\"\n",
    "# dfsg_m = pd.read_csv(fileInput).replace(np.nan, \"\")\n",
    "# print(len(dfsg_m))\n",
    "# dfsg_m.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Clean up streamgage metadata\n",
    "\n",
    "# dfsg_m = dfsg_m.rename(columns={\"0\": \"Sensor Description\",  \"1\": \"SensorNums\", \"2\" : \"Duration\", \"3\" : \"Plot\", \"4\" : \"Data Collection\", \"5\" : \"Data Available\"})\n",
    "# dfsg_m[['Start Date', 'End Date']] = dfsg_m['Data Available'].str.split('to', n=1, expand=True)\n",
    "# dfsg_m['Start Date'] = dfsg_m['Start Date'].str.strip()\n",
    "# dfsg_m['End Date'] = dfsg_m['End Date'].str.replace('present','01/01/2025').str.strip()\n",
    "# dfsg_m.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # left-join by streamgage metadata to streamgage site data\n",
    "\n",
    "# dfsg = pd.merge(dfsg, dfsg_m, left_on='siteid', right_on='siteid', how='left')\n",
    "# print(len(dfsg))\n",
    "# dfsg.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Active Snow Depth Sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already done\n",
    "\n",
    "# %%time\n",
    "# # get Active Snow Depth Sensors metadata\n",
    "\n",
    "# tempList = dfsd['ID'].tolist()\n",
    "# dftemp = pd.DataFrame()\n",
    "\n",
    "# for i in range(len(tempList)):\n",
    "#     idString = str(tempList[i]).strip()   \n",
    "#     url = \"https://cdec.water.ca.gov/dynamicapp/staMeta?station_id=\" + idString\n",
    "#     try:\n",
    "#         response = requests.get(url)\n",
    "#         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#         table = soup.find_all('table')\n",
    "#         rawData = pd.read_html(str(table))[1]\n",
    "#         rawData[\"ID\"] = idString\n",
    "#         dftemp = pd.concat([dftemp, rawData])\n",
    "#     except:\n",
    "#         print(f' did not work, {url}')\n",
    "\n",
    "# dftemp.to_csv('RawInputData/ActiveSnowDepthSesnsors_Metadata.zip', compression=dict(method='zip', archive_name='ActiveSnowDepthSesnsors_Metadata.csv'), index=False)\n",
    "\n",
    "# print(len(dftemp))\n",
    "# dftemp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input File: ActiveSnowDepthSesnsors_Metadata\n",
    "\n",
    "# fileInput = \"RawInputData/ActiveSnowDepthSesnsors_Metadata.zip\"\n",
    "# dfsd_m = pd.read_csv(fileInput).replace(np.nan, \"\")\n",
    "# print(len(dfsd_m))\n",
    "# dfsd_m.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean up ActiveSnowDepthSesnsors_Metadata\n",
    "\n",
    "# dfsd_m = dfsd_m.rename(columns={\"0\": \"Sensor Description\",  \"1\": \"SensorNums\", \"2\" : \"Duration\", \"3\" : \"Plot\", \"4\" : \"Data Collection\", \"5\" : \"Data Available\"})\n",
    "# dfsd_m[['Start Date', 'End Date']] = dfsd_m['Data Available'].str.split('to', n=1, expand=True)\n",
    "# dfsd_m['Start Date'] = dfsd_m['Start Date'].str.strip()\n",
    "# dfsd_m['End Date'] = dfsd_m['End Date'].str.replace('present','01/01/2025').str.strip()\n",
    "# dfsd_m.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # left-join by ActiveSnowDepthSesnsors_Metadata to site data\n",
    "\n",
    "# dfsd = pd.merge(dfsd, dfsd_m, left_on='ID', right_on='ID', how='left')\n",
    "# print(len(dfsd))\n",
    "# dfsd.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # abbreviate Duration \n",
    "\n",
    "# durationDict = {\n",
    "# \"(daily)\" : \"d\",\n",
    "# \"(monthly)\" : \"m\",\n",
    "# \"(event)\" : \"e\", \n",
    "# \"(hourly)\" : \"h\"\n",
    "# }\n",
    "\n",
    "# def CreateDurationAPIValueFunc(val):\n",
    "#     val = str(val).strip()\n",
    "#     try:\n",
    "#         outString = durationDict[val]\n",
    "#     except:\n",
    "#         outString = \"\"\n",
    "#     return outString\n",
    "\n",
    "# dfr['Duration_abb'] = dfr.apply(lambda row: CreateDurationAPIValueFunc(row['Duration']), axis=1)\n",
    "# dfsg['Duration_abb'] = dfsg.apply(lambda row: CreateDurationAPIValueFunc(row['Duration']), axis=1)\n",
    "# dfsd['Duration_abb'] = dfsd.apply(lambda row: CreateDurationAPIValueFunc(row['Duration']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # drop rows that do not contain monthly (m) or dailiy (d) data.\n",
    "\n",
    "# timestep = ['m', 'd']\n",
    "# dfr = dfr[dfr['Duration_abb'].isin(timestep)]\n",
    "# dfsg = dfsg[dfsg['Duration_abb'].isin(timestep)]\n",
    "# dfsd = dfsd[dfsd['Duration_abb'].isin(timestep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we only want the following Reservoir SesnorNums data...\n",
    "# # RESERVOIR ELEVATION, FEET, 6, (daily)\n",
    "# # RESERVOIR STORAGE, AF, 15, (daily), (monthly)\n",
    "# # RESERVOIR OUTFLOW, CFS, 23, (daily)\n",
    "# # RESERVOIR INFLOW, CFS, 76, (daily)\n",
    "\n",
    "# dfr['SensorNums'] = dfr['SensorNums'].fillna(0).astype(np.int64).astype(str)\n",
    "# dfr = dfr[dfr['SensorNums'].astype(str).isin(['6', '15', '23', '76'])]\n",
    "# dfr['SensorNums'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we only want the following streamgage SesnorNums data...\n",
    "# # FLOW, MEAN DAILY, CFS, 41, (daily)\n",
    "# # FLOW, FULL NATURAL, AF, 65, (monthly)\n",
    "# # FLOW, MONTHLY VOLUME, AF, 66, (monthly)\n",
    "\n",
    "# dfsg['SensorNums'] = dfsg['SensorNums'].fillna(0).astype(np.int64).astype(str)\n",
    "# dfsg = dfsg[dfsg['SensorNums'].astype(str).isin(['41', '65', '66'])]\n",
    "# dfsg['SensorNums'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we only want the following snow depth SesnorNums data...\n",
    "# # SNOW DEPTH, INCHES, 18, (daily)\n",
    "\n",
    "# dfsd['SensorNums'] = dfsd['SensorNums'].fillna(0).astype(np.int64).astype(str)\n",
    "# dfsd = dfsd[dfsd['SensorNums'].astype(str).isin(['18'])]\n",
    "# dfsd['SensorNums'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # get timeseries for reservoirs\n",
    "\n",
    "# stationsList = dfr['ID'].tolist()\n",
    "# sensorNumsList = dfr['SensorNums'].tolist()\n",
    "# dur_codeList =  dfr['Duration_abb'].tolist()\n",
    "# plotList = dfr['Plot'].tolist()\n",
    "# datacollectionList = dfr['Data Collection'].tolist()\n",
    "# startList = dfr['Start Date'].tolist()\n",
    "# endList = dfr['End Date'].tolist()\n",
    "\n",
    "# # Time Series Dataframe\n",
    "# dfr_ts = pd.DataFrame()\n",
    "\n",
    "# for i in range(len(stationsList)):\n",
    "#     stationStr = str(stationsList[i]).strip()\n",
    "#     sensorNumsStr = str(sensorNumsList[i]).strip()\n",
    "#     dur_codeStr = str(dur_codeList[i]).strip()\n",
    "#     plotStr = str(plotList[i]).strip()\n",
    "#     datacollectionStr = str(datacollectionList[i]).strip()\n",
    "#     startStr = str(startList[i]).strip()\n",
    "#     endStr = str(endList[i]).strip()   \n",
    "#     urlInput = \"https://cdec.water.ca.gov/dynamicapp/req/CSVDataServlet?Stations=\" + stationStr + \"&SensorNums=\" + sensorNumsStr + \"&dur_code=\" + dur_codeStr + \"&Start=\" + startStr +\"&End=\" + endStr\n",
    "#     try:\n",
    "#         tempdf = pd.read_csv(urlInput).replace(np.nan, \"\")\n",
    "#         dfr_ts = pd.concat([dfr_ts, tempdf])\n",
    "#         dfr_ts['Duration_abbe'] = dur_codeStr\n",
    "#         dfr_ts['Plot'] = plotStr\n",
    "#         dfr_ts['Data Collection'] = datacollectionStr\n",
    "#         dfr_ts['Start Date'] = startStr\n",
    "#         dfr_ts['End Date'] = endStr\n",
    "              \n",
    "#     except:\n",
    "#         print(\"...bad response\")\n",
    "\n",
    "# dfr_ts.to_csv('RawInputData/Reservoirs_timeseries.zip', compression=dict(method='zip', archive_name='Reservoirs_timeseries.csv'), index=False)\n",
    "# print(len(dfr_ts))\n",
    "# dfr_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # get timeseries for streamgages\n",
    "\n",
    "# dfsg['SensorNums'] = dfsg['SensorNums'].astype(int).astype(str)\n",
    "\n",
    "# stationsList = dfsg['siteid'].tolist()\n",
    "# sensorNumsList = dfsg['SensorNums'].tolist()\n",
    "# dur_codeList =  dfsg['Duration_abb'].tolist()\n",
    "# plotList = dfsg['Plot'].tolist()\n",
    "# datacollectionList = dfsg['Data Collection'].tolist()\n",
    "# startList = dfsg['Start Date'].tolist()\n",
    "# endList = dfsg['End Date'].tolist()\n",
    "\n",
    "# # Time Series Dataframe\n",
    "# dfsg_ts = pd.DataFrame()\n",
    "\n",
    "# for i in range(len(stationsList)):\n",
    "#     stationStr = str(stationsList[i]).strip()\n",
    "#     sensorNumsStr = str(sensorNumsList[i]).strip()\n",
    "#     dur_codeStr = str(dur_codeList[i]).strip()\n",
    "#     plotStr = str(plotList[i]).strip()\n",
    "#     datacollectionStr = str(datacollectionList[i]).strip()\n",
    "#     startStr = str(startList[i]).strip()\n",
    "#     endStr = str(endList[i]).strip()   \n",
    "#     urlInput = \"https://cdec.water.ca.gov/dynamicapp/req/CSVDataServlet?Stations=\" + stationStr + \"&SensorNums=\" + sensorNumsStr + \"&dur_code=\" + dur_codeStr + \"&Start=\" + startStr +\"&End=\" + endStr\n",
    "#     try:\n",
    "#         tempdf = pd.read_csv(urlInput).replace(np.nan, \"\")\n",
    "#         dfsg_ts = pd.concat([dfsg_ts, tempdf])\n",
    "#         dfsg_ts['Duration_abbe'] = dur_codeStr\n",
    "#         dfsg_ts['Plot'] = plotStr\n",
    "#         dfsg_ts['Data Collection'] = datacollectionStr\n",
    "#         dfsg_ts['Start Date'] = startStr\n",
    "#         dfsg_ts['End Date'] = endStr\n",
    "              \n",
    "#     except:\n",
    "#         print(\"...bad response\")\n",
    "\n",
    "# dfsg_ts.to_csv('RawInputData/StreamGages_timeseries.zip', compression=dict(method='zip', archive_name='StreamGages_timeseries.csv'), index=False)\n",
    "# print(len(dfsg_ts))\n",
    "# dfsg_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # get timeseries for ActiveSnowDepthSesnsors\n",
    "\n",
    "# dfsd['SensorNums'] = dfsd['SensorNums'].astype(int).astype(str)\n",
    "\n",
    "# stationsList = dfsd['ID'].tolist()\n",
    "# sensorNumsList = dfsd['SensorNums'].tolist()\n",
    "# dur_codeList =  dfsd['Duration_abb'].tolist()\n",
    "# plotList = dfsd['Plot'].tolist()\n",
    "# datacollectionList = dfsd['Data Collection'].tolist()\n",
    "# startList = dfsd['Start Date'].tolist()\n",
    "# endList = dfsd['End Date'].tolist()\n",
    "\n",
    "# # Time Series Dataframe\n",
    "# dfsd_ts = pd.DataFrame()\n",
    "\n",
    "# for i in range(len(stationsList)):\n",
    "#     stationStr = str(stationsList[i]).strip()\n",
    "#     sensorNumsStr = str(sensorNumsList[i]).strip()\n",
    "#     dur_codeStr = str(dur_codeList[i]).strip()\n",
    "#     plotStr = str(plotList[i]).strip()\n",
    "#     datacollectionStr = str(datacollectionList[i]).strip()\n",
    "#     startStr = str(startList[i]).strip()\n",
    "#     endStr = str(endList[i]).strip()   \n",
    "#     urlInput = \"https://cdec.water.ca.gov/dynamicapp/req/CSVDataServlet?Stations=\" + stationStr + \"&SensorNums=\" + sensorNumsStr + \"&dur_code=\" + dur_codeStr + \"&Start=\" + startStr +\"&End=\" + endStr\n",
    "#     try:\n",
    "#         tempdf = pd.read_csv(urlInput).replace(np.nan, \"\")\n",
    "#         dfsd_ts = pd.concat([dfsd_ts, tempdf])\n",
    "#         dfsd_ts['Duration_abbe'] = dur_codeStr\n",
    "#         dfsd_ts['Plot'] = plotStr\n",
    "#         dfsd_ts['Data Collection'] = datacollectionStr\n",
    "#         dfsd_ts['Start Date'] = startStr\n",
    "#         dfsd_ts['End Date'] = endStr\n",
    "              \n",
    "#     except:\n",
    "#         print(\"...bad response\")\n",
    "\n",
    "# dfsd_ts.to_csv('RawInputData/ActiveSnowDepthSesnsors_timeseries.zip', compression=dict(method='zip', archive_name='ActiveSnowDepthSesnsors_timeseries.csv'), index=False)\n",
    "# print(len(dfsd_ts))\n",
    "# dfsd_ts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WaDE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reservoir data\n",
    "# create output POD dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Data Assessment UUID\n",
    "df['WaDEUUID'] = dfin1['WaDEUUID']\n",
    "\n",
    "# Method Info\n",
    "df['in_MethodUUID'] = \"CAwsss_M1\"\n",
    "\n",
    "# Variable Info\n",
    "df['in_AggregationIntervalUnitCV'] = dfin1['Duration_abbe']\n",
    "df['in_AmountUnitCV'] = dfin1['UNITS']\n",
    "df['in_VariableCV'] = \"Water Supply\"\n",
    "\n",
    "# Organization Info\n",
    "df['in_OrganizationUUID'] = \"CAwsss_O1\"\n",
    "\n",
    "# WaterSource Info\n",
    "df['in_Geometry'] = \"\"\n",
    "df['in_GNISFeatureNameCV'] = \"\"\n",
    "df['in_WaterQualityIndicatorCV'] = \"\"\n",
    "df['in_WaterSourceName'] = \"WaDE Blank\" # need this for auto fill below\n",
    "df['in_WaterSourceNativeID'] = \"\" # auto fill in below\n",
    "df['in_WaterSourceTypeCV'] = \"Surface Water\" # need this for auto fill below\n",
    "\n",
    "# Site Info\n",
    "df['in_CoordinateAccuracy'] = \"\"\n",
    "df['in_CoordinateMethodCV'] = \"\"\n",
    "df['in_County'] = dfin1['County']\n",
    "df['in_EPSGCodeCV'] = 4326\n",
    "df['in_Geometry'] = \"\"\n",
    "df['in_GNISCodeCV'] = \"\"\n",
    "df['in_HUC12'] = \"\"\n",
    "df['in_HUC8'] = \"\"\n",
    "df['in_Latitude'] =dfin1['Latitude']\n",
    "df['in_Longitude'] = dfin1['Longitude']\n",
    "df['in_NHDNetworkStatusCV'] = \"\"\n",
    "df['in_NHDProductCV'] = \"\"\n",
    "df['in_PODorPOUSite'] = \"\"\n",
    "df['in_SiteName'] = dfin1['Station']\n",
    "df['in_SiteNativeID'] = dfin1['STATION_ID']\n",
    "df['in_SitePoint'] = \"\"\n",
    "df['in_SiteTypeCV'] = \"Reservoir\"\n",
    "df['in_StateCV'] = \"CA\"\n",
    "df['in_USGSSiteID'] = \"\"\n",
    "\n",
    "# Site VariableAmounts Info\n",
    "df['in_Amount'] = dfin1.VALUE.replace('---',0).replace('BRT',0).replace('ART',0).astype(float)\n",
    "df['in_AllocationCropDutyAmount'] = \"\"\n",
    "df['in_AssociatedNativeAllocationIDs'] = \"\"\n",
    "df['in_BeneficialUseCategory'] = dfin1['SENSOR_TYPE']\n",
    "df['in_CommunityWaterSupplySystem'] = \"\"\n",
    "df['in_CropTypeCV'] = \"\"\n",
    "df['in_CustomerTypeCV'] = \"\"\n",
    "df['in_DataPublicationDate'] = \"\"\n",
    "df['in_DataPublicationDOI'] = \"\"\n",
    "df['in_Geometry'] = \"\"\n",
    "df['in_IrrigatedAcreage'] = \"\"\n",
    "df['in_IrrigationMethodCV'] = \"\"\n",
    "df['in_PopulationServed'] = \"\"\n",
    "df['in_PowerGeneratedGWh'] = \"\"\n",
    "df['in_PowerType'] = \"\"\n",
    "df['in_PrimaryUseCategory'] = dfin1['SENSOR_TYPE']\n",
    "df['in_ReportYearCV'] =  dfin1['OBS DATE']\n",
    "df['in_SDWISIdentifier'] = \"\"\n",
    "df['in_TimeframeEnd'] = dfin1['OBS DATE']\n",
    "df['in_TimeframeStart'] = dfin1['OBS DATE']\n",
    "\n",
    "outdf1 = df.copy()\n",
    "outdf1 = outdf1.drop_duplicates().reset_index(drop=True)\n",
    "print(len(outdf1))\n",
    "outdf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stream gage data\n",
    "# create output POD dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Data Assessment UUID\n",
    "df['WaDEUUID'] = dfin2['WaDEUUID']\n",
    "\n",
    "# Method Info\n",
    "df['in_MethodUUID'] = \"CAwsss_M1\"\n",
    "\n",
    "# Variable Info\n",
    "df['in_AggregationIntervalUnitCV'] = dfin2['Duration_abbe']\n",
    "df['in_AmountUnitCV'] = dfin2['UNITS']\n",
    "df['in_VariableCV'] = \"Water Supply\"\n",
    "\n",
    "# Organization Info\n",
    "df['in_OrganizationUUID'] = \"CAwsss_O1\"\n",
    "\n",
    "# WaterSource Info\n",
    "df['in_Geometry'] = \"\"\n",
    "df['in_GNISFeatureNameCV'] = \"\"\n",
    "df['in_WaterQualityIndicatorCV'] = \"\"\n",
    "df['in_WaterSourceName'] = \"WaDE Blank\" # need this for auto fill below\n",
    "df['in_WaterSourceNativeID'] = \"\" # auto fill in below\n",
    "df['in_WaterSourceTypeCV'] = \"Surface Water\" # need this for auto fill below\n",
    "\n",
    "# Site Info\n",
    "df['in_CoordinateAccuracy'] = \"\"\n",
    "df['in_CoordinateMethodCV'] = \"\"\n",
    "df['in_County'] = \"\"\n",
    "df['in_EPSGCodeCV'] = 4326\n",
    "df['in_Geometry'] = \"\"\n",
    "df['in_GNISCodeCV'] = \"\"\n",
    "df['in_HUC12'] = dfin2['huc12']\n",
    "df['in_HUC8'] = dfin2['huc8']\n",
    "df['in_Latitude'] = dfin2['wade_Latit']\n",
    "df['in_Longitude'] = dfin2['wade_Longi']\n",
    "df['in_NHDNetworkStatusCV'] = \"\"\n",
    "df['in_NHDProductCV'] = \"\"\n",
    "df['in_PODorPOUSite'] = \"\"\n",
    "df['in_SiteName'] = dfin2['sitename']\n",
    "df['in_SiteNativeID'] = dfin2['siteid']\n",
    "df['in_SitePoint'] = \"\"\n",
    "df['in_SiteTypeCV'] = \"Stream Gage\"\n",
    "df['in_StateCV'] = \"CA\"\n",
    "df['in_USGSSiteID'] = \"\"\n",
    "\n",
    "# Site VariableAmounts Info\n",
    "df['in_Amount'] = dfin2.VALUE.replace('---',0).replace('BRT',0).replace('ART',0).astype(float)\n",
    "df['in_AllocationCropDutyAmount'] = \"\"\n",
    "df['in_AssociatedNativeAllocationIDs'] = \"\"\n",
    "df['in_BeneficialUseCategory'] = dfin2['SENSOR_TYPE']\n",
    "df['in_CommunityWaterSupplySystem'] = \"\"\n",
    "df['in_CropTypeCV'] = \"\"\n",
    "df['in_CustomerTypeCV'] = \"\"\n",
    "df['in_DataPublicationDate'] = \"\"\n",
    "df['in_DataPublicationDOI'] = \"\"\n",
    "df['in_Geometry'] = \"\"\n",
    "df['in_IrrigatedAcreage'] = \"\"\n",
    "df['in_IrrigationMethodCV'] = \"\"\n",
    "df['in_PopulationServed'] = \"\"\n",
    "df['in_PowerGeneratedGWh'] = \"\"\n",
    "df['in_PowerType'] = \"\"\n",
    "df['in_PrimaryUseCategory'] = dfin2['SENSOR_TYPE']\n",
    "df['in_ReportYearCV'] =  dfin2['OBS DATE']\n",
    "df['in_SDWISIdentifier'] = \"\"\n",
    "df['in_TimeframeEnd'] = dfin2['OBS DATE']\n",
    "df['in_TimeframeStart'] = dfin2['OBS DATE']\n",
    "\n",
    "outdf2 = df.copy()\n",
    "outdf2 = outdf2.drop_duplicates().reset_index(drop=True)\n",
    "print(len(outdf2))\n",
    "outdf2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snow depth data\n",
    "# create output POD dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Data Assessment UUID\n",
    "df['WaDEUUID'] = dfin3['WaDEUUID']\n",
    "\n",
    "# Method Info\n",
    "df['in_MethodUUID'] = \"CAwsss_M1\"\n",
    "\n",
    "# Variable Info\n",
    "df['in_AggregationIntervalUnitCV'] = dfin3['Duration_abbe']\n",
    "df['in_AmountUnitCV'] = dfin3['UNITS']\n",
    "df['in_VariableCV'] = \"Water Supply\"\n",
    "\n",
    "# Organization Info\n",
    "df['in_OrganizationUUID'] = \"CAwsss_O1\"\n",
    "\n",
    "# WaterSource Info\n",
    "df['in_Geometry'] = \"\"\n",
    "df['in_GNISFeatureNameCV'] = \"\"\n",
    "df['in_WaterQualityIndicatorCV'] = \"\"\n",
    "df['in_WaterSourceName'] = \"WaDE Blank\" # need this for auto fill below\n",
    "df['in_WaterSourceNativeID'] = \"\" # auto fill in below\n",
    "df['in_WaterSourceTypeCV'] = \"Surface Water\" # need this for auto fill below\n",
    "\n",
    "# Site Info\n",
    "df['in_CoordinateAccuracy'] = \"\"\n",
    "df['in_CoordinateMethodCV'] = \"\"\n",
    "df['in_County'] = \"\"\n",
    "df['in_EPSGCodeCV'] = 4326\n",
    "df['in_Geometry'] = \"\"\n",
    "df['in_GNISCodeCV'] = \"\"\n",
    "df['in_HUC12'] = \"\"\n",
    "df['in_HUC8'] = \"\"\n",
    "df['in_Latitude'] = dfin3['LATITUDE']\n",
    "df['in_Longitude'] = dfin3['LONGITUDE']\n",
    "df['in_NHDNetworkStatusCV'] = \"\"\n",
    "df['in_NHDProductCV'] = \"\"\n",
    "df['in_PODorPOUSite'] = \"\"\n",
    "df['in_SiteName'] = dfin3['STATION']\n",
    "df['in_SiteNativeID'] = dfin3['STATION_ID']\n",
    "df['in_SitePoint'] = \"\"\n",
    "df['in_SiteTypeCV'] = \"Snow Depth\"\n",
    "df['in_StateCV'] = \"CA\"\n",
    "df['in_USGSSiteID'] = \"\"\n",
    "\n",
    "# Site VariableAmounts Info\n",
    "df['in_Amount'] = dfin3.VALUE.replace('---',0).replace('BRT',0).replace('ART',0).astype(float)\n",
    "df['in_AllocationCropDutyAmount'] = \"\"\n",
    "df['in_AssociatedNativeAllocationIDs'] = \"\"\n",
    "df['in_BeneficialUseCategory'] = dfin3['SENSOR_TYPE']\n",
    "df['in_CommunityWaterSupplySystem'] = \"\"\n",
    "df['in_CropTypeCV'] = \"\"\n",
    "df['in_CustomerTypeCV'] = \"\"\n",
    "df['in_DataPublicationDate'] = \"\"\n",
    "df['in_DataPublicationDOI'] = \"\"\n",
    "df['in_Geometry'] = \"\"\n",
    "df['in_IrrigatedAcreage'] = \"\"\n",
    "df['in_IrrigationMethodCV'] = \"\"\n",
    "df['in_PopulationServed'] = \"\"\n",
    "df['in_PowerGeneratedGWh'] = \"\"\n",
    "df['in_PowerType'] = \"\"\n",
    "df['in_PrimaryUseCategory'] = dfin3['SENSOR_TYPE']\n",
    "df['in_ReportYearCV'] =  dfin3['OBS DATE']\n",
    "df['in_SDWISIdentifier'] = \"\"\n",
    "df['in_TimeframeEnd'] = dfin3['OBS DATE']\n",
    "df['in_TimeframeStart'] = dfin3['OBS DATE']\n",
    "\n",
    "outdf3 = df.copy()\n",
    "outdf3 = outdf3.drop_duplicates().reset_index(drop=True)\n",
    "print(len(outdf3))\n",
    "outdf3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Concatenate dataframes\n",
    "frames = [outdf1, outdf2, outdf3]\n",
    "outdf = pd.concat(frames)\n",
    "outdf = outdf.drop_duplicates().reset_index(drop=True).replace(np.nan, \"\")\n",
    "print(len(outdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data / data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fix AggregationIntervalUnit input\n",
    "\n",
    "intervalDict = {\n",
    "    \"d\" : \"Daily\",\n",
    "    \"m\" : \"Monthly\",\n",
    "}\n",
    "def FixAggregationIntervalUnit(val):\n",
    "    val = str(val).strip()\n",
    "    try:\n",
    "        outString = intervalDict[val]\n",
    "    except:\n",
    "        outString = \"NULL\"\n",
    "    return outString\n",
    "\n",
    "outdf['in_AggregationIntervalUnitCV'] = outdf.apply(lambda row: FixAggregationIntervalUnit(row['in_AggregationIntervalUnitCV']), axis=1)\n",
    "outdf['in_AggregationIntervalUnitCV'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fix PrimaryUseCategoryCV input\n",
    "# This dictionary is an estimated translation of the metadata from CA's systems\n",
    "\n",
    "PrimaryUseCategoryCVDict = {\n",
    "\"% UP 01\" : \"Percentage of Upper 01 (possibly related to a specific upper layer or zone)\",\n",
    "\"% UP 02\" : \"Percentage of Upper 02\",\n",
    "\"% UP 03\" : \"Percentage of Upper 03\",\n",
    "\"% UP 04\" : \"Percentage of Upper 04\",\n",
    "\"% UP 05\" : \"Percentage of Upper 05\",\n",
    "\"% UP 06\" : \"Percentage of Upper 06\",\n",
    "\"% UP 07\" : \"Percentage of Upper 07\",\n",
    "\"% UP 08\" : \"Percentage of Upper 08\",\n",
    "\"% UP 09\" : \"Percentage of Upper 09\",\n",
    "\"% UP 10\" : \"Percentage of Upper 10\",\n",
    "\"%Q\" : \"Percentage of Discharge Flow\",\n",
    "\"10day%Q\" : \"10-day Percentage of Flow Discharge\",\n",
    "\"10dayQ\" : \"10-day Flow Discharge\",\n",
    "\"1day%Q\" : \"1-day Percentage of Flow Discharge\",\n",
    "\"1DAYQ\" : \"1-day Flow Discharge\",\n",
    "\"ABV TOC\" : \"Above Top of Conservation\",\n",
    "\"AJ 10%\" : \"Adjusted 10% Exceedance\",\n",
    "\"AJ 50%\" : \"Adjusted 50% Exceedance\",\n",
    "\"AJ 90%\" : \"Adjusted 90% Exceedance\",\n",
    "\"AUXFLOW\" : \"Auxiliary Flow\",\n",
    "\"AVG INF\" : \"Average Inflow\",\n",
    "\"BAR PRE\" : \"Barometric Pressure\",\n",
    "\"BAT VOL\" : \"Battery Voltage\",\n",
    "\"BAT VOLA\" : \"Battery Voltage A\",\n",
    "\"CHLORPH\" : \"Chlorophyll\",\n",
    "\"CONTROL\" : \"Control Flow or Parameter\",\n",
    "\"D ORGCO\" : \"Dissolved Organic Carbon\",\n",
    "\"D ORGCZ\" : \"Dissolved Organic Carbon Zone\",\n",
    "\"DC PUMP\" : \"Direct Current Pump\",\n",
    "\"DEW PT\" : \"Dew Point\",\n",
    "\"DIS OXY\" : \"Dissolved Oxygen\",\n",
    "\"DIS PWR\" : \"Discharge Power\",\n",
    "\"Diss Br\" : \"Dissolved Bromide\",\n",
    "\"Diss Cl\" : \"Dissolved Chloride\",\n",
    "\"Diss F\" : \"Dissolved Fluoride\",\n",
    "\"DissNO3\" : \"Dissolved Nitrate\",\n",
    "\"DissPO4\" : \"Dissolved Phosphate\",\n",
    "\"DissSO4\" : \"Dissolved Sulfate\",\n",
    "\"DIVERSN\" : \"Diversion\",\n",
    "\"DIVERSN\" : \"Diversion Flow\",\n",
    "\"DO MAX\" : \"Maximum Dissolved Oxygen\",\n",
    "\"DO MDN\" : \"Median Dissolved Oxygen\",\n",
    "\"DO MIN\" : \"Minimum Dissolved Oxygen\",\n",
    "\"E T\" : \"Evapotranspiration\",\n",
    "\"EC MAX\" : \"Maximum Electrical Conductivity\",\n",
    "\"EC MDN\" : \"Median Electrical Conductivity\",\n",
    "\"EC MIN\" : \"Minimum Electrical Conductivity\",\n",
    "\"EL CND\" : \"Electrical Conductivity (Generic)\",\n",
    "\"EL COND\" : \"Electrical Conductivity\",\n",
    "\"EL CONDB\" : \"Electrical Conductivity (Backup)\",\n",
    "\"EVAP\" : \"Evaporation\",\n",
    "\"EVP PAN\" : \"Evaporation Pan\",\n",
    "\"FDOM\" : \"Fluorescent Dissolved Organic Matter\",\n",
    "\"FGAMRVL\" : \"Flow Gauge Manual River Level\",\n",
    "\"FLOW\" : \"Flow Rate\",\n",
    "\"FLOW.XX\" : \"Flow Rate (specific sensor or parameter)\",\n",
    "\"FNF ACC\" : \"Forecast Natural Flow Accumulation\",\n",
    "\"FNF\" : \"Forecasted Natural Flow\",\n",
    "\"FOUTFLW\" : \"Forecast Outflow\",\n",
    "\"FTEMPVL\" : \"Forecast Temperature Value\",\n",
    "\"FTOCSTO\" : \"Forecast to Storage\",\n",
    "\"HEAD HT\" : \"Head Height\",\n",
    "\"INFLOW\" : \"Inflow\",\n",
    "\"IRR&CNS\" : \"Irrigation and Conservation\",\n",
    "\"LK EVAP\" : \"Lake Evaporation\",\n",
    "\"M FLOW\" : \"Mean Flow\",\n",
    "\"MON FLO\" : \"Monthly Flow\",\n",
    "\"MON FNF\" : \"Monthly Forecasted Natural Flow\",\n",
    "\"NSLR AV\" : \"Net Solar Radiation Average\",\n",
    "\"NSLR MN\" : \"Net Solar Radiation Minimum\",\n",
    "\"NSLR MX\" : \"Net Solar Radiation Maximum\",\n",
    "\"OUTFLOW\" : \"Outflow\",\n",
    "\"OUTFLWV\" : \"Outflow Volume\",\n",
    "\"PEAK WD\" : \"Peak Wind Direction\",\n",
    "\"PEAK WS\" : \"Peak Wind Speed\",\n",
    "\"PH VAL\" : \"pH Value\",\n",
    "\"PPT INC\" : \"Precipitation Increment\",\n",
    "\"PPTINC4\" : \"Precipitation Increment (4-hour)\",\n",
    "\"RAIN\" : \"Rainfall\",\n",
    "\"RAINTIP\" : \"Rain Tip Gauge\",\n",
    "\"REL HUM\" : \"Relative Humidity\",\n",
    "\"REL SCH\" : \"Release Schedule\",\n",
    "\"RES CHG\" : \"Reservoir Change\",\n",
    "\"RES ELE\" : \"Reservoir Elevation\",\n",
    "\"RGAMRVL\" : \"River Gauge Manual River Level\",\n",
    "\"RIV REL\" : \"River Release\",\n",
    "\"RIV STG\" : \"River Stage\",\n",
    "\"RIVST29\" : \"River Stage at Station 29\",\n",
    "\"RIVST88\" : \"River Stage at Station 88\",\n",
    "\"RIVSTGA\" : \"River Stage Gauge\",\n",
    "\"RTEMPVL\" : \"Real-Time Temperature Value\",\n",
    "\"SLRR AV\" : \"Solar Radiation Average\",\n",
    "\"SLRR IN\" : \"Solar Radiation Incoming\",\n",
    "\"SLRR MN\" : \"Solar Radiation Minimum\",\n",
    "\"SLRR MX\" : \"Solar Radiation Maximum\",\n",
    "\"SLRRREF\" : \"Solar Radiation Reference\",\n",
    "\"SNO ADJ\" : \"Snow Adjustment\",\n",
    "\"SNOW DP\" : \"Snow Depth\",\n",
    "\"SNOW WC\" : \"Snow Water Content\",\n",
    "\"SOIL TP\" : \"Soil Temperature\",\n",
    "\"SOILMD1\" : \"Soil Moisture Depth 1\",\n",
    "\"SOILMD2\" : \"Soil Moisture Depth 2\",\n",
    "\"SOILMD3\" : \"Soil Moisture Depth 3\",\n",
    "\"SOILTD1\" : \"Soil Temperature Depth 1\",\n",
    "\"SOILTD2\" : \"Soil Temperature Depth 2\",\n",
    "\"SOILTD3\" : \"Soil Temperature Depth 3\",\n",
    "\"SOLAR R\" : \"Solar Radiation\",\n",
    "\"SPILL\" : \"Spill Rate\",\n",
    "\"STAGE F\" : \"Stage Flow\",\n",
    "\"STORAGE\" : \"Storage Volume\",\n",
    "\"T ORG C\" : \"Total Organic Carbon\",\n",
    "\"T ORGCZ\" : \"Total Organic Carbon Zone\",\n",
    "\"TEMP AV\" : \"Average Temperature\",\n",
    "\"TEMP MN\" : \"Minimum Temperature\",\n",
    "\"TEMP MX\" : \"Maximum Temperature\",\n",
    "\"TEMP W\" : \"Water Temperature\",\n",
    "\"TEMP\" : \"Air Temperature\",\n",
    "\"TEMPIDX\" : \"Temperature Index\",\n",
    "\"TEMPW C\" : \"Water Temperature in Celsius\",\n",
    "\"TMPW MAX\" : \"Maximum Water Temperature\",\n",
    "\"TMPW MDN\" : \"Median Water Temperature\",\n",
    "\"TMPW MIN\" : \"Minimum Water Temperature\",\n",
    "\"TOC STO\" : \"Top of Conservation Storage\",\n",
    "\"TURB W\" : \"Turbidity in Water\",\n",
    "\"TURB WF\" : \"Turbidity Flow Rate\",\n",
    "\"TURBVAR\" : \"Turbidity Variance\",\n",
    "\"VLOCITY\" : \"Velocity (Flow Speed)\",\n",
    "\"WIND DR\" : \"Wind Direction\",\n",
    "\"WIND SP\" : \"Wind Speed\",\n",
    "\"WINDLEN\" : \"Wind Length\",\n",
    "\"WY 10%\" : \"Water Year 10% Exceedance\",\n",
    "\"WY 50%\" : \"Water Year 50% Exceedance\",\n",
    "\"WY 90%\" : \"Water Year 90% Exceedance\"\n",
    "}\n",
    "\n",
    "def FixPrimaryUseCategoryCVFunc(val):\n",
    "    val = str(val).strip()\n",
    "    try:\n",
    "        outString = PrimaryUseCategoryCVDict[val]\n",
    "    except:\n",
    "        if val == \"\":\n",
    "            outString = \"NULL\"\n",
    "        else:\n",
    "            outString = val\n",
    "    return outString\n",
    "\n",
    "\n",
    "outdf['in_BeneficialUseCategory'] = outdf.apply(lambda row: FixPrimaryUseCategoryCVFunc(row['in_BeneficialUseCategory']), axis=1)\n",
    "outdf['in_PrimaryUseCategory'] = outdf.apply(lambda row: FixPrimaryUseCategoryCVFunc(row['in_PrimaryUseCategory']), axis=1)\n",
    "outdf['in_PrimaryUseCategory'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clean name entries of spcial characters\n",
    "def removeSpecialCharsFunc(Val):\n",
    "    Val = str(Val)\n",
    "    Val = re.sub(\"[$@&.;/\\)(-]\", \"\", Val).title().replace(\"  \", \" \").strip().rstrip(',')\n",
    "    return Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outdf['in_WaterSourceName'] = outdf.apply(lambda row: removeSpecialCharsFunc(row['in_WaterSourceName']), axis=1)\n",
    "outdf['in_WaterSourceName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outdf['in_SiteName'] = outdf.apply(lambda row: removeSpecialCharsFunc(row['in_SiteName']), axis=1)\n",
    "outdf['in_SiteName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outdf['in_County'] = outdf.apply(lambda row: removeSpecialCharsFunc(row['in_County']), axis=1)\n",
    "outdf['in_County'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ensure Empty String / remove string value of \"nan\"\n",
    "\n",
    "def ensureEmptyString(val):\n",
    "    val = str(val).strip()\n",
    "    if val == \"\" or val == \" \" or val == \"nan\" or pd.isnull(val):\n",
    "        outString = \"\"\n",
    "    else:\n",
    "        outString = val\n",
    "    return outString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outdf['in_WaterSourceName'] = outdf.apply(lambda row: ensureEmptyString(row['in_WaterSourceName']), axis=1)\n",
    "outdf['in_WaterSourceName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outdf['in_WaterSourceTypeCV'] = outdf.apply(lambda row: ensureEmptyString(row['in_WaterSourceTypeCV']), axis=1)\n",
    "outdf['in_WaterSourceTypeCV'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outdf['in_SiteTypeCV'] = outdf.apply(lambda row: ensureEmptyString(row['in_SiteTypeCV']), axis=1)\n",
    "outdf['in_SiteTypeCV'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outdf['in_SiteName'] = outdf.apply(lambda row: ensureEmptyString(row['in_SiteName']), axis=1)\n",
    "outdf['in_SiteName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outdf['in_County'] = outdf.apply(lambda row: ensureEmptyString(row['in_County']), axis=1)\n",
    "outdf['in_County'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outdf['in_BeneficialUseCategory'] = outdf.apply(lambda row: ensureEmptyString(row['in_BeneficialUseCategory']), axis=1)\n",
    "uniqueList = list(set([i.strip() for i in ','.join(outdf['in_BeneficialUseCategory'].astype(str)).split(',')]))\n",
    "uniqueList.sort()\n",
    "uniqueList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ensure Latitude entry is numireic, replace '0' values for removal\n",
    "outdf['in_Latitude'] = pd.to_numeric(outdf['in_Latitude'], errors='coerce').replace(0,\"\").fillna(\"\")\n",
    "outdf['in_Latitude'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ensure Longitude entry is numireic, replace '0' values for removal\n",
    "outdf['in_Longitude'] = pd.to_numeric(outdf['in_Longitude'], errors='coerce').replace(0,\"\").fillna(\"\")\n",
    "outdf['in_Longitude'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ensure Amount entry is either numireic or blank, no 0 entries\n",
    "outdf['in_Amount'] = pd.to_numeric(outdf['in_Amount'], errors='coerce').round(2).replace(0,\"\").fillna(\"\")\n",
    "outdf['in_Amount'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ensure PopulationServed entry is numireic WITH 0 entries (no blank strings)\n",
    "outdf['in_PopulationServed'] = pd.to_numeric(outdf['in_PopulationServed'], errors='coerce').round().replace(\"\",0).fillna(0).astype(int).replace(0,\"\").fillna(\"\")\n",
    "outdf['in_PopulationServed'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert TimeframeEnd to YYYY-MM-DD format.\n",
    "outdf['in_TimeframeEnd'] = pd.to_datetime(outdf['in_TimeframeEnd'], utc=True, errors = 'coerce').fillna(\"\")\n",
    "outdf['in_TimeframeEnd'] = pd.to_datetime(outdf[\"in_TimeframeEnd\"].dt.strftime('%m/%d/%Y'))\n",
    "outdf['in_TimeframeEnd'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert TimeframeStart to YYYY-MM-DD format.\n",
    "outdf['in_TimeframeStart'] = pd.to_datetime(outdf['in_TimeframeStart'], utc=True, errors = 'coerce').fillna(\"\")\n",
    "outdf['in_TimeframeStart'] = pd.to_datetime(outdf[\"in_TimeframeStart\"].dt.strftime('%m/%d/%Y'))\n",
    "outdf['in_TimeframeStart'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract year out\n",
    "outdf['in_ReportYearCV'] = pd.to_datetime(outdf['in_ReportYearCV'], utc=True, errors = 'coerce').fillna(\"\")\n",
    "outdf['in_ReportYearCV'] = pd.to_datetime(outdf[\"in_ReportYearCV\"].dt.strftime('%m/%d/%Y'))\n",
    "outdf['in_ReportYearCV'] = outdf['in_ReportYearCV'].dt.year\n",
    "outdf['in_ReportYearCV'] = outdf['in_ReportYearCV'].fillna(0).astype(int)\n",
    "outdf['in_ReportYearCV'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Assign Primary Use Category\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"C:/Users/rjame/Documents/WSWC Documents/MappingStatesDataToWaDE2.0/5_CustomFunctions/AssignPrimaryUseCategory\")\n",
    "# import AssignPrimaryUseCategoryFile # Use Custom import file\n",
    "\n",
    "# outdf['in_PrimaryUseCategory'] = outdf.apply(lambda row: AssignPrimaryUseCategoryFile.retrievePrimaryUseCategory(row['in_BeneficialUseCategory']), axis=1)\n",
    "# outdf['in_PrimaryUseCategory'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change in_TimeframeEnd to be months end if a monthly value\n",
    "import datetime\n",
    "import calendar\n",
    "\n",
    "def MonthsEndFunc(date, timestep):\n",
    "    timestep = str(timestep).strip()\n",
    "    if timestep == \"Monthly\":\n",
    "        last_day_of_month  = calendar.monthrange(date.year, date.month)[1]\n",
    "        outString = date.replace(day=last_day_of_month)\n",
    "    else:\n",
    "        outString = date\n",
    "    return outString\n",
    "\n",
    "outdf['in_TimeframeEnd'] = outdf.apply(lambda row: MonthsEndFunc(row['in_TimeframeEnd'], row['in_AggregationIntervalUnitCV']), axis=1)\n",
    "outdf['in_TimeframeEnd'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating WaDE Custom VariableSpecificCV\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "def createVariableSpecificCV(inV, inAIU, inPU, inWST):\n",
    "    inV = str(inV).strip()\n",
    "    inAIU = str(inAIU).strip()\n",
    "    inPU = str(inPU).strip().title()\n",
    "    inWST = str(inWST).strip()\n",
    "    outString = inV + \"_\" + inAIU + \"_\" + inPU + \"_\" + inWST\n",
    "    return outString\n",
    "\n",
    "outdf['in_VariableSpecificCV'] = outdf.apply(lambda row: createVariableSpecificCV(row['in_VariableCV'], \n",
    "                                                                                  row['in_AggregationIntervalUnitCV'],\n",
    "                                                                                  row['in_PrimaryUseCategory'],\n",
    "                                                                                  row['in_WaterSourceTypeCV']), axis=1)\n",
    "outdf['in_VariableSpecificCV'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating WaDE Custom water source native ID for easy water source identification\n",
    "# use unique WaterSourceName and WaterSourceType values\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create temp in_WaterSourceNativeID dataframe of unique water source.\n",
    "def assignIdValueFunc(colRowValue):\n",
    "    string1 = str(colRowValue)\n",
    "    outstring = \"wadeId\" + string1\n",
    "    return outstring\n",
    "\n",
    "dfTempID = pd.DataFrame()\n",
    "dfTempID['in_WaterSourceName'] = outdf['in_WaterSourceName'].astype(str).str.strip()\n",
    "dfTempID['in_WaterSourceTypeCV'] = outdf['in_WaterSourceTypeCV'].astype(str).str.strip()\n",
    "dfTempID = dfTempID.drop_duplicates()\n",
    "\n",
    "dfTempCount = pd.DataFrame(index=dfTempID.index)\n",
    "dfTempCount[\"Count\"] = range(1, len(dfTempCount.index) + 1)\n",
    "dfTempID['in_WaterSourceNativeID'] = dfTempCount.apply(lambda row: assignIdValueFunc(row['Count']), axis=1)\n",
    "dfTempID['linkKey'] = dfTempID['in_WaterSourceName'].astype(str) + dfTempID['in_WaterSourceTypeCV'].astype(str)\n",
    "IdDict = pd.Series(dfTempID.in_WaterSourceNativeID.values, index=dfTempID.linkKey.astype(str)).to_dict()\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Retreive WaDE Custom site native ID\n",
    "def retrieveIdValueFunc(checkVal, valA, valB):\n",
    "    checkVal = str(checkVal).strip()\n",
    "    if checkVal == \"\":\n",
    "        linkKeyVal = str(valA).strip() + str(valB).strip()\n",
    "        outString = IdDict[linkKeyVal]\n",
    "    else:\n",
    "        outString = checkVal\n",
    "    return outString\n",
    "\n",
    "outdf['in_WaterSourceNativeID'] = outdf.apply(lambda row: retrieveIdValueFunc(row['in_WaterSourceNativeID'], \n",
    "                                                                              row['in_WaterSourceName'], row['in_WaterSourceTypeCV']), axis=1)\n",
    "outdf['in_WaterSourceNativeID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating WaDE Custom site native ID for easy site identification\n",
    "# use Unique Latitude, Longitude, SiteName and SiteTypeCV values\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create temp in_SiteNativeID dataframe of unique water source.\n",
    "def assignIdValueFunc(colRowValue):\n",
    "    string1 = str(colRowValue)\n",
    "    outstring = \"wadeId\" + string1\n",
    "    return outstring\n",
    "\n",
    "dfTempID = pd.DataFrame()\n",
    "dfTempID['in_Latitude'] = outdf['in_Latitude'].astype(str).str.strip()\n",
    "dfTempID['in_Longitude'] = outdf['in_Longitude'].astype(str).str.strip()\n",
    "dfTempID['in_SiteName'] = outdf['in_SiteName'].astype(str).str.strip()\n",
    "dfTempID['in_SiteTypeCV'] = outdf['in_SiteTypeCV'].astype(str).str.strip()\n",
    "dfTempID = dfTempID.drop_duplicates()\n",
    "\n",
    "dfTempCount = pd.DataFrame(index=dfTempID.index)\n",
    "dfTempCount[\"Count\"] = range(1, len(dfTempCount.index) + 1)\n",
    "dfTempID['in_SiteNativeID'] = dfTempCount.apply(lambda row: assignIdValueFunc(row['Count']), axis=1)\n",
    "dfTempID['linkKey'] = dfTempID['in_Latitude'].astype(str) + dfTempID['in_Longitude'].astype(str) + dfTempID['in_SiteName'].astype(str)+ dfTempID['in_SiteTypeCV'].astype(str)\n",
    "IdDict = pd.Series(dfTempID.in_SiteNativeID.values, index=dfTempID.linkKey.astype(str)).to_dict()\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Retreive WaDE Custom site native ID\n",
    "def retrieveIdValueFunc(checkVal, valA, valB, valC, valD):\n",
    "    checkVal = str(checkVal).strip()\n",
    "    if checkVal == \"\":\n",
    "        linkKeyVal = str(valA).strip() + str(valB).strip() + str(valC).strip() + str(valD).strip()\n",
    "        outString = IdDict[linkKeyVal]\n",
    "    else:\n",
    "        outString = checkVal\n",
    "    return outString\n",
    "\n",
    "outdf['in_SiteNativeID'] = outdf.apply(lambda row: retrieveIdValueFunc(row['in_SiteNativeID'], \n",
    "                                                                       row['in_Latitude'], row['in_Longitude'],\n",
    "                                                                       row['in_SiteName'], row['in_SiteTypeCV']), axis=1)\n",
    "outdf['in_SiteNativeID'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Export the output dataframe\n",
    "outdf.to_csv('RawInputData/Pwsss_caMain.zip', compression=dict(method='zip', archive_name='Pwsss_caMain.csv'), index=False)  # The output, save as a zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
