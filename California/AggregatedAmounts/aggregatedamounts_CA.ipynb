{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working directory\n",
    "working_dir = \"./ProcessedInputData/\"\n",
    "os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = [\"OrganizationUUID\", \"VariableSpecificUUID\", \n",
    "                  \"ReportingUnitUUID\",\n",
    "                  \"PrimaryUseCategory\", \"BeneficialUseCategory\", \n",
    "                  \"WaterSourceUUID\", \"MethodUUID\", \"TimeframeStart\", \"TimeframeEnd\", \n",
    "                  \"DataPublicationDate\", \"DataPublicationDOI\", \"ReportYearCV\", \"Amount\",  \n",
    "                  \"PopulationServed\", \"PowerGeneratedGWh\", \"IrrigatedAcreage\",\n",
    "                  \"InterbasinTransferToID\", \"InterbasinTransferFromID\", \"CustomerTypeCV\",\n",
    "                  \"AllocationCropDutyAmount\", \"IrrigationMethodCV\", \"CropTypeCV\",\n",
    "                  \"CommunityWaterSupplySystem\", \"SDWISIdentifierCV\", \"PowerType\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf100 = pd.DataFrame(columns=target_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input files\n",
    "fileInput1_prefix = \"CA-DWR-WaterBalance-Level2-DP-1000-\"\n",
    "fileInput1_postfix = \"-DAUCO.csv\"\n",
    "\n",
    "# reporting units\n",
    "# ---- no need here ---- inp_repunts = 'reportingunits.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading input...\")\n",
    "\n",
    "# combine data from multiple files to one dataFrame\n",
    "startYear = 2011\n",
    "endYear = 2015\n",
    "numYears = 5\n",
    "yearList = np.linspace(startYear, endYear, numYears)\n",
    "df500_list = []\n",
    "for isx in range (numYears):\n",
    "    fileInput1 = fileInput1_prefix + str(int(yearList[isx])) + fileInput1_postfix\n",
    "    df50 = pd.read_csv(fileInput1,encoding = \"ISO-8859-1\") #, usecols =input_owner_cols) \n",
    "    df500_list.append(df50)\n",
    "    \n",
    "df500 = pd.concat(df500_list, sort=True, ignore_index=True)\n",
    "\n",
    "df500.drop_duplicates(inplace=True)\n",
    "print(len(df500.index))\n",
    "\n",
    "#df500.head(5)\n",
    "df500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sum/Aggregate amount by Ben use, DAU, and Year...\")\n",
    "\n",
    "df200 = df500.groupby(['CategoryA', 'DAU', 'Year']).agg({'KAcreFt': sum})\n",
    "df200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "df200KA = pd.DataFrame(columns=['KAcreFt'])\n",
    "df200KA['KAcreFt'] = df200['KAcreFt']\n",
    "df200KA = df200KA.reset_index(drop=True)\n",
    "df200KA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change units to AF from KAF\n",
    "\n",
    "print(\"Adjust units to AF...\")\n",
    "\n",
    "df200KA = df200KA.assign(AmountAF=np.nan)\n",
    "df200KA['AmountAF'] = df200KA.apply(lambda row: float(row['KAcreFt'])/1000.0, axis=1)\n",
    "df200KA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Find unique rows by Ben use, DAU, and Year...\")\n",
    "\n",
    "df100 = df500.drop_duplicates(subset=['CategoryA', 'DAU', 'Year'])\n",
    "df100 = df100.reset_index(drop=True)\n",
    "df100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine\n",
    "df100['AmountAF'] = df200KA['AmountAF']\n",
    "df100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"reporting units...\")\n",
    "\n",
    "#ReportingUnitUUID\tCA_DAU\n",
    "\n",
    "df100 = df100.assign(ReportingUnitUUID='')\n",
    "df100['ReportingUnitUUID'] = df100.apply(lambda row: '_'.join([\"CA\", str(row[\"DAU\"])]), axis=1)\n",
    "df100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Copying Columns...\")\n",
    "\n",
    "destCols = [\"ReportingUnitUUID\", \"BeneficialUseCategory\", \"Amount\", \"ReportYearCV\"]\n",
    "srsCols = [\"ReportingUnitUUID\", \"CategoryA\", \"AmountAF\", \"Year\"]\n",
    "\n",
    "outdf100[destCols] = df100[srsCols]\n",
    "\n",
    "outdf100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hardcoded\n",
    "\n",
    "outdf100.OrganizationUUID = \"CDWR\"\n",
    "#\n",
    "outdf100.VariableSpecificUUID = \"CA_Consumptive Use\"\n",
    "outdf100.WaterSourceUUID = \"CA_1\"\n",
    "outdf100.MethodUUID = \"CDWR_Water_uses\"\n",
    "\n",
    "# check this later\n",
    "outdf100.PrimaryUseCategory = \"Irrigation\"\n",
    "outdf100.TimeframeStart = \"01/01\"\n",
    "outdf100.TimeframeEnd = \"12/31\"\n",
    "#\n",
    "outdf100.DataPublicationDate = datetime.now().strftime('%m/%d/%Y') \n",
    "\n",
    "outdf100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Droping null amounts...\")\n",
    "\n",
    "# if Amount empty drop row and save it to a _missing.csv\n",
    "\n",
    "#outdf100 = outdf100.replace(np.nan, '') #replace NaN by blank strings\n",
    "\n",
    "outdf100purge = outdf100.loc[(outdf100[\"Amount\"] == '') | (outdf100[\"Amount\"] == np.nan)]\n",
    "if len(outdf100purge.index) > 0:\n",
    "    outdf100purge.to_csv('aggregatedallocations_missing.csv')    #index=False,\n",
    "    dropIndex = outdf100.loc[(outdf100[\"Amount\"] == '') | (outdf100[\"Amount\"] == np.nan)].index\n",
    "    outdf100 = outdf100.drop(dropIndex)\n",
    "    outdf100 = outdf100.reset_index(drop=True)\n",
    "#outdf100[\"PopulationServed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Droping null ReportingUnitID ...\")\n",
    "outdf100nullPR = outdf100.loc[(outdf100[\"ReportingUnitUUID\"] == '') \n",
    "                              | (outdf100[\"ReportingUnitUUID\"] == np.nan)]\n",
    "if len(outdf100nullPR.index) > 0:\n",
    "    dropIndex = outdf100.loc[(outdf100[\"ReportingUnitUUID\"] == '') \n",
    "                              | (outdf100[\"ReportingUnitUUID\"] == np.nan)].index\n",
    "    outdf100 = outdf100.drop(dropIndex)\n",
    "    outdf100 = outdf100.reset_index(drop=True)\n",
    "#outdf100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Droping duplicates...\")\n",
    "#drop duplicate rows; just make sure\n",
    "outdf100Duplicated=outdf100.loc[outdf100.duplicated()]\n",
    "if len(outdf100Duplicated.index) > 0:\n",
    "    outdf100Duplicated.to_csv(\"aggregatedallocations_duplicaterows.csv\")  # index=False,\n",
    "    outdf100.drop_duplicates(inplace=True)   #\n",
    "    outdf100 = outdf100.reset_index(drop=True)\n",
    "#outdf100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing outputs...\")\n",
    "\n",
    "#replace NaN by blank strings--this is to avoid blank columns getting default 0 by the import code\n",
    "outdf100 = outdf100.replace(np.nan, '') \n",
    "# outputs aggregated amounts\n",
    "out_agamount = \"aggregatedamounts.csv\"\n",
    "outdf100.to_csv(out_agamount, index=False, encoding = \"utf-8\")\n",
    "\n",
    "print(\"Done Aggregated amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The following is only for inspection when something seems not right from the above output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inspect duplicates for subset of columns...\")\n",
    "\n",
    "target_columns = [\"OrganizationUUID\", \"VariableSpecificUUID\", \n",
    "                  \"ReportingUnitUUID\",\n",
    "                  #\"PrimaryUseCategory\", \"BeneficialUseCategory\", \n",
    "                  \"WaterSourceUUID\", \"MethodUUID\", \"ReportYearCV\"]\n",
    "\n",
    "out_agamount1 = \"aggregatedamounts.csv\"\n",
    "outdf100 = pd.read_csv(out_agamount1,encoding = \"ISO-8859-1\")\n",
    "#drop duplicate rows; just make sure\n",
    "outdf100Duplicated=outdf100.loc[outdf100.duplicated(subset = target_columns)]\n",
    "if len(outdf100Duplicated.index) > 0:\n",
    "    outdf100Duplicated.to_csv(\"aggregateuse_duplicateID_rows.csv\")  # index=False,\n",
    "    outdf100.drop_duplicates(subset = target_columns, inplace=True)   #\n",
    "    outdf100 = outdf100.reset_index(drop=True)\n",
    "    \n",
    "#TODO: These may need removing \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
