{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f571be",
   "metadata": {},
   "source": [
    "# Data Assessment & Analytics - Regulatory Data\n",
    "Notes:\n",
    "- change os directory location\n",
    "- be aware of the number of provdied native source data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ee81d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with data\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd # the library that lets us read in shapefiles\n",
    "import geoplot as gplt # for plotting maps\n",
    "import geoplot.crs as gcrs #used to pull in webdata\n",
    "\n",
    "# visulizaiton\n",
    "import missingno as msno # creates a matrix chart to show missing values\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go # for subplot creation\n",
    "from plotly.subplots import make_subplots # for subplot creation\n",
    "import matplotlib.pyplot as mplt # use with gplt to save fig to pdf\n",
    "\n",
    "# Cleanup\n",
    "from datetime import datetime\n",
    "pd.set_option('display.max_columns', 999)  # How to display all columns of a Pandas DataFrame in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f24dccf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rjame\\Documents\\WSWC Documents\\MappingStatesDataToWaDE2.0\\2_CodeMappingTemplates\\Regulatory\n"
     ]
    }
   ],
   "source": [
    "# Working Directory\n",
    "#################################################################\n",
    "print(os.getcwd()) # see the current working directory\n",
    "\n",
    "# # ---- set working directory, if need be ----\n",
    "# workingDir = \"\" # file location\n",
    "# os.chdir(workingDir)\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62de750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Native Input Data\n",
    "# #################################################################\n",
    "# # ---- This needs to be custom per state ----\n",
    "\n",
    "# # Data Set 1: PointsOfDiversion_input\n",
    "# df1 = pd.read_csv('RawinputData/{enter file name here}.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecbf2d48",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ProcessedInputData/sites.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4908\\1585419982.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdfropurge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ProcessedInputData/regulatoryoverlays_missing.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ProcessedInputData/sites.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 )\n\u001b[1;32m--> 331\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[1;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1442\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"b\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1735\u001b[1;33m             self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 856\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ProcessedInputData/sites.csv'"
     ]
    }
   ],
   "source": [
    "# WaDE Processed Input Data\n",
    "#################################################################\n",
    "\n",
    "dfru = pd.read_csv(\"ProcessedInputData/reportingunits.csv\").replace(np.nan, \"\")\n",
    "dfrupurge = pd.read_csv(\"ProcessedInputData/reportingunits_missing.csv\").replace(np.nan, \"\")\n",
    "\n",
    "dfro = pd.read_csv(\"ProcessedInputData/regulatoryoverlays.csv\").replace(np.nan, \"\")\n",
    "dfropurge = pd.read_csv(\"ProcessedInputData/regulatoryoverlays_missing.csv\").replace(np.nan, \"\")\n",
    "\n",
    "dfs = pd.read_csv(\"ProcessedInputData/sites.csv\").replace(np.nan, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a75f368",
   "metadata": {},
   "source": [
    "## Why Removed Records Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56877eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---- Reasons why records were removed from reporting unit source info ----\")\n",
    "if len(dfrupurge) != 0:\n",
    "    print(dfrupurge['ReasonRemoved'].value_counts())\n",
    "else:\n",
    "    print(\"...nothing removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dfebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" ---- Reasons why records were removed from regula toryoverlays info ---- \")\n",
    "if len(dfropurge) != 0:\n",
    "    print(dfropurge['ReasonRemoved'].value_counts())\n",
    "else:\n",
    "    print(\"...nothing removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce42e7a5",
   "metadata": {},
   "source": [
    "# Reporting Unit Info\n",
    "- reportingunits.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fafebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dfru))\n",
    "dfru.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dfbef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what columns contain missing information.\n",
    "msno.matrix(dfru, figsize=(10,5), fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f09cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReportingUnitName: histogram distribution of WaDE values\n",
    "print(dfru.ReportingUnitName.value_counts())\n",
    "\n",
    "fig = px.histogram(dfru, x=\"ReportingUnitName\")\n",
    "fig.update_layout(bargap=0.2,\n",
    "                  title=\"Histogram of ReportingUnitName Entries in reportingunits.csv\",\n",
    "                  xaxis_title=\"ReportingUnitName Value\",\n",
    "                  yaxis_title=\"# of entries\",\n",
    "                    font=dict(\n",
    "                        family=\"Arial Bold\",\n",
    "                        size=12,\n",
    "                        color=\"Black\")\n",
    "                 )\n",
    "fig.show()\n",
    "fig.write_image('DataAssessment/figures/ReportingUnitName.pdf', engine=\"kaleido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReportingUnitTypeCV: histogram distribution of WaDE values\n",
    "print(dfru.ReportingUnitTypeCV.value_counts())\n",
    "\n",
    "fig = px.histogram(dfru, x=\"ReportingUnitTypeCV\")\n",
    "fig.update_layout(bargap=0.2,\n",
    "                  title=\"Histogram of ReportingUnitTypeCV Entries in reportingunits.csv\",\n",
    "                  xaxis_title=\"ReportingUnitTypeCV Value\",\n",
    "                  yaxis_title=\"# of entries\",\n",
    "                    font=dict(\n",
    "                        family=\"Arial Bold\",\n",
    "                        size=12,\n",
    "                        color=\"Black\")\n",
    "                 )\n",
    "fig.show()\n",
    "fig.write_image('DataAssessment/figures/ReportingUnitTypeCV.pdf', engine=\"kaleido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7762a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map poly info\n",
    "try:\n",
    "    contiguous_usa = gpd.read_file(gplt.datasets.get_path('contiguous_usa')) # use for background map in subplot\n",
    "    ax = gplt.webmap(contiguous_usa, projection=gcrs.WebMercator()) # set subplot\n",
    "    \n",
    "    dfruPoly = dfru.copy()\n",
    "    dfruPoly = dfruPoly[dfruPoly['Geometry'] != \"\"].reset_index(drop=True)\n",
    "    dfruPoly['Geometry'] = gpd.GeoSeries.from_wkt(dfruPoly['Geometry'], crs=\"EPSG:4326\")\n",
    "    gdfruPoly = gpd.GeoDataFrame(dfruPoly, geometry=dfruPoly['Geometry'], crs=\"EPSG:4326\") # covert to geodataframe\n",
    "    #gdfruPoly['Geometry'] = gdfruPoly.simplify(0.001) # simplify the geometry. Lower the number the larger the exported file.\n",
    "    gplt.polyplot(gdfruPoly, ax=ax)\n",
    "    mplt.savefig(format=\"pdf\", fname='DataAssessment/figures/ReportingUnitMap.pdf')\n",
    "except:\n",
    "    print('No geometry data to plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f94051",
   "metadata": {},
   "source": [
    "# Regulatory Overlay Info\n",
    "- regulatoryoverlays.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00742c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dfro))\n",
    "dfro.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f82ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OversightAgency: histogram distribution of WaDE values\n",
    "print(dfro.OversightAgency.value_counts())\n",
    "\n",
    "fig = px.histogram(dfro, x=\"OversightAgency\")\n",
    "fig.update_layout(bargap=0.2,\n",
    "                  title=\"Histogram of OversightAgency Entries in regulatoryoverlays.csv\",\n",
    "                  xaxis_title=\"OversightAgency Value\",\n",
    "                  yaxis_title=\"# of entries\",\n",
    "                    font=dict(\n",
    "                        family=\"Arial Bold\",\n",
    "                        size=12,\n",
    "                        color=\"Black\")\n",
    "                 )\n",
    "fig.show()\n",
    "fig.write_image('DataAssessment/figures/OversightAgency.pdf', engine=\"kaleido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a611547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegulatoryName: histogram distribution of WaDE values\n",
    "print(dfro.RegulatoryName.value_counts())\n",
    "\n",
    "fig = px.histogram(dfro, x=\"RegulatoryName\")\n",
    "fig.update_layout(bargap=0.2,\n",
    "                  title=\"Histogram of RegulatoryName Entries in regulatoryoverlays.csv\",\n",
    "                  xaxis_title=\"RegulatoryName Value\",\n",
    "                  yaxis_title=\"# of entries\",\n",
    "                    font=dict(\n",
    "                        family=\"Arial Bold\",\n",
    "                        size=12,\n",
    "                        color=\"Black\")\n",
    "                 )\n",
    "fig.show()\n",
    "fig.write_image('DataAssessment/figures/RegulatoryName.pdf', engine=\"kaleido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4af852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegulatoryOverlayTypeCV: histogram distribution of WaDE values\n",
    "print(dfro.RegulatoryOverlayTypeCV.value_counts())\n",
    "\n",
    "fig = px.histogram(dfro, x=\"RegulatoryOverlayTypeCV\")\n",
    "fig.update_layout(bargap=0.2,\n",
    "                  title=\"Histogram of RegulatoryOverlayTypeCV Entries in regulatoryoverlays.csv\",\n",
    "                  xaxis_title=\"RegulatoryOverlayTypeCV Value\",\n",
    "                  yaxis_title=\"# of entries\",\n",
    "                    font=dict(\n",
    "                        family=\"Arial Bold\",\n",
    "                        size=12,\n",
    "                        color=\"Black\")\n",
    "                 )\n",
    "fig.show()\n",
    "fig.write_image('DataAssessment/figures/RegulatoryOverlayTypeCV.pdf', engine=\"kaleido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d210db6f",
   "metadata": {},
   "source": [
    "# Site Info (related to Overlays)\n",
    "- sites.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8139f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dfs))\n",
    "dfs.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafbf9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegulatoryOverlayUUIDs: histogram distribution of WaDE values\n",
    "print(dfs.RegulatoryOverlayUUIDs.value_counts())\n",
    "\n",
    "fig = px.histogram(dfs, x=\"RegulatoryOverlayUUIDs\")\n",
    "fig.update_layout(bargap=0.2,\n",
    "                  title=\"Histogram of RegulatoryOverlayUUIDs Entries in sites.csv\",\n",
    "                  xaxis_title=\"RegulatoryOverlayUUIDs Value\",\n",
    "                  yaxis_title=\"# of entries\",\n",
    "                  #legend_title=\"Legend Title\",\n",
    "                    font=dict(\n",
    "                        family=\"Arial Bold\",\n",
    "                        size=12,\n",
    "                        color=\"Black\")\n",
    "                 )\n",
    "fig.show()\n",
    "fig.write_image('DataAssessment/figures/SiteRegulatoryOverlayUUIDs.pdf', engine=\"kaleido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b284749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the site info (this would be lat & long Points only)\n",
    "try:\n",
    "    contiguous_usa = gpd.read_file(gplt.datasets.get_path('contiguous_usa')) # use for background map in subplot\n",
    "    ax = gplt.webmap(contiguous_usa, projection=gcrs.WebMercator()) # set subplot\n",
    "    \n",
    "    dfsPoint = dfs.copy()\n",
    "    dfsPoint = dfsPoint[dfsPoint['RegulatoryOverlayUUIDs'] != \"\"]\n",
    "    gdfsPoint = gpd.GeoDataFrame(dfsPoint, geometry=gpd.points_from_xy(dfsPoint.Longitude.astype(float), dfsPoint.Latitude.astype(float)), crs=\"EPSG:4326\")\n",
    "    gplt.pointplot(gdfsPoint, hue='PODorPOUSite', legend=True, legend_var='hue', ax=ax)\n",
    "    mplt.savefig(format=\"pdf\", fname='DataAssessment/figures/PointInRegMap.pdf')\n",
    "    \n",
    "except:\n",
    "    print('No point data to plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd9f7f3",
   "metadata": {},
   "source": [
    "# Merge all figure pdfs into single output pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f254c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all figure pdfs into single output pdf\n",
    "\n",
    "'''\n",
    "Notes:\n",
    "'merger' is used for merging multiple files into one and merger.append(absfile) will append \n",
    " the files one by one until all pdfs are appended in the result file.\n",
    "'''\n",
    "\n",
    "from PyPDF2 import PdfFileMerger\n",
    "\n",
    "# If files are saved in the folder 'C:\\Users' then Full_Path will be replaced with C:\\Users\n",
    "filePath = str(os.getcwd()) + '/DataAssessment/figures'\n",
    "pdfsList = os.listdir(filePath)\n",
    "print(pdfsList)\n",
    "\n",
    "\n",
    "# os.listdir will create the list of all files in a directory\n",
    "merger = PdfFileMerger(strict=False)\n",
    "\n",
    "for file in pdfsList:\n",
    "    if file.endswith(\".pdf\"):\n",
    "        path_with_file = os.path.join(filePath, file)\n",
    "        print(path_with_file)\n",
    "        merger.append(path_with_file,  import_bookmarks=False )\n",
    "merger.write(\"DataAssessment/Figures Merged Copy.pdf\")\n",
    "\n",
    "merger.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc1baa7",
   "metadata": {},
   "source": [
    "# Removed Records compared to Source Data\n",
    "- this is working just fine\n",
    "- just want to comment out temporarily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode purge.xlsx files by WaDEUUID, concat together\n",
    "#################################################################\n",
    "\n",
    "# Explode watersources_missing.xlsx records by WaDEUUID\n",
    "dfwspurgeCopy = dfwspurge.assign(WaDEUUID=dfwspurge['WaDEUUID'].str.split(',')).explode('WaDEUUID').reset_index(drop=True)\n",
    "dfwspurgeCopy = dfwspurgeCopy[['WaDEUUID','ReasonRemoved','IncompleteField']]\n",
    "\n",
    "# Explode sites_missing.xlsx records by WaDEUUID\n",
    "dfspurgeCopy = dfspurge.assign(WaDEUUID=dfspurge['WaDEUUID'].str.split(',')).explode('WaDEUUID').reset_index(drop=True)\n",
    "dfspurgeCopy = dfspurgeCopy[['WaDEUUID','ReasonRemoved','IncompleteField']]\n",
    "\n",
    "# Explode waterallocations_missing.xlsx records by WaDEUUID\n",
    "dfaapurgeCopy = dfaapurge.assign(WaDEUUID=dfaapurge['WaDEUUID'].str.split(',')).explode('WaDEUUID').reset_index(drop=True)\n",
    "dfaapurgeCopy = dfaapurgeCopy[['WaDEUUID','ReasonRemoved','IncompleteField']]\n",
    "\n",
    "# concat purge dataframes togehter\n",
    "frames = [dfwspurgeCopy, dfspurgeCopy, dfaapurgeCopy] \n",
    "dfWaDEUUID = pd.concat(frames)\n",
    "dfWaDEUUID = dfWaDEUUID.drop_duplicates().reset_index(drop=True).replace(np.nan, \"\")\n",
    "print(len(dfWaDEUUID))\n",
    "dfWaDEUUID.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6035a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is custom to the provided data\n",
    "\n",
    "# # attach purge dataframe to Native Source Data\n",
    "# # df1: OSE POD Data\n",
    "# #################################################################\n",
    "\n",
    "# if 'ReasonRemoved' in df1:\n",
    "#     df1 = df1.drop(['ReasonRemoved', 'IncompleteField'], axis=1)\n",
    "\n",
    "# df1Copy = dfWaDEUUID.merge(df1, how='right', on='WaDEUUID')\n",
    "# df1Copy = df1Copy.groupby('WaDEUUID').agg(lambda x: ','.join([str(elem) for elem in (list(set(x))) if elem!=''])).replace(np.nan, \"\").reset_index()\n",
    "# df1Copy.to_csv('RawInputData/OSE_PODs.zip', compression=dict(method='zip', archive_name='OSE_PODs.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c080897",
   "metadata": {},
   "source": [
    "# Custom Queries and Analysis for this Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5661603",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# asdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
