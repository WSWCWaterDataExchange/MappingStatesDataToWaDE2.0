{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f178c7c",
   "metadata": {},
   "source": [
    "# Preprocessing Wyoming Reservoir and Gage data for WaDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b11961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed Libararies\n",
    "\n",
    "# working with data\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# visulizaiton\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# API retrieval\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Cleanup\n",
    "from datetime import datetime\n",
    "pd.set_option('display.max_columns', 999)  # How to display all columns of a Pandas DataFrame in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b71bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working Directory\n",
    "workingDir = \"G:/Shared drives/WaDE Data/Wyoming/SS_ReservoirsObservationSites/RawInputData\"\n",
    "os.chdir(workingDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6955129d",
   "metadata": {},
   "source": [
    "## Data: Total Lake Reservoir Storage Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba333a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1a = pd.read_csv(\"total_lake_reservoir_storage_volume/data set label.csv\")\n",
    "df2a = pd.read_csv(\"total_lake_reservoir_storage_volume/location identifier.csv\")\n",
    "df3a = pd.read_csv(\"total_lake_reservoir_storage_volume/location name.csv\")\n",
    "df4a = pd.read_csv(\"total_lake_reservoir_storage_volume/location type.csv\")\n",
    "df5a = pd.read_csv(\"total_lake_reservoir_storage_volume/latitude.csv\")\n",
    "df6a = pd.read_csv(\"total_lake_reservoir_storage_volume/longitude.csv\")\n",
    "\n",
    "df1a = df1a.rename({'Value': 'data set label'}, axis=1)\n",
    "df1a = df1a.merge(df2a[['Data Set Id', 'Value']], on='Data Set Id', how='left').rename({'Value': 'location identifier'}, axis=1)\n",
    "df1a = df1a.merge(df3a[['Data Set Id', 'Value']], on='Data Set Id', how='left').rename({'Value': 'location name'}, axis=1)\n",
    "df1a = df1a.merge(df4a[['Data Set Id', 'Value']], on='Data Set Id', how='left').rename({'Value': 'location type'}, axis=1)\n",
    "df1a = df1a.merge(df5a[['Data Set Id', 'Value']], on='Data Set Id', how='left').rename({'Value': 'latitude'}, axis=1)\n",
    "df1a = df1a.merge(df6a[['Data Set Id', 'Value']], on='Data Set Id', how='left').rename({'Value': 'longitude'}, axis=1)\n",
    "\n",
    "dftlrsv_site = df1a.copy()\n",
    "print(len(dftlrsv_site))\n",
    "dftlrsv_site.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd6b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# datasetidList = dftlrsv_site['Data Set Id'].astype(str).str.replace(\" \", \"%20\").str.replace(\"@\", \"%40\").tolist()  \n",
    "\n",
    "# # issue with SSL verification for this data. Use this to ignore\n",
    "# import ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# # create empty url dataframe for timeseries data\n",
    "# dftlrsv_timeseries = pd.DataFrame()\n",
    "\n",
    "# slength = len(datasetidList)\n",
    "# for i in range(slength):\n",
    "#     fileInputURL = \"https://seoflow.wyo.gov/Export/BulkExport?DateRange=EntirePeriodOfRecord&TimeZone=0&Calendar=CALENDARYEAR&Interval=Daily&Step=1&ExportFormat=csv&TimeAligned=True&RoundData=False&IncludeGradeCodes=False&IncludeApprovalLevels=False&IncludeQualifiers=undefined&IncludeInterpolationTypes=False&Datasets[0].Calculation=Aggregate&Datasets[0].UnitId=198&_=1679336048953&Datasets[0].DatasetName=\" + str(datasetidList[i])\n",
    "#     print(fileInputURL)\n",
    "#     try:\n",
    "#         dftemp = pd.read_csv(fileInputURL, skiprows=4)\n",
    "#         dftemp['timeseriesID'] =  str(datasetidList[i])\n",
    "#         dftemp['url'] = fileInputURL\n",
    "#         dftlrsv_timeseries = pd.concat([dftlrsv_timeseries, dftemp])\n",
    "#     except:\n",
    "#         dftemp = pd.DataFrame()\n",
    "#         dftemp['timeseriesID'] =  str(datasetidList[i])\n",
    "#         dftemp['url'] = fileInputURL\n",
    "#         dftlrsv_timeseries = pd.concat([dftlrsv_timeseries, dftemp])\n",
    "#         print(\"Error, issue with API return.\")\n",
    "\n",
    "        \n",
    "# dftlrsv_timeseries.to_csv('total_lake_reservoir_storage_volume/tlrsv_timeseries.zip', compression=dict(method='zip', archive_name='tlrsv_timeseries.csv'), index=False)\n",
    "# print(len(dftlrsv_timeseries))\n",
    "# dftlrsv_timeseries.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc73b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input File - dataframeTimeSeries.zip\n",
    "dftlrsv_timeseries = pd.read_csv('total_lake_reservoir_storage_volume/tlrsv_timeseries.zip', compression='zip')\n",
    "\n",
    "# WaDE UUID tracker for data assessment\n",
    "if 'WaDEUUID' not in dftlrsv_timeseries:\n",
    "    dftlrsv_timeseries['WaDEUUID'] = \"tx_tlrsv\" + dftlrsv_timeseries.index.astype(str)\n",
    "    dftlrsv_timeseries.to_csv('total_lake_reservoir_storage_volume/tlrsv_timeseries.zip', compression=dict(method='zip', archive_name='tlrsv_timeseries.csv'), index=False)\n",
    "\n",
    "dftlrsv_timeseries['timeseriesID'] = dftlrsv_timeseries['timeseriesID'].astype(str).str.replace(\"%20\", \" \").str.replace(\"%40\", \"@\")\n",
    "\n",
    "print(len(dftlrsv_timeseries))\n",
    "dftlrsv_timeseries.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b606e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftlrsv = pd.merge(dftlrsv_timeseries, dftlrsv_site, left_on='timeseriesID', right_on='Data Set Id', how='left')\n",
    "print(len(dftlrsv))\n",
    "dftlrsv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ba125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WaDE Fields\n",
    "\n",
    "# output dataframe\n",
    "df1 = pd.DataFrame(index=dftlrsv.index)\n",
    "\n",
    "# data assessment\n",
    "df1['WaDEUUID'] = dftlrsv['WaDEUUID']\n",
    "\n",
    "# variable info\n",
    "df1['in_VariableCV'] = dftlrsv['WaDE Interpretation']\n",
    "\n",
    "# water source info\n",
    "df1['in_WaterSourceName'] = \"WaDE Unspecified\"\n",
    "df1['in_WaterSourceNativeID'] = \"WaDEID_WYws1\"\n",
    "df1['in_WaterSourceTypeCV'] = \"Surface Water\"\n",
    "\n",
    "# Site Info\n",
    "df1['in_CoordinateAccuracy'] = \"WaDE Unspecified\"\n",
    "df1['in_CoordinateMethodCV'] = \"WaDE Unspecified\"\n",
    "df1['in_County'] = \"WaDE Unspecified\"\n",
    "df1['in_HUC12'] = \"WaDE Unspecified\"\n",
    "df1['in_HUC8'] = \"WaDE Unspecified\"\n",
    "df1['in_Latitude'] = dftlrsv['latitude']\n",
    "df1['in_Longitude'] = dftlrsv['longitude']\n",
    "df1['in_PODorPOUSite'] = \"Reservoir\"\n",
    "df1['in_SiteNativeID'] =dftlrsv['location identifier']\n",
    "df1['in_SiteName'] = dftlrsv['location name']\n",
    "df1['in_SiteTypeCV'] = \"Reservoir/Lake\"\n",
    "df1['in_StateCV'] = 'WY'\n",
    "\n",
    "# Site VariableAmounts Info\n",
    "df1['in_Amount'] = dftlrsv['Average (Acre-ft)'] # change here\n",
    "df1['in_BeneficialUseCategory'] = \"Storage\"\n",
    "df1['in_ReportYearCV'] = \"\" # will fill in below\n",
    "df1['in_TimeframeEnd'] = dftlrsv['End of Interval (UTC)']\n",
    "df1['in_TimeframeStart'] = dftlrsv['Start of Interval (UTC)']\n",
    "\n",
    "df1 = df1.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(len(df1))\n",
    "df1.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f34f76b",
   "metadata": {},
   "source": [
    "## Data: Discharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19181135",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1b = pd.read_csv(\"discharge/data set label.csv\")\n",
    "df2b = pd.read_csv(\"discharge/location identifier.csv\")\n",
    "df3b = pd.read_csv(\"discharge/location name.csv\")\n",
    "df4b = pd.read_csv(\"discharge/location type.csv\")\n",
    "df5b = pd.read_csv(\"discharge/latitude.csv\")\n",
    "df6b = pd.read_csv(\"discharge/longitude.csv\")\n",
    "\n",
    "df1b = df1b.rename({'Value': 'data set label'}, axis=1)\n",
    "df1b = df1b.merge(df2b[['Data Set Id', 'Value']], on='Data Set Id', how='left').rename({'Value': 'location identifier'}, axis=1)\n",
    "df1b = df1b.merge(df3b[['Data Set Id', 'Value']], on='Data Set Id', how='left').rename({'Value': 'location name'}, axis=1)\n",
    "df1b = df1b.merge(df4b[['Data Set Id', 'Value']], on='Data Set Id', how='left').rename({'Value': 'location type'}, axis=1)\n",
    "df1b = df1b.merge(df5b[['Data Set Id', 'Value']], on='Data Set Id', how='left').rename({'Value': 'latitude'}, axis=1)\n",
    "df1b = df1b.merge(df6b[['Data Set Id', 'Value']], on='Data Set Id', how='left').rename({'Value': 'longitude'}, axis=1)\n",
    "\n",
    "dfdis_site = df1b.copy()\n",
    "print(len(dfdis_site))\n",
    "dfdis_site.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a05500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# datasetidList = dfdis_site['Data Set Id'].astype(str).str.replace(\" \", \"%20\").str.replace(\"@\", \"%40\").tolist()  \n",
    "\n",
    "# # issue with SSL verification for this data. Use this to ignore\n",
    "# import ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# # create empty url dataframe for timeseries data\n",
    "# dfdis_timeseries = pd.DataFrame()\n",
    "\n",
    "# slength = len(datasetidList)\n",
    "# for i in range(slength):\n",
    "#     fileInputURL = \"https://seoflow.wyo.gov/Export/BulkExport?DateRange=EntirePeriodOfRecord&TimeZone=0&Calendar=CALENDARYEAR&Interval=Daily&Step=1&ExportFormat=csv&TimeAligned=True&RoundData=False&IncludeGradeCodes=False&IncludeApprovalLevels=False&IncludeQualifiers=undefined&IncludeInterpolationTypes=False&Datasets[0].Calculation=Instantaneous&Datasets[0].UnitId=208&_=1679418181067&Datasets[0].DatasetName=\" + str(datasetidList[i])\n",
    "#     print(fileInputURL)\n",
    "#     try:\n",
    "#         dftemp = pd.read_csv(fileInputURL, skiprows=4)\n",
    "#         dftemp['timeseriesID'] =  str(datasetidList[i])\n",
    "#         dftemp['url'] = fileInputURL\n",
    "#         dfdis_timeseries = pd.concat([dfdis_timeseries, dftemp])\n",
    "#     except:\n",
    "#         dftemp = pd.DataFrame()\n",
    "#         dftemp['timeseriesID'] =  str(datasetidList[i])\n",
    "#         dftemp['url'] = fileInputURL\n",
    "#         dfdis_timeseries = pd.concat([dfdis_timeseries, dftemp])\n",
    "#         print(\"Error, issue with API return.\")\n",
    "\n",
    "        \n",
    "# dfdis_timeseries.to_csv('discharge/dis_timeseries.zip', compression=dict(method='zip', archive_name='dis_timeseries.csv'), index=False)\n",
    "# print(len(dfdis_timeseries))\n",
    "# dfdis_timeseries.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c937b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input File - dataframeTimeSeries.zip\n",
    "dfdis_timeseries = pd.read_csv('discharge/dis_timeseries.zip', compression='zip')\n",
    "\n",
    "# WaDE UUID tracker for data assessment\n",
    "if 'WaDEUUID' not in dfdis_timeseries:\n",
    "    dfdis_timeseries['WaDEUUID'] = \"tx_dis\" + dfdis_timeseries.index.astype(str)\n",
    "    dfdis_timeseries.to_csv('discharge/dis_timeseries.zip', compression=dict(method='zip', archive_name='dis_timeseries.csv'), index=False)\n",
    "\n",
    "dfdis_timeseries['timeseriesID'] = dfdis_timeseries['timeseriesID'].astype(str).str.replace(\"%20\", \" \").str.replace(\"%40\", \"@\")\n",
    "\n",
    "print(len(dfdis_timeseries))\n",
    "dfdis_timeseries.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb107516",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfdis = pd.merge(dfdis_timeseries, dfdis_site, left_on='timeseriesID', right_on='Data Set Id', how='left')\n",
    "print(len(dfdis))\n",
    "dfdis.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e961cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WaDE Fields\n",
    "\n",
    "# output dataframe\n",
    "df2 = pd.DataFrame(index=dfdis.index)\n",
    "\n",
    "# data assessment\n",
    "df2['WaDEUUID'] = dfdis['WaDEUUID']\n",
    "\n",
    "# variable info\n",
    "df2['in_VariableCV'] = dfdis['WaDE Interpretation']\n",
    "\n",
    "# water source info\n",
    "df2['in_WaterSourceName'] = \"WaDE Unspecified\"\n",
    "df2['in_WaterSourceNativeID'] = \"WaDEID_WYws1\"\n",
    "df2['in_WaterSourceTypeCV'] = \"Surface Water\"\n",
    "\n",
    "# Site Info\n",
    "df2['in_CoordinateAccuracy'] = \"WaDE Unspecified\"\n",
    "df2['in_CoordinateMethodCV'] = \"WaDE Unspecified\"\n",
    "df2['in_County'] = \"WaDE Unspecified\"\n",
    "df2['in_HUC12'] = \"WaDE Unspecified\"\n",
    "df2['in_HUC8'] = \"WaDE Unspecified\"\n",
    "df2['in_Latitude'] = dfdis['latitude']\n",
    "df2['in_Longitude'] = dfdis['longitude']\n",
    "df2['in_PODorPOUSite'] = \"Stream Gage\"\n",
    "df2['in_SiteNativeID'] =dfdis['location identifier']\n",
    "df2['in_SiteName'] = dfdis['location name']\n",
    "df2['in_SiteTypeCV'] = \"Hydrology Station\"\n",
    "df2['in_StateCV'] = 'WY'\n",
    "\n",
    "# Site VariableAmounts Info\n",
    "df2['in_Amount'] = dfdis['Value at End of Interval (ft^3/s)'] # change here\n",
    "df2['in_BeneficialUseCategory'] = \"Discharge\"\n",
    "df2['in_ReportYearCV'] = \"\" # will fill in below\n",
    "df2['in_TimeframeEnd'] = dfdis['End of Interval (UTC)']\n",
    "df2['in_TimeframeStart'] = dfdis['Start of Interval (UTC)']\n",
    "\n",
    "df2 = df2.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(len(df2))\n",
    "df2.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32a4d8a",
   "metadata": {},
   "source": [
    "## Concatenate Together. Output Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425554d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate Produced Data Together\n",
    "frames = [df1, df2]\n",
    "dfout = pd.concat(frames).reset_index(drop=True)\n",
    "print(len(dfout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea89a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfout.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e485f862",
   "metadata": {},
   "source": [
    "## Fixing a few errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd0a05",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create VariableSpecificCV field\n",
    "\n",
    "dfout['in_VariableSpecificCV'] = dfout['in_VariableCV'].astype(str) + \"_Daily_\" + dfout['in_BeneficialUseCategory'].astype(str) + \"_\" + dfout['in_WaterSourceTypeCV'].astype(str)\n",
    "dfout['in_VariableSpecificCV'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82766db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data from string to datetime64[ns]\n",
    "# extracting year component of datetime64[ns]\n",
    "dfout['in_TimeframeEnd'] = pd.to_datetime(dfout['in_TimeframeEnd'])\n",
    "dfout['in_TimeframeStart'] = pd.to_datetime(dfout['in_TimeframeStart'])\n",
    "dfout['in_ReportYearCV'] = dfout['in_TimeframeStart'].dt.to_period('Y')\n",
    "dfout.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e454a3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_Latitude \n",
    "dfout['in_Latitude'] = pd.to_numeric(dfout['in_Latitude'], errors='coerce').fillna(0)\n",
    "dfout['in_Latitude'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f141a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_Longitude\n",
    "dfout['in_Longitude'] = pd.to_numeric(dfout['in_Longitude'], errors='coerce').fillna(0)\n",
    "dfout['in_Longitude'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff6f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing Water Amount datatype\n",
    "dfout['in_Amount'] = pd.to_numeric(dfout['in_Amount'], errors='coerce').fillna(0)\n",
    "dfout.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89691d7e",
   "metadata": {},
   "source": [
    "## Review and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd47e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfout.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c37ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting to Finished File\n",
    "dfout.to_csv('P_wySSROMain.zip', index=False, compression=\"zip\")  # The output, save as a zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b60b8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
