{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Texas Reservoir and Observation Site data for WaDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Needed Libararies\n",
    "\n",
    "# working with data\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# visulizaiton\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# API retrieval\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Cleanup\n",
    "from datetime import datetime\n",
    "pd.set_option('display.max_columns', 999)  # How to display all columns of a Pandas DataFrame in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working Directory\n",
    "workingDir = \"G:/Shared drives/WaDE Data/Texas/SS_ReservoirsObservationSites/RawInputData\"\n",
    "os.chdir(workingDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Site Data\n",
    "- use csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileInput = \"recent-conditions.csv\"\n",
    "dfs = pd.read_csv(fileInput)\n",
    "print(len(dfs))\n",
    "dfs.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unique site name to use for API service\n",
    "dfs['apiSiteName'] = dfs['short_name'].str.lower().str.replace(' ', '-').str.strip()\n",
    "dfs.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeseries Data\n",
    "- use API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of apiSiteName\n",
    "apiSiteNameList = dfs['apiSiteName'].tolist()   \n",
    "print(len(apiSiteNameList))\n",
    "apiSiteNameList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# done already, use zip file\n",
    "\n",
    "# %%time\n",
    "# # get timeseries results. this took about 23 minutes.\n",
    "# # use StationNumber in url\n",
    "\n",
    "# # issue with SSL verification for this TX data. Use this to ignore\n",
    "# import ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# # create empty dataframe\n",
    "# columnsList = [\"date\",\n",
    "#                \"water_level\",\n",
    "#                \"surface_area\",\n",
    "#                \"reservoir_storage\",\n",
    "#                \"conservation_storage\",\n",
    "#                \"percent_full\",\n",
    "#                \"conservation_capacity\",\n",
    "#                \"dead_pool_capacity\",\n",
    "#                \"apiSiteName\"]\n",
    "# dfts = pd.DataFrame(columns=columnsList)\n",
    "\n",
    "# sglength = len(apiSiteNameList)\n",
    "# for i in range(sglength):\n",
    "#     fileInputURL = \"https://www.waterdatafortexas.org/reservoirs/individual/\" + str(apiSiteNameList[i]) + \".csv\"\n",
    "#     print(fileInputURL)\n",
    "#     try:\n",
    "#         dftemp = pd.read_csv(fileInputURL, comment=\"#\", skip_blank_lines=True) # skip comment lines with a #\n",
    "#         dftemp['apiSiteName'] = str(apiSiteNameList[i])\n",
    "#         dftemp.columns = dfts.columns\n",
    "#         dfts = pd.concat([dfts, dftemp])   \n",
    "#     except:\n",
    "#         dftemp = pd.DataFrame()\n",
    "#         dftemp['apiSiteName'] = str(apiSiteNameList[i])\n",
    "#         dfts = pd.concat([dfts, dftemp], ignore_index=False, sort=False) \n",
    "#         print(\"Error, issue with API return.\")\n",
    "\n",
    "# dfts.to_csv('timeSeriesData.zip', index=False, compression=\"zip\")  # The output, save as a zip\n",
    "# print(len(dfts))\n",
    "# dfts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input File - dataframeTimeSeries.zip\n",
    "dfts = pd.read_csv('timeSeriesData.zip', compression='zip')\n",
    "\n",
    "# WaDE UUID tracker for data assessment\n",
    "if 'WaDEUUID' not in dfts:\n",
    "    dfts['WaDEUUID'] = \"txssro\" + dfts.index.astype(str)\n",
    "    dfts.to_csv('timeSeriesData.zip', index=False)\n",
    "\n",
    "print(len(dfts))\n",
    "dfts.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Merging dataframes into one, using left-join.\n",
    "dfts = pd.merge(dfts, dfs, on='apiSiteName', how='left')\n",
    "print(len(dfts))\n",
    "dfts.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### water_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output dataframe\n",
    "dfwl = pd.DataFrame(index=dfts.index)\n",
    "\n",
    "# data assessment\n",
    "dfwl['WaDEUUID'] = dfts['WaDEUUID']\n",
    "\n",
    "# variable info\n",
    "dfwl['in_VariableCV'] = 'Reservoir Level' # change here\n",
    "\n",
    "# water source info\n",
    "dfwl['in_WaterSourceName'] = \"WaDE Unspecified\"\n",
    "dfwl['in_WaterSourceNativeID'] = \"WaDEID_TXws1\"\n",
    "dfwl['in_WaterSourceTypeCV'] = \"Surface Water\"\n",
    "\n",
    "# Site Info\n",
    "dfwl['in_CoordinateAccuracy'] = \"WaDE Unspecified\"\n",
    "dfwl['in_CoordinateMethodCV'] = \"WaDE Unspecified\"\n",
    "dfwl['in_County'] = \"WaDE Unspecified\"\n",
    "dfwl['in_HUC12'] = \"WaDE Unspecified\"\n",
    "dfwl['in_HUC8'] = \"WaDE Unspecified\"\n",
    "dfwl['in_Latitude'] = dfts['Latitude']\n",
    "dfwl['in_Longitude'] = dfts['Longitude']\n",
    "dfwl['in_PODorPOUSite'] = \"Observation Site\"\n",
    "dfwl['in_SiteNativeID'] = \"\" # will fill in below\n",
    "dfwl['in_SiteName'] = dfts['short_name']\n",
    "dfwl['in_SiteTypeCV'] = 'Reservoir'\n",
    "dfwl['in_StateCV'] = 'TX'\n",
    "\n",
    "# Site VariableAmounts Info\n",
    "dfwl['in_Amount'] = dfts['water_level'] # change here\n",
    "dfwl['in_BeneficialUseCategory'] = \"Storage\"\n",
    "dfwl['in_ReportYearCV'] = dfts['date']\n",
    "dfwl['in_TimeframeEnd'] = dfts['date']\n",
    "dfwl['in_TimeframeStart'] = dfts['date']\n",
    "\n",
    "print(len(dfwl))\n",
    "dfwl.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reservoir_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output dataframe\n",
    "dfrs = pd.DataFrame(index=dfts.index)\n",
    "\n",
    "# data assessment\n",
    "dfrs['WaDEUUID'] = dfts['WaDEUUID']\n",
    "\n",
    "# variable info\n",
    "dfrs['in_VariableCV'] = 'Reservoir Storage' # change here\n",
    "\n",
    "# water source info\n",
    "dfrs['in_WaterSourceName'] = \"WaDE Unspecified\"\n",
    "dfrs['in_WaterSourceNativeID'] = \"WaDEID_TXws1\"\n",
    "dfrs['in_WaterSourceTypeCV'] = \"Surface Water\"\n",
    "\n",
    "# Site Info\n",
    "dfrs['in_CoordinateAccuracy'] = \"WaDE Unspecified\"\n",
    "dfrs['in_CoordinateMethodCV'] = \"WaDE Unspecified\"\n",
    "dfrs['in_County'] = \"WaDE Unspecified\"\n",
    "dfrs['in_HUC12'] = \"WaDE Unspecified\"\n",
    "dfrs['in_HUC8'] = \"WaDE Unspecified\"\n",
    "dfrs['in_Latitude'] = dfts['Latitude']\n",
    "dfrs['in_Longitude'] = dfts['Longitude']\n",
    "dfrs['in_PODorPOUSite'] = \"Observation Site\"\n",
    "dfrs['in_SiteNativeID'] = \"\" # will fill in below\n",
    "dfrs['in_SiteName'] = dfts['short_name']\n",
    "dfrs['in_SiteTypeCV'] = 'Reservoir'\n",
    "dfrs['in_StateCV'] = 'TX'\n",
    "\n",
    "# Site VariableAmounts Info\n",
    "dfrs['in_Amount'] = dfts['reservoir_storage'] # change here\n",
    "dfrs['in_BeneficialUseCategory'] = \"Storage\"\n",
    "dfrs['in_ReportYearCV'] = dfts['date']\n",
    "dfrs['in_TimeframeEnd'] = dfts['date']\n",
    "dfrs['in_TimeframeStart'] = dfts['date']\n",
    "\n",
    "print(len(dfrs))\n",
    "dfrs.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [dfwl, dfrs]\n",
    "dfout = pd.concat(frames).drop_duplicates().reset_index(drop=True)\n",
    "print(len(dfout))\n",
    "dfout.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Creating WaDE Custom site native ID for easy site identificaiion\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create temp SiteNativeID dataframe of unique site.\n",
    "def assignSiteUUID(colrowValue):\n",
    "    string1 = str(colrowValue)\n",
    "    outstring = \"WaDETX_S\" + string1\n",
    "    return outstring\n",
    "\n",
    "dfSiteNativeID = pd.DataFrame()\n",
    "dfSiteNativeID['in_SiteName'] = dfout['in_SiteName']\n",
    "dfSiteNativeID = dfSiteNativeID.drop_duplicates()\n",
    "\n",
    "dftemp = pd.DataFrame(index=dfSiteNativeID.index)\n",
    "dftemp[\"Count\"] = range(1, len(dftemp.index) + 1)\n",
    "dfSiteNativeID['in_SiteNativeID'] = dftemp.apply(lambda row: assignSiteUUID(row['Count']), axis=1)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Retreive WaDE Custom site native ID\n",
    "def retrieveSiteNativeID(val):\n",
    "    if (val == \"\") or (pd.isnull(val)):\n",
    "        outList = \"\"\n",
    "    else:\n",
    "        ml = dfSiteNativeID.loc[(dfSiteNativeID['in_SiteName'] == val), 'in_SiteNativeID']\n",
    "        if not (ml.empty):  # check if the series is empty\n",
    "            outList = ml.iloc[0]\n",
    "        else:\n",
    "            outList = \"\"\n",
    "    return outList\n",
    "\n",
    "dfout['in_SiteNativeID'] = dfout.apply(lambda row: retrieveSiteNativeID(row['in_SiteName']), axis=1)\n",
    "dfout['in_SiteNativeID'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfout.info()\n",
    "dfout.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert History Year to YYYY-MM-DD format.\n",
    "dfout['in_TimeframeEnd'] = pd.to_datetime(dfout['in_TimeframeEnd'], utc=True)\n",
    "dfout['in_TimeframeEnd'] = pd.to_datetime(dfout[\"in_TimeframeEnd\"].dt.strftime('%m/%d/%Y'))\n",
    "\n",
    "dfout['in_TimeframeStart'] = pd.to_datetime(dfout['in_TimeframeStart'], utc=True)\n",
    "dfout['in_TimeframeStart'] = pd.to_datetime(dfout[\"in_TimeframeStart\"].dt.strftime('%m/%d/%Y'))\n",
    "\n",
    "dfout.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract year out\n",
    "dfout['in_ReportYearCV'] = pd.to_datetime(dfout['in_ReportYearCV'], utc=True)\n",
    "dfout['in_ReportYearCV'] = pd.to_datetime(dfout[\"in_ReportYearCV\"].dt.strftime('%m/%d/%Y'))\n",
    "dfout['in_ReportYearCV'] = dfout['in_ReportYearCV'].dt.year\n",
    "dfout.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_Latitude & in_Longitude\n",
    "dfout['in_Latitude'] = pd.to_numeric(dfout['in_Latitude'], errors='coerce').fillna(0)\n",
    "dfout['in_Longitude'] = pd.to_numeric(dfout['in_Longitude'], errors='coerce').fillna(0)\n",
    "dfout.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(dfout.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting to Finished File\n",
    "dfout.to_csv('P_txSSROMain.zip', index=False, compression=\"zip\")  # The output, save as a zip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
