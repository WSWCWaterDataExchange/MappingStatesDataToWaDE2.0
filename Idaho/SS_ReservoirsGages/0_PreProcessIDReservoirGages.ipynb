{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Idaho Site Specific Reservoir and Gage stream data for WaDE upload.\n",
    "\n",
    "\n",
    "#### Idaho Nested Data Aqua API Data\n",
    "The Aqua Info interface has an API behind it.  Here's a sample using “Pole Creek” site in the Upper Salmon Basin, using the GUID (99db207c15774d1c9a2f2a9daad85efa) in the URL, followed by the requsted time stamp.\n",
    "- https://research.idwr.idaho.gov/apps/hydrologic/aquainfo/api/telemetry/99db207c15774d1c9a2f2a9daad85efa?fromDate=2020-09-21T14:33:47.134Z&toDate=2020-10-21T14:33:47.134Z\n",
    "\n",
    "The GUID is inside each object’s “locDescription” property.\n",
    "- https://research.idwr.idaho.gov/apps/hydrologic/aquainfo/api/telemetry/locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed Libararies\n",
    "\n",
    "# Working with data\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Visulizaiton\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Working with API\n",
    "import requests\n",
    "import io\n",
    "import json\n",
    "\n",
    "# Cleanup\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "pd.set_option('display.max_columns', 999)  # How to display all columns of a Pandas DataFrame in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working Directory\n",
    "workingDir = \"G:/Shared drives/WaDE Data/Idaho/SS_ReservoirsGages/RawInputData\"\n",
    "os.chdir(workingDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get API Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The general API search.  Returns a nestede data list with 228 entires.\n",
    "\n",
    "url = \"https://research.idwr.idaho.gov/apps/hydrologic/aquainfo/api/telemetry/locations\"\n",
    "responseList = json.loads(requests.get(url).text)\n",
    "\n",
    "print(type(responseList))\n",
    "print(len(responseList))\n",
    "responseList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is working. Just need to finish.\n",
    "#This will produce step 1 of using the API.  Step two will be to take the output here and use the other API format using the uniqueID and the time start and end date.\n",
    "\n",
    "# Disecting the nested list per nested list.  Input based on index of dataframe.\n",
    "\n",
    "url = \"https://research.idwr.idaho.gov/apps/hydrologic/aquainfo/api/telemetry/locations\"\n",
    "responseList = json.loads(requests.get(url).text)\n",
    "SearchIndex = 0\n",
    "outdfIndex = 0\n",
    "datasetsIndex = 0\n",
    "\n",
    "outdf = pd.DataFrame(columns=['locationName', 'identifier', 'loc_uniqueId', 'locationType', 'latitude', 'longitude', 'srid',\n",
    "                              'ds_uniqueId', 'parameter', 'unit', 'rawStartTime', 'rawEndTime', 'label']) \n",
    "dfdatasets = pd.DataFrame(columns=['locationName', 'identifier', 'loc_uniqueId', 'locationType', 'latitude', 'longitude', 'srid',\n",
    "                                   'ds_uniqueId', 'parameter', 'unit', 'rawStartTime', 'rawEndTime', 'label']) \n",
    "\n",
    "\n",
    "while SearchIndex < len(responseList):\n",
    "    LtDi = responseList[SearchIndex]\n",
    "    \n",
    "    #locationName\n",
    "    LtDi_lsn = LtDi['locationName']\n",
    "    outdf.loc[outdfIndex, 'locationName'] = LtDi_lsn\n",
    "    \n",
    "    #list of identifier\n",
    "    LtDi_is = LtDi['identifier']\n",
    "    outdf.loc[outdfIndex, 'identifier'] = LtDi_is\n",
    "    \n",
    "    #list of loc_uniqueId\n",
    "    LtDi_un = LtDi['uniqueId']\n",
    "    outdf.loc[outdfIndex, 'loc_uniqueId'] = LtDi_un\n",
    "    \n",
    "    #list of locationType\n",
    "    LtDi_lt = LtDi['locationType']\n",
    "    outdf.loc[outdfIndex, 'locationType'] = LtDi_lt\n",
    "    \n",
    "    #list of latitude\n",
    "    LtDi_la = LtDi['latitude']\n",
    "    outdf.loc[outdfIndex, 'latitude'] = LtDi_la\n",
    "    \n",
    "    #list of longitude\n",
    "    LtDi_lo = LtDi['longitude']\n",
    "    outdf.loc[outdfIndex, 'longitude'] = LtDi_lo\n",
    "    \n",
    "    #list of srid\n",
    "    LtDi_sr = LtDi['srid']\n",
    "    outdf.loc[outdfIndex, 'srid'] = LtDi_sr\n",
    "       \n",
    "    # Time Series Info in New Nested List\n",
    "    DL = LtDi['datasets']\n",
    "    if DL is None:\n",
    "        outdf.loc[outdfIndex, 'ds_uniqueId'] = \"\"\n",
    "        outdf.loc[outdfIndex, 'parameter'] = \"\"\n",
    "        outdf.loc[outdfIndex, 'unit'] = \"\"\n",
    "        outdf.loc[outdfIndex, 'rawEndTime'] = \"\"\n",
    "        outdf.loc[outdfIndex, 'rawStartTime'] = \"\"\n",
    "        outdf.loc[outdfIndex, 'label'] = \"\"\n",
    "    else:\n",
    "        while datasetsIndex < len(DL):\n",
    "            DLD = DL[datasetsIndex]\n",
    "            \n",
    "            #list of ds_uniqueId\n",
    "            DLDstr = DLD['uniqueId']\n",
    "            outdf.loc[outdfIndex, 'ds_uniqueId'] = DLDstr\n",
    "            \n",
    "            #list of parameter\n",
    "            DLDstr = DLD['parameter']\n",
    "            outdf.loc[outdfIndex, 'parameter'] = DLDstr\n",
    "            \n",
    "            #list of unit\n",
    "            DLDstr = DLD['unit']\n",
    "            outdf.loc[outdfIndex, 'unit'] = DLDstr\n",
    "            \n",
    "            #list of rawEndTime\n",
    "            DLDstr = DLD['rawEndTime']\n",
    "            outdf.loc[outdfIndex, 'rawEndTime'] = DLDstr\n",
    "            \n",
    "            #list of rawStartTime\n",
    "            DLDstr = DLD['rawStartTime']\n",
    "            outdf.loc[outdfIndex, 'rawStartTime'] = DLDstr\n",
    "            \n",
    "            #list of label\n",
    "            DLDstr = DLD['label']\n",
    "            outdf.loc[outdfIndex, 'label'] = DLDstr\n",
    "            \n",
    "            #Copy exiting rows from index row, repeat with index - datasetsIndex.\n",
    "            outdf.loc[outdfIndex, 'locationName'] = outdf.loc[outdfIndex-datasetsIndex, 'locationName']\n",
    "            outdf.loc[outdfIndex, 'identifier'] = outdf.loc[outdfIndex-datasetsIndex, 'identifier']\n",
    "            outdf.loc[outdfIndex, 'loc_uniqueId'] = outdf.loc[outdfIndex-datasetsIndex, 'loc_uniqueId']\n",
    "            outdf.loc[outdfIndex, 'locationType'] = outdf.loc[outdfIndex-datasetsIndex, 'locationType']\n",
    "            outdf.loc[outdfIndex, 'latitude'] = outdf.loc[outdfIndex-datasetsIndex, 'latitude']\n",
    "            outdf.loc[outdfIndex, 'longitude'] = outdf.loc[outdfIndex-datasetsIndex, 'longitude']\n",
    "            outdf.loc[outdfIndex, 'srid'] = outdf.loc[outdfIndex-datasetsIndex, 'srid']\n",
    "            datasetsIndex += 1  # Advanced datasetsIndex counter\n",
    "            outdfIndex += 1 # Advanced outdfIndex counter due to extra rows if datasets is not None.\n",
    "            \n",
    "        datasetsIndex = 0  # reset datasetsIndex\n",
    "    \n",
    "    # Advanced Index Counter\n",
    "    SearchIndex += 1\n",
    "    outdfIndex += 1\n",
    "\n",
    "print(len(outdf))\n",
    "outdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating outputString for the url.\n",
    "\n",
    "str1 = \"https://research.idwr.idaho.gov/apps/hydrologic/aquainfo/api/telemetry/\"\n",
    "\n",
    "def assignoutputString(A, B, C):\n",
    "    if A == '' or pd.isnull(A):\n",
    "        outString = \"Unspecified\"\n",
    "    else:\n",
    "        outString = str1 + str(A) + \"?fromDate=\" + str(B) + \"&toDate=\" + str(C)\n",
    "    return outString\n",
    "\n",
    "outdf['outputString'] = outdf.apply(lambda row: assignoutputString(row['ds_uniqueId'], row['rawStartTime'], row['rawEndTime']), axis=1)\n",
    "print(len(outdf))\n",
    "outdf = outdf.reset_index(drop=True)\n",
    "outdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop and only keep columns rows with \"label\" = Daily or daily.\n",
    "outdf = outdf[(outdf.label == \"Daily\") | (outdf.label == \"daily\")]\n",
    "print(len(outdf))\n",
    "outdf = outdf.reset_index(drop=True)\n",
    "outdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The TS API search.\n",
    "\n",
    "tsdf = pd.DataFrame(columns=['locationName', 'loc_uniqueId',\n",
    "                             'timeStamp', 'numericValue1']) \n",
    "\n",
    "tsSearchIndex = 0\n",
    "outdfIndex = 0\n",
    "tsdfIndex = 0\n",
    "\n",
    "while outdfIndex < len(outdf):\n",
    "    \n",
    "    url = outdf.loc[outdfIndex, 'outputString']\n",
    "    print(outdfIndex)\n",
    "    print(url)\n",
    "    \n",
    "    if url == \"Unspecified\":\n",
    "        # Copy exsisting rows from outdf to tsdf.\n",
    "        tsdf.loc[tsdfIndex, \"locationName\"] = outdf.loc[outdfIndex, \"locationName\"]  \n",
    "        tsdf.loc[tsdfIndex, \"loc_uniqueId\"] = outdf.loc[outdfIndex, \"loc_uniqueId\"] \n",
    "        tsdfIndex += 1\n",
    "    else:\n",
    "        #The URl\n",
    "        resD = json.loads(requests.get(url).text)\n",
    "        resDL = resD['points']\n",
    "        print(\"Length is: \", len(resDL))\n",
    "        \n",
    "        tsSearchIndex = 0\n",
    "        while tsSearchIndex < len(resDL):\n",
    "            \n",
    "            # Copy exsisting rows from outdf to tsdf.\n",
    "            tsdf.loc[tsdfIndex, \"locationName\"] = outdf.loc[outdfIndex, \"locationName\"]  \n",
    "            tsdf.loc[tsdfIndex, \"loc_uniqueId\"] = outdf.loc[outdfIndex, \"loc_uniqueId\"]\n",
    "            \n",
    "            #Time Series serach index.\n",
    "            LtD = resDL[tsSearchIndex]\n",
    "    \n",
    "            #timeStamp\n",
    "            LtD_ts = LtD['timeStamp']\n",
    "            tsdf.loc[tsdfIndex, 'timeStamp'] = LtD_ts\n",
    "            \n",
    "            #numericValue1\n",
    "            LtD_nv1 = LtD['numericValue1']\n",
    "            tsdf.loc[tsdfIndex, 'numericValue1'] = LtD_nv1\n",
    "            \n",
    "            \n",
    "#             print(\"outdfIndex is \" + str(outdfIndex) + \", tsdfIndex is \" + str(tsdfIndex) + \", tsSearchIndex is \" + str(tsSearchIndex))\n",
    "#             tsdflist = tsdf.loc[tsdfIndex].tolist()\n",
    "#             print(tsdflist)\n",
    "            \n",
    "            tsdfIndex += 1\n",
    "            tsSearchIndex += 1\n",
    "    \n",
    "    # Advanced Index Counter\n",
    "    outdfIndex += 1\n",
    "\n",
    "print(len(tsdf))\n",
    "tsdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge\n",
    "outdf = pd.merge(outdf, tsdf, left_on='loc_uniqueId', right_on='loc_uniqueId', how='left')\n",
    "print(len(outdf))\n",
    "outdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update datatype of date to fit WaDE 2.0 structure\n",
    "outdf['timeStamp'] = pd.to_datetime(outdf['timeStamp'], utc=True)\n",
    "outdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract year value for ReportYearCV\n",
    "outdf['in_ReportYear'] = pd.DatetimeIndex(outdf['timeStamp']).year\n",
    "outdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting to Finished File\n",
    "outdf.to_csv('P_idOSMaster.csv', index=False)  # The output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
