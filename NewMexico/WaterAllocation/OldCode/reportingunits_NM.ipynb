{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from waterallocationsFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working directory\n",
    "working_dir = \"C:/tseg/NMTest/aggregatedamounts/\"\n",
    "os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = [\"ReportingUnitUUID\", \"ReportingUnitNativeID\", \"ReportingUnitName\", \n",
    "                  \"ReportingUnitTypeCV\", \"ReportingUnitUpdateDate\", \"ReportingUnitProductVersion\",\n",
    "                  \"StateCV\", \"EPSGCodeCV\", \"Geometry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf100 = pd.DataFrame(columns=target_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input files\n",
    "fileInput1 = \"Summary of withdrawals by county 90-15.xlsx\" \n",
    "fileInput2 = \"Summary of withdrawals by River Basin 90-15.xlsx\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = pd.read_excel(fileInput1, header=0, sheet_name=0, skiprows=1, encoding = \"ISO-8859-1\")\n",
    "#df20 = pd.read_excel(fileInput2, header=0, sheet_name=0, skiprows=1, encoding = \"ISO-8859-1\")\n",
    "list(df10.columns)\n",
    "#list(df20.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine multiple sheets to one dataFrame\n",
    "\n",
    "startYear = 1990\n",
    "endYear = 2015\n",
    "numSheets = 5\n",
    "yearList = np.linspace(startYear, endYear, numSheets)\n",
    "df100_list = []\n",
    "for isx in range (numSheets):\n",
    "    df10 = pd.read_excel(fileInput1, header=0, sheet_name=isx, skiprows=1, encoding = \"ISO-8859-1\")\n",
    "    df10 = df10.assign(ReportYearCV=yearList[isx])\n",
    "    df10.ReportYearCV = df10.ReportYearCV.astype(int)\n",
    "    df100_list.append(df10)\n",
    "    \n",
    "df100 = pd.concat(df100_list, sort=True)\n",
    "\n",
    "df100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Copying County name...\")\n",
    "\n",
    "# all we need from input is county name\n",
    "\n",
    "# first remove redundancies\n",
    "df100 = df100.drop_duplicates(subset=[\"COUNTY\"])   #\n",
    "df100 = df100.reset_index(drop=True)\n",
    "\n",
    "print(len(df100.index))\n",
    "\n",
    "outdf100[\"ReportingUnitName\"] = df100[\"COUNTY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReportingUnitNativeID\t \t Auto generate\n",
    "outdf100['ReportingUnitNativeID'] = range(1, len(outdf100.index) + 1)\n",
    "\n",
    "#ReportingUnitUUID\t \t NM_NativeID\n",
    "outdf100['ReportingUnitUUID'] = outdf100.apply(lambda row: \n",
    "                                              \"_\".join([\"NM\", str(row['ReportingUnitNativeID'])]),\n",
    "                                                axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapefile\n",
    "import pygeoif\n",
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "print(\"Geometry...\")\n",
    "\n",
    "working_dir = \"C:/tseg/MappingStatesDataToWaDE2.0/NewMexico/WaterAllocation/NMShapefile/\"\n",
    "os.chdir(working_dir)\n",
    "\n",
    "#Geometry\tGet it from shapefile as WKT\n",
    "shapefilName = 'OSE_Points_of_Diversion.shp'\n",
    "nm_counties = \"NM_Historical_Counties.shp\"\n",
    "us_counties = \"US_County_Boundaries.shp\"\n",
    "basins = \"WUR_surface_Basins.shp\"\n",
    "\n",
    "\n",
    "sf = shapefile.Reader(basins)\n",
    "\n",
    "print(sf)\n",
    "\n",
    "shapes = sf.shapes()\n",
    "\n",
    "shapes[0].shapeType\n",
    "\n",
    "fields = sf.fields\n",
    "\n",
    "print (fields)\n",
    "\n",
    "gm= pygeoif.geometry.as_shape(shapes[0])\n",
    "#print (gm.wkt)\n",
    "\n",
    "#print (fields[0])\n",
    "records = sf.records()\n",
    "print(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shapefile\n",
    "import pygeoif\n",
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "working_dir = \"C:/tseg/MappingStatesDataToWaDE2.0/NewMexico/WaterAllocation/NMShapefile/\"\n",
    "os.chdir(working_dir)\n",
    "\n",
    "#Geometry\tGet it from shapefile as WKT\n",
    "shapefilName = 'OSE_Points_of_Diversion.shp'\n",
    "nm_counties = \"NM_Historical_Counties.shp\"\n",
    "us_counties = \"US_County_Boundaries.shp\"\n",
    "basins = \"WUR_surface_Basins.shp\"\n",
    "\n",
    "geojson_file = \"basins_geojson\"\n",
    "geodf = gpd.read_file(basins)\n",
    "geodf.to_file(geojson_file, driver = \"GeoJSON\")\n",
    "\n",
    "with open(geojson_file) as geofile:\n",
    "    basinsJson = json.load(geofile)\n",
    "\n",
    "\"\"\"\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
    "    basinsJson = json.load(response)\n",
    "\n",
    "i=1\n",
    "for feature in basinsJson[\"features\"]:\n",
    "    #feature ['id'] = str(i).zfill(2)\n",
    "    #locations.append(str(i).zfill(2))\n",
    "    zvals.append(1.0*i/6.0)\n",
    "    i += 1\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "locations =[]\n",
    "zvals = []\n",
    "\n",
    "if 'id' not in basinsJson['features'][0].keys():\n",
    "    if 'properties' in basinsJson['features'][0].keys():\n",
    "        if 'id' in basinsJson['features'][0]['properties'] and basinsJson['features'][0]['properties']['id'] is not None:\n",
    "            for k, feat in enumerate(basinsJson['features']):\n",
    "                basinsJson['features'][k]['id'] = feat['properties']['id']\n",
    "        else:\n",
    "            for k in range(len(basinsJson['features'])):\n",
    "                basinsJson['features'][k]['id'] = str(k+1).zfill(2)\n",
    "                basinsJson['features'][k]['properties']['id'] = str(k+1).zfill(2)\n",
    "\"\"\"\n",
    "for k in range(len(basinsJson['features'])):\n",
    "                basinsJson['features'][k]['id'] = str(k+1).zfill(2)\n",
    "                basinsJson['features'][k]['properties']['id'] = str(k+1).zfill(2)\n",
    "            \n",
    "for k in range(len(basinsJson['features'])):\n",
    "    locations.append(basinsJson['features'][k]['id'])  \n",
    "    zvals.append(100.0*k)\n",
    "\n",
    "print(len(locations))\n",
    "print(len(zvals))\n",
    "print(len(basinsJson['features']))\n",
    "#for k in range(len(basinsJson['features'])):basinsJson['features'][k]['id'] = str(k+1).zfill(2)\n",
    "        \n",
    "#basinsJson\n",
    "#print(locations)\n",
    "idS = [basinsJson['features'][k]['id'] for k in range(len(basinsJson['features']))]\n",
    "print(locations)\n",
    "print(idS)\n",
    "print(zvals)\n",
    "#print(basinsJson['features'][5].keys())\n",
    "\n",
    "#print(basinsJson['features'][0]['properties'])\n",
    "#print(basinsJson['features'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[feat['properties']['REG_NAME'] for feat in basinsJson['features'] if feat['id'] in locations]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = open(\"../.mapbox_token\").read() \n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "   \n",
    "trace = go.Choroplethmapbox(z=zvals,\n",
    "                            locations=locations,\n",
    "                            colorscale=\"Viridis\",\n",
    "                            colorbar=dict(thickness=20, ticklen=3),\n",
    "                            geojson=basinsJson,\n",
    "                            text=text,\n",
    "                            hoverinfo='all',\n",
    "                            marker_line_width=0.1, marker_opacity=0.7)\n",
    "                            \n",
    "                            \n",
    "layout = go.Layout(title_text= 'Choroplethmapbox',\n",
    "                   title_x=0.5, width = 700, height=700,\n",
    "                   mapbox = dict(center= dict(lat=35.1,  lon=-106.6),\n",
    "                                 accesstoken= token,\n",
    "                                 zoom=3,\n",
    "                               ))\n",
    "\n",
    "fig = go.Figure(data=[trace], layout =layout)\n",
    "fig.update_layout(mapbox_style = \"light\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = open(\"../.mapbox_token\").read() \n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "fig = go.Figure(go.Choroplethmapbox(geojson=basinsJson, locations=locations, z=zvals,\n",
    "                                    colorscale=\"Viridis\", zmin=0, zmax=6.0, marker_line_width=0.5, marker_opacity=0.7))\n",
    "fig.update_layout(mapbox_style=\"light\", mapbox_accesstoken=token,\n",
    "                  mapbox_zoom=5, mapbox_center = {\"lat\": 35.1, \"lon\": -106.6})\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "shapeRecs = sf.shapeRecords()\n",
    "\n",
    "for shaperec in r.iterShapeRecords():\n",
    "    w.record(*shaperec.record)\n",
    "\n",
    "\n",
    "fields = sf.fields[1:] \n",
    "field_names = [field[0] for field in fields] \n",
    "# construction of a dctionary field_name:value  \n",
    "for r in sf.shapeRecords():  \n",
    "    atr = dict(zip(field_names, r.record)) \n",
    "    if atr['STATE'] == 'New Mexico':\n",
    "        action\n",
    "\n",
    "for shape in sf.iterShapes():\n",
    "...     # do something here\n",
    "...     pass\n",
    "\n",
    ">>> for rec in sf.iterRecords():\n",
    "...     # do something here\n",
    "...     pass\n",
    "\n",
    ">>> for shapeRec in sf.iterShapeRecords():\n",
    "...     # do something here\n",
    "...     pass\n",
    "\n",
    ">>> for shapeRec in sf: # same as iterShapeRecords()\n",
    "...     # do something here\n",
    "...     pass\n",
    "\n",
    "gm=[]\n",
    "\n",
    "for sp in shapes:\n",
    "    gm.append(pygeoif.geometry.as_shape(sp)) \n",
    "\n",
    "m = pygeoif.MultiPoint(gm)\n",
    "\n",
    "print (m.wkt)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hardcoded\n",
    "\n",
    "outdf100.StateCV = \"NM\"\n",
    "outdf100.ReportingUnitTypeCV = \"County\"\n",
    "outdf100.EPSGCodeCV = \"EPSG:4326\"\n",
    "sampleWKT =  'POLYGON((-99.54319297853704 37.15853229006052, -97.26976797641987 37.15759429005948, -105.11636298372741 37.14764529005038, -104.52740598317905 37.15119229005359, -104.09963198278069 37.15376929005606, -103.56062798227867 37.156443290058405, -103.12301898187116 37.157137290059154, -103.08639398183686 37.15689329005886, -103.00203898175846 37.156332290058344, -99.90287697887197 37.162385290064094, -99.54319297853704 37.15853229006052))'\n",
    "outdf100.Geometry = sampleWKT\n",
    "\n",
    "# replace NaN with blank cells\n",
    "outdf100 = outdf100.replace(np.nan, '')\n",
    "outdf100.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing out...\")\n",
    "\n",
    "#write out\n",
    "out_repunit = 'reportingunits.csv'\n",
    "outdf100.to_csv(out_repunit, index=False, encoding = \"utf-8\")\n",
    "\n",
    "print(\"Done sites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
